[
  {
    "name": "ai-in-the-terminal",
    "path": "integrations/docs/ai-in-the-terminal.md",
    "content": "# AI in the Terminal - Complete Guide\n\nWelcome to the companion guide for NetworkChuck's \"AI in the Terminal\" video! This repository contains everything you need to follow along and master AI tools in the terminal.\n\n## ðŸ“º Watch the Video\n\n[![AI in the Terminal - NetworkChuck](https://img.youtube.com/vi/MsQACpcuTkU/maxresdefault.jpg)](https://youtu.be/MsQACpcuTkU)\n\n**[â–¶ï¸ Watch on YouTube: AI in the Terminal](https://youtu.be/MsQACpcuTkU)**\n\n## ðŸŽ¯ What You'll Learn\n\nThis guide covers how to:\n\n- Break free from browser-based AI limitations\n- Use **Gemini CLI**, **Claude Code**, **Codex**, and **opencode** in your terminal\n- Maintain persistent context across sessions with context files\n- Deploy AI agents for specialized tasks\n- Run multiple AI tools simultaneously on the same project\n- Sync context files across different AI tools\n- Build custom workflows for maximum productivity\n\n## ðŸš€ Why Terminal AI?\n\n**Browser AI problems:**\n\n- Lost context after scrolling too far\n- Multiple scattered chats across different platforms\n- No file system access\n- Copy/paste hell\n- Limited control over your work\n\n**Terminal AI advantages:**\n\n- âœ… **Persistent context** - Your work lives in files, not chat windows\n- âœ… **File system access** - Read and write files directly\n- âœ… **Multiple AI tools** working together on the same project\n- âœ… **Complete control** - Everything stored locally on your hard drive\n- âœ… **Professional workflows** - Build custom agents and automation\n- âœ… **10x faster** - No more context switching or copy/paste\n\n## ðŸ“š Guide Structure\n\n### Getting Started\n\n1. [Prerequisites](docs/01-prerequisites.md) - What you need before starting\n2. [Quick Start](docs/02-quickstart.md) - Get up and running in 5 minutes\n\n### Individual Tools\n\n3. [Gemini CLI Guide](docs/03-gemini-cli.md) - Google's free terminal AI\n4. [Claude Code Guide](docs/04-claude-code.md) - The most powerful terminal AI\n5. [Codex Guide](docs/05-codex.md) - ChatGPT in your terminal\n6. [opencode Guide](docs/06-opencode.md) - Open-source with local model support\n\n### Advanced Workflows\n\n7. [Context Files Explained](docs/07-context-files.md) - Master persistent context\n8. [Multi-Tool Workflow](docs/08-multi-tool-workflow.md) - Use all tools simultaneously\n9. [AI Agents Deep Dive](docs/09-agents.md) - Deploy specialized AI workers\n10. [Output Styles & Customization](docs/10-customization.md) - Make AI work YOUR way\n\n### Real-World Examples\n\n11. [Productivity Workflows](docs/11-productivity-workflows.md) - Writing, research, planning\n12. [Development Workflows](docs/12-development-workflows.md) - Coding and debugging\n13. [Homelab & IT Workflows](docs/13-homelab-workflows.md) - Sysadmin tasks\n\n### Reference\n\n14. [Command Cheat Sheet](docs/14-cheat-sheet.md) - Quick reference for all commands\n15. [Troubleshooting](docs/15-troubleshooting.md) - Common issues and solutions\n16. [FAQ](docs/16-faq.md) - Frequently asked questions\n\n## ðŸŽ¬ Follow Along with the Video\n\nEach section of this guide corresponds to a segment in the video:\n\n- **0:00-1:26** - The Problem â†’ [Prerequisites](docs/01-prerequisites.md)\n- **1:27-4:14** - Gemini CLI Demo â†’ [Gemini CLI Guide](docs/03-gemini-cli.md)\n- **8:44-14:26** - Claude Code â†’ [Claude Code Guide](docs/04-claude-code.md)\n- **18:03-19:25** - Multi-Tool Workflow â†’ [Multi-Tool Workflow](docs/08-multi-tool-workflow.md)\n- **20:31-24:48** - Real Workflow Demo â†’ [Productivity Workflows](docs/11-productivity-workflows.md)\n- **26:32-30:00** - opencode â†’ [opencode Guide](docs/06-opencode.md)\n\n## âš¡ Quick Start (5 Minutes)\n\nWant to start right now? Here's the fastest path:\n\n```bash\n# Install Gemini CLI (FREE)\nnpm install -g @google/generative-ai-cli\n\n# Create a test project\nmkdir my-ai-project\ncd my-ai-project\n\n# Launch Gemini\ngemini\n\n# Try your first command\n# Ask: \"Create a plan for learning Python\"\n```\n\n[Continue to full Quick Start guide â†’](docs/02-quickstart.md)\n\n## ðŸ’° Pricing Breakdown\n\n| Tool            | Free Tier                   | Paid Option                    | Best For                      |\n| --------------- | --------------------------- | ------------------------------ | ----------------------------- |\n| **Gemini CLI**  | âœ… Generous free tier       | Google One AI Premium ($20/mo) | Getting started, research     |\n| **Claude Code** | âŒ No free tier             | Claude Pro ($20/mo)            | Professional work, agents     |\n| **Codex**       | Limited free                | ChatGPT Plus ($20/mo)          | Analysis, high-level thinking |\n| **opencode**    | âœ… Free (Grok/local models) | Provider subscription          | Flexibility, experimentation  |\n\n**Chuck's Recommendation:** Start with Gemini CLI (free), then get Claude Pro if you can only choose one paid subscription.\n\n## ðŸ”— Official Links\n\n- [Gemini CLI Documentation](https://ai.google.dev/gemini-api/docs/cli)\n- [Claude Code Documentation](https://docs.anthropic.com/claude/docs/claude-code)\n- [Codex Documentation](https://platform.openai.com/docs/tools/codex)\n- [opencode Repository](https://github.com/stackblitz-labs/opencode)\n\n## ðŸ›¡ï¸ Security Note (from the video)\n\n**TwinGate Sponsor Message:** If you're giving AI access to your computer, make sure your remote access is secured properly. Traditional VPNs give full network access - consider zero-trust solutions like TwinGate for granular control.\n\n[Learn about TwinGate](https://twingate.com/networkchuck)\n\n## ðŸ™ Credits\n\n**Created by:** NetworkChuck\n**Video:** [AI in the Terminal](https://youtu.be/MsQACpcuTkU)\n**Guide Maintained by:** NetworkChuck community\n\n## ðŸ“ Contributing\n\nFound an error? Want to add a workflow? Submit a pull request or open an issue!\n\n## âš–ï¸ License\n\nThis guide is provided as a free companion to the NetworkChuck video. Feel free to use, share, and modify for personal and educational use.\n\n---\n\n**Ready to get started?** â†’ [Prerequisites](docs/01-prerequisites.md)\n\n**Have questions?** â†’ [FAQ](docs/16-faq.md)\n\n**Need help?** â†’ [Troubleshooting](docs/15-troubleshooting.md)\n\n---\n\n# docs/01-prerequisites.md\n\n# Prerequisites\n\nBefore diving into AI terminal tools, let's make sure you have everything you need.\n\n## Required\n\n### 1. Terminal Access\n\nYou need a terminal/command line:\n\n- **macOS**: Built-in Terminal app (Cmd+Space, type \"Terminal\")\n- **Linux**: Any terminal emulator (usually Ctrl+Alt+T)\n- **Windows**:\n  - Windows Subsystem for Linux (WSL) - **Recommended**\n  - PowerShell\n  - Git Bash\n\n#### Why WSL for Windows?\n\nChuck uses WSL (Ubuntu) in the video. It provides a Linux environment on Windows, which most terminal tools are optimized for.\n\n**Install WSL:** [Chuck's WSL video](https://www.youtube.com/watch?v=[WSL_VIDEO_ID])\n\n```powershell\n# In PowerShell (Admin)\nwsl --install\n```\n\n### 2. Node.js & npm\n\nMost terminal AI tools are installed via npm (Node Package Manager).\n\n**Check if you have it:**\n\n```bash\nnode --version\nnpm --version\n```\n\n**Don't have it?** Install from [nodejs.org](https://nodejs.org/) (LTS version recommended)\n\n### 3. A Google Account (for Gemini CLI)\n\n- Any free Gmail account works\n- No Google One AI Premium required for basic usage\n\n### 4. AI Subscriptions (Optional but Recommended)\n\n| Tool        | Free Option                 | Paid Option                  | Chuck's Take                         |\n| ----------- | --------------------------- | ---------------------------- | ------------------------------------ |\n| Gemini CLI  | âœ… Generous free tier       | Google One AI Premium $20/mo | \"Start here - it's FREE\"             |\n| Claude Code | âŒ Requires Claude Pro      | Claude Pro $20/mo            | \"This is my daily driver - worth it\" |\n| Codex       | Limited free                | ChatGPT Plus $20/mo          | \"Good for analysis\"                  |\n| opencode    | âœ… Grok free / Local models | Various providers            | \"Great for experimentation\"          |\n\n**Chuck's Recommendation:**\n\n> \"If you can only pay for one AI subscription, Claude Pro is the one I would choose.\"\n\n## Recommended\n\n### 5. Git (Optional but Useful)\n\nChuck treats his projects like code - version control for everything.\n\n**Check if installed:**\n\n```bash\ngit --version\n```\n\n**Install:**\n\n- macOS: `xcode-select --install`\n- Linux: `sudo apt install git` (Ubuntu/Debian)\n- Windows: [git-scm.com](https://git-scm.com/)\n\n### 6. Text Editor\n\nYou'll be editing context files. Any text editor works:\n\n- **Terminal-based**: nano (easiest), vim, emacs\n- **GUI**: VS Code, Sublime Text, Notepad++\n\n### 7. Basic Terminal Skills\n\nYou should be comfortable with:\n\n- Navigating directories (`cd`, `ls`/`dir`)\n- Creating directories (`mkdir`)\n- Basic file operations (`cat`, `nano`)\n- Understanding file paths\n\n**New to terminal?** Don't worry - Chuck walks through everything in the video!\n\n## System Requirements\n\n### Minimum\n\n- **RAM**: 4GB (8GB recommended)\n- **Storage**: 1GB free space\n- **Internet**: Required for cloud AI models\n- **OS**: macOS 10.15+, Ubuntu 20.04+, Windows 10+ (with WSL)\n\n### Recommended for Local Models (opencode)\n\n- **RAM**: 16GB+\n- **Storage**: 10GB+ (for model files)\n- **Ollama installed** (for local models)\n\n## Permissions & Security\n\n### What These Tools Can Access\n\nâš ï¸ **Important Security Note:**\n\nTerminal AI tools can:\n\n- âœ… Read files in directories you open them in\n- âœ… Write files to your system\n- âœ… Execute bash commands\n- âœ… Access your Obsidian vault (or any files)\n- âœ… Run Python/bash scripts\n\nThis is POWERFUL but requires responsibility.\n\n### Chuck's Security Tips\n\n1. **Start in a test directory** - Don't open AI tools in your root or home directory initially\n2. **Review file permissions** - Claude Code asks permission by default (good!)\n3. **Use `--dangerously-skip-permissions` carefully** - Only when you trust what you're doing\n4. **Secure your remote access** - If using AI on remote servers, use zero-trust tools like TwinGate\n\n### Dangerous Mode Flag\n\nMany tools have a \"skip permissions\" mode:\n\n```bash\n# Claude Code without safety checks\nclaude --dangerously-skip-permissions\n\n# Use this ONLY when you know what you're doing\n# Chuck uses it for speed in the video\n```\n\n## Quick Environment Check\n\nRun this to verify you're ready:\n\n```bash\n# Check terminal\necho \"Terminal works!\"\n\n# Check Node.js\nnode --version && npm --version\n\n# Check available space\ndf -h .\n\n# Create test directory\nmkdir -p ~/ai-terminal-test\ncd ~/ai-terminal-test\necho \"Setup complete! Ready to install tools.\"\n```\n\n## What's Next?\n\nâœ… Prerequisites complete? â†’ [Quick Start Guide](02-quickstart.md)\n\nâ“ Having issues? â†’ [Troubleshooting](15-troubleshooting.md)\n\n---\n\n[â† Back to README](../README.md) | [Next: Quick Start â†’](02-quickstart.md)\n\n# docs/02-quickstart.md\n\n# Quick Start Guide\n\nGet up and running with AI in the terminal in **5 minutes**.\n\n## Choose Your Path\n\n### Path A: Free Start (Gemini CLI)\n\n**Best for:** First-timers, budget-conscious users, testing the concept\n\n### Path B: Power User Start (Claude Code)\n\n**Best for:** Professionals ready to commit, agent users, serious workflows\n\n## Path A: Free Start (Gemini CLI)\n\n### Step 1: Install (30 seconds)\n\n**Linux / macOS / WSL:**\n\n```bash\nnpm install -g @google/generative-ai-cli\n```\n\n**macOS (Homebrew):**\n\n```bash\nbrew install gemini-cli\n```\n\n**Permission error?** Add `sudo`:\n\n```bash\nsudo npm install -g @google/generative-ai-cli\n```\n\n### Step 2: Create Project (10 seconds)\n\n```bash\nmkdir ai-test-project\ncd ai-test-project\n```\n\n### Step 3: Launch & Login (1 minute)\n\n```bash\ngemini\n```\n\n**First launch:**\n\n1. Browser opens automatically\n2. Sign in with Google account (any Gmail works!)\n3. Click \"Allow\"\n4. Return to terminal - you're in!\n\n### Step 4: Your First Task (2 minutes)\n\n**Try this:**\n\n```bash\n> How do I make the best cup of coffee?\n```\n\n**Watch it:**\n\n- Search the web\n- Craft response\n- Return results\n\n**Now try this (the magic part):**\n\n```bash\n> Research the top 5 coffee brewing methods.\n  Create a comparison document called coffee-methods.md\n```\n\n**Gemini asks:** \"Create file?\" â†’ Type `y`\n\n**Check it out:**\n\n```bash\nls\n# You'll see: coffee-methods.md\n\ncat coffee-methods.md\n# Your research, saved locally!\n```\n\n### Step 5: Create Context File (1 minute)\n\n**The game-changer:**\n\n```bash\n> /init\n```\n\nGemini analyzes your project and creates `gemini.md`\n\n**Now exit and reopen:**\n\n```bash\nexit\ngemini\n```\n\n**Try this:**\n\n```bash\n> Continue working on the coffee project\n```\n\n**It remembers!** No re-explaining needed.\n\n### âœ… You're Done! (5 minutes total)\n\n**What you learned:**\n\n- âœ… Gemini can create files\n- âœ… Context files persist knowledge\n- âœ… No more re-explaining your project\n- âœ… Everything stored locally\n\n**Next steps:**\n\n- [Full Gemini CLI Guide](03-gemini-cli.md)\n- [Understanding Context Files](07-context-files.md)\n\n---\n\n## Path B: Power User Start (Claude Code)\n\n**Requirements:**\n\n- Claude Pro subscription ($20/mo)\n- Node.js installed\n\n### Step 1: Install (30 seconds)\n\n```bash\nnpm install -g @anthropic-ai/claude-code\n```\n\n**Permission error?**\n\n```bash\nsudo npm install -g @anthropic-ai/claude-code\n```\n\n### Step 2: Create Project (10 seconds)\n\n```bash\nmkdir my-project\ncd my-project\n```\n\n### Step 3: Launch & Authenticate (1 minute)\n\n```bash\nclaude\n```\n\n**First launch:**\n\n1. Browser opens for authentication\n2. Login with Claude Pro account\n3. Approve directory access\n4. Return to terminal\n\n### Step 4: Create Context File (30 seconds)\n\n```bash\n> /init\n```\n\nClaude analyzes directory and creates `claude.md`\n\n### Step 5: Create Your First Agent (2 minutes)\n\n**This is where it gets powerful:**\n\n```bash\n> /agents\n```\n\n**Select:** \"Create new agent\"\n\n**Choose:** \"Project-specific agent\"\n\n**Name it:** `research-expert`\n\n**Describe it:**\n\n```\nYou are a research specialist. When given a topic:\n1. Search for authoritative sources\n2. Compile key findings\n3. Create structured summaries\n```\n\n**Configure:**\n\n- Tools: All tools\n- Model: Sonnet\n\n**Press Enter** to save\n\n### Step 6: Deploy Your Agent (1 minute)\n\n```bash\n> @research-expert\n  Research zero-trust network architecture.\n  Create a summary document.\n```\n\n**Watch your agent work:**\n\n- Fresh context window (200K tokens!)\n- Independent research\n- Returns results\n\n**Check the file:**\n\n```bash\nls\ncat zero-trust-summary.md\n```\n\n### âœ… You're Done! (5 minutes total)\n\n**What you learned:**\n\n- âœ… Claude Code basics\n- âœ… Agent creation\n- âœ… Context persistence\n- âœ… Agent deployment\n\n**Next steps:**\n\n- [Full Claude Code Guide](04-claude-code.md)\n- [AI Agents Deep Dive](09-agents.md)\n- [Output Styles Guide](10-customization.md)\n\n---\n\n## Quick Comparison\n\n| Feature            | Gemini CLI   | Claude Code |\n| ------------------ | ------------ | ----------- |\n| **Cost**           | Free         | $20/mo      |\n| **Setup Time**     | 2 min        | 3 min       |\n| **Best Feature**   | Web research | AI agents   |\n| **Context File**   | gemini.md    | claude.md   |\n| **Learning Curve** | Easy         | Medium      |\n\n**Chuck's recommendation:**\n\n> \"Start with Gemini CLI. It's FREE. But if you can afford one subscription, Claude Pro is the one I'd choose.\"\n\n---\n\n## Common First-Time Issues\n\n### \"Command not found\"\n\n```bash\n# Reload your shell\nsource ~/.bashrc\n# or\nsource ~/.zshrc\n\n# Or close and reopen terminal\n```\n\n### \"Permission denied\"\n\n```bash\n# Run with sudo\nsudo npm install -g [package-name]\n```\n\n### \"Node.js not found\"\n\nInstall from [nodejs.org](https://nodejs.org/) (LTS version)\n\n### Context file not loading\n\n```bash\n# Verify you're in the right directory\npwd\nls gemini.md\n\n# Recreate if needed\n> /init\n```\n\n---\n\n## What to Try Next\n\n### Beginner Tasks\n\n1. **Research task:** Ask Gemini to research a topic and create a summary\n2. **Writing task:** Ask Claude to write a blog intro\n3. **File organization:** Create a project structure and let AI populate it\n\n### Intermediate Tasks\n\n4. **Create an agent:** Make a specialized agent for your work\n5. **Multi-session work:** Exit, reopen, verify context loads\n6. **File manipulation:** Update existing files with AI help\n\n### Advanced Tasks\n\n7. **Multiple AIs:** Run Gemini and Claude simultaneously\n8. **Context syncing:** Keep gemini.md and claude.md in sync\n9. **Custom output style:** Create personality for your work\n\n---\n\n## 5-Minute Challenges\n\n### Challenge 1: Coffee Research (Gemini)\n\n**Goal:** Research, create file, reload context\n\n```bash\nmkdir coffee-challenge\ncd coffee-challenge\ngemini\n\n> Research French press vs pour-over coffee.\n  Create comparison-chart.md\n\n> /init\n\nexit\ngemini\n\n> Add a recommendation section to comparison-chart.md\n  based on taste preferences\n```\n\n**Success if:** File created and AI remembers project without re-explaining\n\n### Challenge 2: Agent Deploy (Claude)\n\n**Goal:** Create and use an agent\n\n```bash\nmkdir agent-challenge\ncd agent-challenge\nclaude\n\n> /agents\n# Create \"homelab-helper\" agent\n# Instructions: \"Expert in homelab hardware and networking\"\n\n> @homelab-helper\n  What's the best budget NAS for a home lab?\n```\n\n**Success if:** Agent responds with detailed recommendations\n\n### Challenge 3: Multi-Tool (Advanced)\n\n**Goal:** Use Gemini and Claude together\n\n```bash\nmkdir multi-tool-challenge\ncd multi-tool-challenge\n\n# Terminal 1:\ngemini\n> Research the top 3 text editors\n> /init\n\n# Terminal 2:\nclaude\n> /init\n> Read the research and write a blog post intro\n```\n\n**Success if:** Claude reads Gemini's research file\n\n---\n\n## Next Steps by Interest\n\n### I want to use AI for writing\n\nâ†’ [Productivity Workflows](11-productivity-workflows.md)\nâ†’ [Output Styles Guide](10-customization.md)\n\n### I want to use AI for coding\n\nâ†’ [Development Workflows](12-development-workflows.md)\nâ†’ [Claude Code Guide](04-claude-code.md)\n\n### I want to use AI for IT/homelab work\n\nâ†’ [Homelab Workflows](13-homelab-workflows.md)\nâ†’ [AI Agents Guide](09-agents.md)\n\n### I want to understand the concepts deeply\n\nâ†’ [Context Files Explained](07-context-files.md)\nâ†’ [Multi-Tool Workflow](08-multi-tool-workflow.md)\n\n---\n\n## Questions?\n\n**\"Which tool should I start with?\"**\n\n- Free: Gemini CLI\n- Professional: Claude Code\n- Experimental: opencode\n\n**\"Do I need all three?\"**\n\n- No! Start with one\n- Add others as you see benefits\n- Chuck uses all three for different strengths\n\n**\"How much does this cost?\"**\n\n- Gemini CLI: FREE\n- Claude Code: $20/mo (Claude Pro)\n- opencode: Free (Grok) or various providers\n\n**\"Is this just for developers?\"**\n\n- NO! Chuck uses it for video scripts\n- Works for writing, research, planning, any text work\n- Coding is just one use case\n\n---\n\n**Ready to dive deeper?** â†’ [Full Guide Navigation](../README.md)\n\n**Having issues?** â†’ [Troubleshooting](15-troubleshooting.md)\n\n**Want the commands?** â†’ [Cheat Sheet](14-cheat-sheet.md)\n\n---\n\n[â† Back to Prerequisites](01-prerequisites.md) | [Next: Gemini CLI â†’](03-gemini-cli.md)\n\n# docs/03-gemini-cli.md\n\n# Gemini CLI Complete Guide\n\n**Video Timestamp:** 1:27-4:14\n\nGemini CLI is Google's terminal AI tool. It's **FREE** (with generous limits) and perfect for getting started with terminal AI.\n\n## Why Start with Gemini CLI?\n\nChuck's reasoning:\n\n> \"We're diving straight into Gemini CLI first. Why? Because it has a very generous free tier. That's right, you heard it - FREE.\"\n\n**Best for:**\n\n- âœ… Getting started (no credit card required)\n- âœ… Research and web searches\n- âœ… File creation and manipulation\n- âœ… Learning context file workflows\n- âœ… Writing and content creation\n\n## Installation\n\n### Linux / WSL / macOS\n\n```bash\n# Install globally with npm\nnpm install -g @google/generative-ai-cli\n```\n\n**Permission error?** Run with sudo:\n\n```bash\nsudo npm install -g @google/generative-ai-cli\n```\n\n### macOS (Alternative with Homebrew)\n\n```bash\nbrew install gemini-cli\n```\n\n### Verify Installation\n\n```bash\ngemini --version\n```\n\n## First Launch\n\n### 1. Create a Project Directory\n\nChuck's approach from the video:\n\n```bash\n# Create a new directory for your project\nmkdir coffee-project\ncd coffee-project\n\n# Launch Gemini\ngemini\n```\n\n**Why create a directory first?**\n\n- Gemini can read/write files in the current directory\n- Keeps your work organized\n- Context files will be saved here\n\n### 2. Initial Setup\n\nFirst time you run `gemini`:\n\n1. **Sign in with Google account** - Opens browser automatically\n2. **Authorize the CLI** - Click \"Allow\"\n3. **Return to terminal** - You're logged in!\n\n```\n     _____                 _       _    ____ _     ___\n    / ____|               (_)     (_)  / ___| |   |_ _|\n   | |  __  ___ _ __ ___  _ _ __  _  | |   | |    | |\n   | | |_ |/ _ \\ '_ ` _ \\| | '_ \\| | | |   | |    | |\n   | |__| |  __/ | | | | | | | | | | | |___| |___ | |\n    \\_____|\\___|_| |_| |_|_|_| |_|_|  \\____|_____|___|\n\n   Welcome to Gemini CLI!\n```\n\n## Basic Usage\n\n### Your First Question\n\n```bash\n# Just start typing after the prompt\n> How do I make the best cup of coffee in the world?\n```\n\n**What happens:**\n\n1. Gemini searches the web (if relevant)\n2. \"Herding digital cats...\" (loading message)\n3. Response appears with formatting\n\n### Key Interface Elements\n\n```\n> Your question here\n\nHerding digital cats... ðŸ± (processing)\nCrafting the guide... âœ¨ (generating response)\n\n[Response appears]\n\n99% context left\n```\n\n**Context indicator:** Shows how much of your conversation window remains\n\n## The Superpower: File System Access\n\n### Creating Files\n\nChuck's demo from the video:\n\n```bash\n> I really want you to find the best way to make coffee.\n  Research the top 10 sites, only reputable sources,\n  and then compile the results into a document named best-coffee-method.\n  And then create me a blog plan, just an outline.\n```\n\n**Gemini will ask:**\n\n```\nðŸ“ Do you want me to create a file for you? (y/n)\n```\n\nType `y` and hit enter.\n\n**Result:**\n\n```\nCreated files:\n- best-coffee-method.md\n- coffee-blog-outline.md\n```\n\n### Reading Files\n\nGemini automatically reads files in your current directory when relevant.\n\n```bash\n> What files are in this directory?\n> Read the coffee blog outline and suggest improvements\n> Add a new section to best-coffee-method.md about water temperature\n```\n\n## The Game-Changer: Context Files\n\n### The `/init` Command\n\n**Video Timestamp:** 4:00-4:14\n\nThis is THE feature that changes everything:\n\n```bash\n> /init\n```\n\n**What it does:**\n\n1. Analyzes your current directory\n2. Reads all files in the project\n3. Creates a `gemini.md` context file\n4. Saves project understanding for future sessions\n\n**Gemini asks:**\n\n```\nðŸ“ Create Gemini.md context file? (y/n)\n```\n\nSay yes!\n\n### What's in gemini.md?\n\n```bash\n# View your context file\ncat gemini.md\n```\n\n**Example content:**\n\n```markdown\n# Project: Coffee Blog Series\n\n## Overview\n\nThis project involves researching and creating content about coffee brewing methods.\n\n## Current Files\n\n- best-coffee-method.md: Research compilation\n- coffee-blog-outline.md: Blog series outline\n\n## Project Goals\n\n- Create comprehensive coffee brewing guide\n- Develop blog series structure\n```\n\n### Using Context Across Sessions\n\n**The magic moment from the video:**\n\n1. Close your Gemini session (Ctrl+C or type `exit`)\n2. Open a NEW Gemini session: `gemini`\n3. Notice it loads `gemini.md` automatically\n\n```\nLoading context from gemini.md... âœ“\n100% context left (fresh session)\n```\n\nNow try:\n\n```bash\n> Write the intro for blog post 1 in the coffee series\n```\n\n**No additional context needed!** It knows what you're working on.\n\nChuck's reaction:\n\n> \"I didn't give it ANY context. It just knew. This is a new chat session.\"\n\n## Real-World Workflow (from the video)\n\n### Chuck's Actual Video Project\n\n**Video Timestamp:** 5:48-6:09\n\n```bash\n# Navigate to video project\ncd ~/Projects/531-ai-terminal\n\n# Launch Gemini\ngemini\n\n# It loads the context file automatically\n# Ask about project status\n> Where are we at in the project?\n```\n\n**Gemini responds with:**\n\n- Current phase\n- Completed tasks\n- Next steps\n- Referenced documents\n\n### Updating Context\n\n```bash\n> Update the gemini.md file with:\n  - We completed the coffee brewing research\n  - Next step is writing the first blog post\n  - Decision made: Focus on pour-over method first\n```\n\nGemini updates the file. Next session? It remembers everything.\n\n## Available Commands\n\n### View All Tools\n\n```bash\n> /tools\n```\n\n**Shows capabilities:**\n\n- Web search\n- File read/write\n- Code execution\n- Data analysis\n\n### Common Commands\n\n```bash\n> /init           # Create context file\n> /tools          # Show available tools\n> /help           # Show help\n> exit            # Exit Gemini (or Ctrl+C)\n```\n\n## Context Window Management\n\n### What is Context?\n\nEvery AI has a \"context window\" - how much conversation it can remember.\n\n**Browser AI:** Hides this from you (you hit limits unexpectedly)\n**Gemini CLI:** Shows you exactly where you're at\n\n```\n99% context left  â† Plenty of room\n50% context left  â† Halfway through\n10% context left  â† Start new session or summarize\n```\n\n### When Context Gets Low\n\n**Option 1:** Start a new session\n\n```bash\n# Exit current session\nexit\n\n# Start fresh\ngemini\n# Context file loads automatically!\n```\n\n**Option 2:** Ask Gemini to update context file\n\n```bash\n> Summarize our conversation and update gemini.md with key decisions\n```\n\n## Tips from Chuck\n\n### 1. One Directory = One Project\n\n```bash\n# Good: Separate projects\n~/coffee-project/      â†’ One Gemini session\n~/video-script/        â†’ Another Gemini session\n~/homelab-docs/        â†’ Another Gemini session\n```\n\n### 2. Let Gemini Create Your Context File\n\nDon't write `gemini.md` manually - let `/init` analyze your project.\n\n### 3. Update Context as You Work\n\n```bash\n> Add to gemini.md: We decided to use the French press method instead\n```\n\n### 4. Context Files = Your Project Memory\n\nThink of `gemini.md` as your project's brain:\n\n- Current state\n- Decisions made\n- Files to reference\n- Next steps\n\n## Advanced: Multiple Gemini Sessions\n\n**From the video:** Chuck opens multiple terminal tabs with different Gemini sessions.\n\n```bash\n# Terminal Tab 1: Coffee project\ncd ~/coffee-project\ngemini\n\n# Terminal Tab 2: Video project\ncd ~/video-script\ngemini\n\n# Terminal Tab 3: Homelab docs\ncd ~/homelab-docs\ngemini\n```\n\nEach session loads its own context file - no mixing!\n\n## Example Workflows\n\n### Research Workflow\n\n```bash\nmkdir research-project\ncd research-project\ngemini\n\n> Research the top 5 enterprise NAS solutions for small business.\n  Include pricing, features, and pros/cons.\n  Create a comparison document called nas-comparison.md\n\n> /init\n\n> Based on the research, write a recommendation for a 10-person company\n  with 5TB storage needs. Save as nas-recommendation.md\n```\n\n### Writing Workflow\n\n```bash\nmkdir blog-series\ncd blog-series\ngemini\n\n> Help me plan a 5-part blog series about network security basics.\n  Create an outline file.\n\n> /init\n\n> Write the introduction for part 1. Save as part-1-intro.md\n\n# Later (new session):\ngemini\n\n> Review the part-1-intro and suggest improvements\n```\n\n### Obsidian Integration\n\n**As mentioned in the video:**\n\n```bash\n# Navigate to your Obsidian vault\ncd ~/Obsidian/MyVault\ngemini\n\n> Read my daily note for today and summarize key tasks\n\n> Create a new note about [topic] with backlinks to related notes\n```\n\nGemini can access ALL your Obsidian notes because they're just markdown files!\n\n## Troubleshooting\n\n### \"Permission Denied\" on Install\n\n```bash\n# Use sudo\nsudo npm install -g @google/generative-ai-cli\n```\n\n### \"Command not found: gemini\"\n\n```bash\n# Reload your shell\nsource ~/.bashrc  # or ~/.zshrc\n\n# Or close and reopen terminal\n```\n\n### Context File Not Loading\n\n```bash\n# Make sure you're in the right directory\npwd\nls gemini.md\n\n# Recreate if needed\n> /init\n```\n\n### Web Search Not Working\n\nGemini needs internet access. Check your connection.\n\n## Pricing & Limits\n\n### Free Tier\n\n- **Generous usage limits** (exact limits vary)\n- **Gemini 2.5 Pro model** (latest and greatest!)\n- **Web search included**\n- **No credit card required**\n\n### Google One AI Premium ($20/mo)\n\n- Higher rate limits\n- Priority access\n- Integrated with other Google services\n\n**Chuck's take:**\n\n> \"Everyone has a Google account, and yes, this can be a free regular Gmail account.\"\n\n## What's Next?\n\nNow that you understand Gemini CLI, you're ready for the big leagues:\n\nâž¡ï¸ [Claude Code Guide](04-claude-code.md) - Chuck's daily driver with AI agents\n\nOr explore:\n\n- [Context Files Deep Dive](07-context-files.md)\n- [Productivity Workflows](11-productivity-workflows.md)\n\n---\n\n[â† Back to Prerequisites](01-prerequisites.md) | [Next: Claude Code â†’](04-claude-code.md)\n\n# docs/04-claude-code.md\n\n# Claude Code Complete Guide\n\n**Video Timestamp:** 8:44-14:26\n\nClaude Code is Anthropic's terminal AI tool - Chuck's daily driver. It's the most powerful tool covered in the video.\n\n## Why Claude Code?\n\nChuck's endorsement:\n\n> \"I use Claude Code for pretty much everything. It's my default. And here's why: it has a feature that changes the game - **agents**.\"\n\n> \"If you can only pay for one AI subscription, Claude Pro is the one I would choose, especially for the last feature I'm going to show you.\" _(Output Styles)_\n\n**Best for:**\n\n- âœ… Professional workflows with AI agents\n- âœ… Complex multi-step tasks\n- âœ… Custom personalities (Output Styles)\n- âœ… Planning mode for strategic thinking\n- âœ… Maximum control and customization\n\n**Requires:** Claude Pro subscription ($20/mo)\n\n## Installation\n\n### All Platforms (npm)\n\n```bash\n# Install globally\nnpm install -g @anthropic-ai/claude-code\n\n# Or with sudo if needed\nsudo npm install -g @anthropic-ai/claude-code\n```\n\n### Verify Installation\n\n```bash\nclaude --version\n```\n\n## First Launch & Setup\n\n### Basic Launch\n\n```bash\n# Navigate to your project\ncd coffee-project\n\n# Launch Claude Code\nclaude\n```\n\n### Initial Login\n\nFirst time:\n\n1. Prompted to login with Claude Pro account\n2. Opens browser for authentication\n3. Select directory permissions (approve access to current folder)\n\n**Permission prompt:**\n\n```\nðŸ“ Allow Claude Code to access /Users/you/coffee-project? (y/n)\n```\n\n**Chuck's take:** \"It's security first. It asks permission for most things, and that's good.\"\n\n## Interface Overview\n\n### TUI (Terminal User Interface)\n\n```\nâ”Œâ”€ Claude Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ðŸ’­ Thinking...                                      â”‚\nâ”‚                                                     â”‚\nâ”‚ [Your question here]                                â”‚\nâ”‚                                                     â”‚\nâ”‚ [Claude's response]                                 â”‚\nâ”‚                                                     â”‚\nâ”‚ Context: 42% used (85,234 tokens)                  â”‚\nâ”‚ Mode: Normal | Thinking: ON                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Toggle Thinking Mode\n\nPress **TAB** to turn thinking on/off:\n\n```\nThinking: OFF  â†’  Press TAB  â†’  Thinking: ON\n```\n\n**Thinking mode:** See Claude's internal reasoning process\n\n## Context Files: claude.md\n\n### Initialize Context\n\n```bash\n> /init\n```\n\nSame concept as Gemini, but creates `claude.md`:\n\n```markdown\n# Project: Coffee Project\n\n## Overview\n\n[Claude analyzes your directory and creates context]\n\n## Files\n\n- best-coffee-method.md\n- coffee-blog-outline.md\n\n## Goals\n\n[Project objectives]\n```\n\n### View Context Usage\n\n```bash\n> /context\n```\n\n**Shows detailed breakdown:**\n\n```\nContext Usage: 85,234 tokens (42% used)\n\nLoaded Files:\n- claude.md (1,234 tokens)\n- best-coffee-method.md (2,456 tokens)\n- coffee-blog-outline.md (890 tokens)\n\nConversation: 80,654 tokens\n```\n\n**Chuck's observation:**\n\n> \"With Claude, that doesn't really matter too much as long as you know how to use their most powerful feature: **agents**.\"\n\n## ðŸš€ Agents: The Game-Changer\n\n### What Are Agents?\n\n**From the video:**\n\n> \"Claude was like, 'Cool, I've got a task, but it's not for me. I'm gonna delegate this task to one of my employees or coworkers.' This is another Claude instance... He's giving him a fresh set of instructions and get this: a **fresh context window**.\"\n\n**Key concept:** Agents are separate Claude instances with:\n\n- âœ… Fresh context window (200K tokens each)\n- âœ… Specialized instructions\n- âœ… Custom tool access\n- âœ… Independent memory\n\n### Create Your First Agent\n\n**Video Timestamp:** 10:41-11:20\n\n```bash\n> /agents\n```\n\n**Menu appears:**\n\n```\nâ”Œâ”€ Agent Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 1. Create new agent                        â”‚\nâ”‚ 2. View agents                             â”‚\nâ”‚ 3. Edit agent                              â”‚\nâ”‚ 4. Delete agent                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n#### Step-by-Step Agent Creation\n\n**From Chuck's demo:**\n\n1. **Select \"Create new agent\"**\n\n2. **Choose scope:**\n\n   ```\n   ðŸ“ Project-specific agent (coffee-project)\n   ðŸŒ Personal agent (available everywhere)\n   ```\n\n   Chuck chooses: \"Just this project\"\n\n3. **Describe the agent:**\n\n   ```\n   Name: homelab-guru\n   Description: Expert in homelab hardware, networking, and infrastructure\n   ```\n\n4. **Configure:**\n\n   ```\n   Tools: [x] All tools\n   Model: Sonnet 4.5\n   Color: Auto\n   ```\n\n5. **Press Enter to save, ESC to exit**\n\n**Agent created!** ðŸŽ‰\n\n### Using Agents\n\n#### Deploy an Agent\n\n**Chuck's example:**\n\n```bash\n> @homelab-guru\n  Research document and create a buying guide.\n  Make sure you reference the research we made in @nas-rec-folder\n```\n\n**What happens:**\n\n1. Main Claude sees the task\n2. Delegates to `homelab-guru` agent\n3. Agent gets fresh 200K context window\n4. Agent works independently\n5. Returns results to main Claude\n\n**Visual in terminal:**\n\n```\nâ”Œâ”€ Agent: homelab-guru â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ðŸ” Researching NAS solutions...             â”‚\nâ”‚ ðŸ“Š Context: 15% used (30,000 tokens)       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n#### Reference Files with @\n\n```bash\n# Reference specific files\n> @homelab-guru create a summary of @nas-comparison.md\n\n# Reference folders\n> @homelab-guru review all documents in @research-folder\n```\n\n### Multiple Agents Simultaneously\n\n**Video Timestamp:** 13:50-14:04\n\n**Chuck's incredible demo:**\n\n```bash\n> Launch @homelab-guru to research the best proxmox servers.\n  At the same time, use a general agent to find the best pizza place in Dallas.\n  And another @homelab-guru to find the best graphics card for gaming.\n  Put it all together in a comprehensive report.\n```\n\n**What happens:**\n\n- ðŸ¤– Agent 1: Proxmox server research\n- ðŸ• Agent 2: Pizza recommendations\n- ðŸŽ® Agent 3: Graphics card research\n- ðŸ“Š Main Claude: Compiles all results\n\n**Chuck's reaction:**\n\n> \"I feel so powerful right now. This is so fun!\"\n\n### Pre-Built Agents from the Video\n\n#### 1. Homelab Guru\n\n```bash\nAgent: homelab-guru\nPurpose: Network equipment, server recommendations, homelab setup\nTools: All\nModel: Sonnet\n```\n\n#### 2. Brutal Critic\n\n```bash\nAgent: brutal-critic\nPurpose: Ruthlessly review scripts/outlines against NetworkChuck framework\nPersonality: Intentionally harsh, framework-focused\nTools: Read, Web Search\nModel: Sonnet\n```\n\n**Chuck's use case:**\n\n> \"I want it to be super mean. So that when it DID tell me I did a good job, I knew it was good.\"\n\n#### 3. Gemini Research Agent\n\n```bash\nAgent: gemini-research\nPurpose: Use Gemini CLI in headless mode for research tasks\nTools: Bash (to run gemini -p)\nModel: Sonnet\n```\n\n**Chuck's example:**\n\n```bash\n> @gemini-research find the best AI terminal videos on YouTube\n```\n\nAgent runs:\n\n```bash\ngemini -p \"find the top 10 AI terminal videos on YouTube\"\n```\n\n## ðŸŽ¨ Output Styles: Custom Personalities\n\n**Video Timestamp:** 16:31-17:27\n\n**Chuck's discovery:**\n\n> \"I'm embarrassed to say I just found this out while making this video.\"\n\n### What Are Output Styles?\n\n**System prompts** that control:\n\n- How Claude responds\n- Persona and tone\n- Domain expertise\n- Task-specific behaviors\n\n### View Output Styles\n\n```bash\n> /output-style\n```\n\n**Default styles:**\n\n```\nðŸ“‹ Available Output Styles:\n- code (default) - Optimized for software development\n- concise - Brief, to-the-point responses\n- detailed - Comprehensive explanations\n```\n\n### Create Custom Output Style\n\n**Chuck's demo:**\n\n```bash\n> /output-style new\n```\n\n**Prompt:**\n\n```\nName: homelab-expert\nDescription: You are a homelab expert designed to help me create\nthe best homelab possible.\n```\n\n**More complex example (Chuck's actual script-writing style):**\n\n```bash\nName: networkchuck-scriptwriter\nDescription:\nYou are an AI assistant specialized in writing NetworkChuck YouTube scripts.\nYou understand:\n- Hook psychology and CTR optimization\n- Viewer retention patterns\n- NetworkChuck's energetic, coffee-fueled voice\n- Educational entertainment balance\n- The \"you\" voice (viewer as hero)\n\nAlways:\n- Keep lines short and punchy\n- Add coffee transitions between segments\n- Use \"Let's go!\" at key moments\n- Explain complex topics simply\n- Add pattern breaks every 20-40 seconds\n```\n\n### Activate Output Style\n\n```bash\n> /output-style\n\n# Select from list, or it activates on next launch\n```\n\n**Verify:**\n\n```\nCurrent Output Style: networkchuck-scriptwriter âœ“\n```\n\n**Chuck's actual usage:**\n\n> \"I'm using the output style right now to make this video. This is what it looks like. It's pretty intense, optimized for what I do.\"\n\n### Scope: Project vs Global\n\n**Project-specific:**\n\n- Lives in `.claude/output-styles/` in current project\n- Only available in this project\n\n**Global:**\n\n- Lives in `~/.config/claude/output-styles/`\n- Available in all projects\n\n## ðŸŽ¯ Planning Mode\n\n**Video Timestamp:** 17:39-17:54\n\n### Activate Planning Mode\n\nPress **Shift+Tab** to toggle:\n\n```\nMode: Normal  â†’  Shift+Tab  â†’  Mode: Planning\n```\n\n### How It Works\n\n1. You give Claude a task\n2. Claude creates a detailed plan\n3. You review and approve\n4. Claude executes the plan\n\n**Example:**\n\n```bash\n> Refactor the authentication system to use JWT tokens\n```\n\n**Planning mode response:**\n\n```\nðŸ“‹ Plan:\n1. Review current authentication implementation\n2. Install jsonwebtoken package\n3. Create JWT utility functions\n4. Update login endpoint\n5. Add token verification middleware\n6. Update protected routes\n7. Add token refresh logic\n8. Update tests\n\nApprove this plan? (y/n/edit)\n```\n\nType `y` to execute, or `edit` to modify.\n\n**Chuck's take:**\n\n> \"This will put a very well thought-out plan together, and then you approve it. And then it just does it.\"\n\n## ðŸŽ® Advanced Features\n\n### Resume Previous Session\n\n**From the video:** \"Yes, you can do that.\"\n\n```bash\n# Resume last session\nclaude -r\n\n# Choose from recent sessions\n```\n\n### Dangerous Mode (Skip Permissions)\n\n**Video Timestamp:** 14:36-14:52\n\n```bash\n# Launch without permission prompts\nclaude --dangerously-skip-permissions\n```\n\nâš ï¸ **Warning:** Claude will execute actions without asking\n\n**Chuck's take:**\n\n> \"This is Claude without training wheels.\"\n\n**Use when:**\n\n- You trust your instructions completely\n- You want maximum speed\n- You're doing repetitive tasks\n\n### Combine Flags\n\n```bash\n# Resume previous session + dangerous mode\nclaude -r --dangerously-skip-permissions\n```\n\n## Real-World Workflows\n\n### 1. Outline Review with Brutal Critic\n\n**Video Timestamp:** 12:56-13:05\n\n```bash\n# Working on a YouTube script\n> @brutal-critic review my outline at @outline.md\n```\n\n**Agent launches with fresh context:**\n\n- Reads outline.md\n- Applies NetworkChuck framework\n- Returns ruthless critique\n\n**Chuck's result:** \"8.2/10 - Not bad!\"\n\n### 2. Cross-Tool Research\n\n**Video Timestamp:** 16:06-16:17\n\n```bash\n> Find the best AI terminal videos on YouTube.\n  Use the @gemini-research agent.\n```\n\n**What happens:**\n\n- Claude deploys gemini-research agent\n- Agent runs: `gemini -p \"search YouTube for AI terminal videos\"`\n- Gemini returns top 10 results\n- Claude compiles into report\n\n**Chuck's observation:**\n\n> \"We're having an AI use an AI right now!\"\n\n### 3. Context-Protected Reviews\n\n**Video Timestamp:** 12:27-12:35\n\n**Problem:** Your main conversation is 85K tokens deep with the outline\n\n**Solution:** Deploy fresh agent with 200K tokens\n\n```bash\n> @brutal-critic review my current work\n```\n\n**Why this matters:**\n\n- Main context: 85K tokens (cluttered with iterations)\n- Agent context: 0K tokens (fresh eyes)\n- No bias from previous conversation\n\n**Chuck's take:**\n\n> \"I want a fresh cup of coffee. Ready to go. Fresh eyes.\"\n\n## Tips from Chuck\n\n### 1. Protect Your Context\n\n> \"I use agents all the time to **protect my context** and avoid any kind of weird bias.\"\n\n**Strategy:** Delegate reviews, research, and analysis to agents\n\n### 2. One Project = One Claude Session\n\n```bash\n# Terminal Tab 1: Video script\ncd ~/video-project\nclaude\n\n# Terminal Tab 2: Homelab docs\ncd ~/homelab-project\nclaude\n```\n\n### 3. Name Agents by Function\n\n**Good names:**\n\n- `homelab-guru`\n- `brutal-critic`\n- `research-assistant`\n- `code-reviewer`\n\n**Bad names:**\n\n- `agent1`\n- `test`\n- `bob`\n\n### 4. Give Agents Specific Instructions\n\n**Vague:**\n\n```\nYou are helpful.\n```\n\n**Specific:**\n\n```\nYou are a homelab expert specializing in enterprise NAS solutions.\nWhen making recommendations:\n- Consider budget constraints\n- Explain technical trade-offs\n- Provide specific product recommendations\n- Include pricing and availability\n```\n\n## Agent Management\n\n### List All Agents\n\n```bash\n> /agents\n```\n\n**View:**\n\n- Project agents (local to current directory)\n- Personal agents (available everywhere)\n\n### Edit an Agent\n\n```bash\n> /agents\n# Select \"Edit agent\"\n# Choose agent\n# Modify instructions\n```\n\n### Delete an Agent\n\n```bash\n> /agents\n# Select \"Delete agent\"\n# Confirm\n```\n\n## Hidden Features\n\n### Paste Images\n\n**From the video:**\n\n> \"You can paste images into your terminal.\"\n\n```bash\n# In Claude Code session\n> Analyze this screenshot\n[Paste image]\n```\n\n### Custom Hooks\n\n**From the video mention:**\n\n> \"They have prompts, hooks, custom status lines.\"\n\nAdvanced: Create event-triggered actions\n\n### Status Line Customization\n\nCustomize your terminal status bar with project info\n\n## Troubleshooting\n\n### \"Not authorized\" Error\n\nEnsure you have:\n\n1. Active Claude Pro subscription\n2. Logged in correctly: `claude auth login`\n\n### Agent Not Working\n\n```bash\n# Verify agent exists\n> /agents\n\n# Check agent configuration\n> /agents\n# Select \"View agents\"\n```\n\n### Context Not Loading\n\n```bash\n# Recreate context file\n> /init\n```\n\n### Permission Denied on Files\n\n```bash\n# Relaunch with directory approval\nclaude\n# Approve file access when prompted\n```\n\n## Pricing\n\n**Requires:** Claude Pro ($20/mo)\n\n**Includes:**\n\n- Access to Claude Code terminal tool\n- Use your existing web subscription\n- No separate API key needed\n- Unlimited-ish usage (fair use policy)\n\n**Chuck's recommendation:**\n\n> \"If you already pay for Claude Pro, which starts at like 20 bucks a month, you can log into the terminal with this subscription and use it. So yeah, you don't have to use API keys.\"\n\n## Comparison: Gemini vs Claude\n\n| Feature            | Gemini CLI      | Claude Code       |\n| ------------------ | --------------- | ----------------- |\n| **Price**          | Free            | $20/mo            |\n| **Agents**         | âŒ No           | âœ… Yes            |\n| **Output Styles**  | âŒ No           | âœ… Yes            |\n| **Planning Mode**  | âŒ No           | âœ… Yes            |\n| **Context Window** | 200K            | 200K (per agent!) |\n| **Best For**       | Getting started | Professional work |\n\n**Chuck's verdict:**\n\n> \"Gemini's not even close to the best one.\"\n\n## What's Next?\n\n**Master these features:**\n\n1. Create 2-3 specialized agents for your work\n2. Design a custom output style\n3. Practice delegating tasks to agents\n4. Try planning mode on complex tasks\n\n**Then explore:**\nâž¡ï¸ [Multi-Tool Workflow](08-multi-tool-workflow.md) - Use Claude + Gemini + Codex simultaneously\n\n---\n\n[â† Back to Gemini CLI](03-gemini-cli.md) | [Next: Codex â†’](05-codex.md)\n\n# docs/05-codex.md\n\n# Codex (ChatGPT CLI) Guide\n\n**Video mentions:** 18:07-18:28, 18:54-19:13\n\nCodex is OpenAI's terminal tool that brings ChatGPT to the command line.\n\n## Overview\n\n**Chuck's usage:**\n\n> \"I find ChatGPT is very good at analyzing things from a high view. Gemini and Claude are very good at the work, the deep work.\"\n\n**Best for:**\n\n- High-level analysis\n- Strategic thinking\n- Quality review\n- Different perspective from Claude/Gemini\n\n## Installation\n\n```bash\nnpm install -g @openai/codex-cli\n\n# Or follow official OpenAI documentation\n```\n\n## Basic Usage\n\n### Launch\n\n```bash\ncd your-project\ncodex\n```\n\n### Context File\n\nUses **agents.md** (same as opencode)\n\n```bash\n> /init\n```\n\nCreates `agents.md` in your project directory.\n\n## Chuck's Workflow\n\n**In multi-tool setup:**\n\n```bash\n# Terminal 1: Claude (writing)\nclaude\n> Write a hook for this video, authority angle\n\n# Terminal 2: Gemini (research)\ngemini\n> Write a hook on discovery angle\n\n# Terminal 3: Codex (review)\ncodex\n> Review both hooks and compare their strengths\n```\n\n**Chuck's observation:**\n\n> \"They're all using the same context, different roles.\"\n\n## Role in Multi-Tool Workflow\n\n### When to Use Codex\n\n**âœ… Use Codex for:**\n\n- Reviewing Claude's output\n- High-level strategy\n- Comparing approaches\n- Ensuring clarity\n- Catching issues Claude/Gemini miss\n\n**âŒ Don't use Codex for:**\n\n- Deep technical writing (Claude better)\n- Current web research (Gemini better)\n- Long-form content creation\n\n### Typical Workflow\n\n```bash\n# 1. Claude creates\nclaude\n> Write a technical blog post about ZFS\n\n# 2. Gemini researches\ngemini\n> Verify technical accuracy and find recent benchmarks\n\n# 3. Codex reviews\ncodex\n> Review the blog post at blog.md\n  Check: clarity, flow, technical accuracy, audience fit\n```\n\n## Context File Syncing\n\n**Important:** Codex uses `agents.md`\n\n**Sync with other tools:**\n\n```bash\n# In Claude terminal\nclaude\n> Sync claude.md content to gemini.md and agents.md\n```\n\n**Now all three tools share context!**\n\n## Pricing\n\n**Requires:** ChatGPT Plus ($20/mo) or API key\n\n- ChatGPT Plus: Use existing subscription\n- API key: Pay per token usage\n\n## Comparison\n\n| Feature          | Codex     | Claude Code | Gemini CLI |\n| ---------------- | --------- | ----------- | ---------- |\n| **Strength**     | Analysis  | Deep work   | Research   |\n| **Context File** | agents.md | claude.md   | gemini.md  |\n| **Cost**         | $20/mo    | $20/mo      | Free       |\n| **Best For**     | Review    | Creating    | Research   |\n\n## Tips from Chuck\n\n### 1. Use for High-Level Analysis\n\n> \"ChatGPT is very good at analyzing things from a high view.\"\n\n**Good prompts:**\n\n```bash\n> Review this architecture and identify weaknesses\n> Is this explanation clear for beginners?\n> Compare these two approaches strategically\n```\n\n### 2. Last Step in Pipeline\n\n```\nClaude writes â†’ Gemini verifies â†’ Codex reviews\n```\n\n### 3. Different Perspective\n\nWhen Claude and Gemini agree, ask Codex for a third opinion:\n\n```bash\n> Claude and Gemini both recommend approach A.\n  What do you think? Any risks we're missing?\n```\n\n## Official Documentation\n\n[OpenAI Codex Documentation](https://platform.openai.com/docs/tools/codex)\n\n## What's Next?\n\n**Understand multi-tool workflows:**\n\nâž¡ï¸ [Multi-Tool Workflow Guide](08-multi-tool-workflow.md)\n\nâž¡ï¸ [Context Files Explained](07-context-files.md)\n\n---\n\n[â† Back to Claude Code](04-claude-code.md) | [Next: opencode â†’](06-opencode.md)\n\n# docs/06-opencode.md\n\n# opencode Complete Guide\n\n**Video Timestamp:** 26:32-30:00\n\nopencode is the **open-source** terminal AI tool that supports multiple providers and local models.\n\n## Why opencode?\n\n**Chuck's take:**\n\n> \"There's a tool that's actually open-source. You can use any model you want with this open-source alternative, and it might be the best tool of all of them. I'm still testing it.\"\n\n**Key advantages:**\n\n- âœ… **Open source** - Community-driven development\n- âœ… **Multiple providers** - Claude, OpenAI, Grok, Gemini, local models\n- âœ… **Grok free tier** - Free usage with X/Twitter integration\n- âœ… **Local models** - Run Ollama models completely offline\n- âœ… **Claude Pro login** - Use existing subscription (like Claude Code)\n- âœ… **Session sharing** - Share your AI sessions with others\n- âœ… **Timeline feature** - Time-travel through conversations\n\n**Best for:**\n\n- Experimentation with different models\n- Local/offline AI usage\n- Cost optimization (mix free + paid)\n- Open-source preference\n\n## Installation\n\n### Quick Install (Recommended)\n\n```bash\ncurl -fsSL https://opencode.sh/install.sh | sh\n```\n\n**Reload your shell:**\n\n```bash\nsource ~/.bashrc\n# or for zsh:\nsource ~/.zshrc\n```\n\n### Manual Install (npm)\n\n```bash\nnpm install -g @opencodenet/cli\n```\n\n### Verify Installation\n\n```bash\nopencode --version\n```\n\n## First Launch\n\n### Basic Launch\n\n```bash\ncd your-project\nopencode\n```\n\n**First time experience:**\n\n- Launches with **Grok Fast** model by default (FREE!)\n- Beautiful TUI interface\n- Reads current directory automatically\n\n### The Interface\n\n```\nâ”Œâ”€ opencode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                      â”‚\nâ”‚  ðŸš€ Welcome to opencode                              â”‚\nâ”‚  Model: grok-fast-1                                  â”‚\nâ”‚                                                      â”‚\nâ”‚  > Your prompt here                                  â”‚\nâ”‚                                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Chuck's reaction:**\n\n> \"Nice TUI, terminal user interface.\"\n\n## Free Tier: Grok Integration\n\n### What is Grok?\n\n**From the video:**\n\n- X/Twitter's AI model\n- Free tier available through opencode partnership\n- Fast inference\n- Good for general tasks\n\n### Using Grok (Default)\n\n**Just launch opencode:**\n\n```bash\nopencode\n```\n\n**Already on Grok Fast by default!**\n\n```bash\n> Help me plan a homelab project\n```\n\n**No API key needed!** Partnership with X provides free access.\n\n**Chuck's take:**\n\n> \"They have a deal with Grok AI that allows you to use this for free for a while.\"\n\n## Model Management\n\n### View Available Models\n\n```bash\n# In opencode session\n> /model\n```\n\n**Shows:**\n\n```\nAvailable Models:\n- grok-fast-1 (FREE - current)\n- claude-sonnet-4\n- claude-opus-4\n- gpt-4\n- gemini-2.5-pro\n- llama-3.2 (local via Ollama)\n```\n\n### Switch Models\n\n**Video Timestamp:** 27:57-28:21\n\n```bash\n> /model\n# Select from list\n\n# Or specify directly\n> /model claude-sonnet-4\n> /model grok-fast-1\n> /model llama-3.2\n```\n\n**Chuck switching live:**\n\n```bash\n> /model claude-sonnet-4\n# \"Cool, what's our next step?\"\n\n> /model grok-fast-1\n# \"While it's doing that, I can do /sessions\"\n```\n\n### Model Switching Mid-Conversation\n\n**The power move:**\n\n```bash\n# Start with Claude for deep thinking\n> /model claude-sonnet-4\n> Create a comprehensive system architecture\n\n# Switch to Grok for quick follow-up\n> /model grok-fast-1\n> Summarize that in bullet points\n```\n\n**Chuck's observation:**\n\n> \"I can switch models midway.\"\n\n## Provider Authentication\n\n### Login with Claude Pro\n\n**Video Timestamp:** 28:35-28:46\n\n```bash\nopencode auth login\n```\n\n**Select:** \"Anthropic\"\n\n**Browser opens:**\n\n1. Login with Claude Pro account\n2. Copy authorization code\n3. Paste in terminal\n\n**Now you have access to:**\n\n- Claude Sonnet 4.5\n- Claude Opus 4\n- Uses your existing subscription!\n\n**Chuck's endorsement:**\n\n> \"The fact that you can log in and use your Claude Pro subscription... that's next level.\"\n\n### Other Providers\n\n**OpenAI (ChatGPT):**\n\n```bash\nopencode auth login\n# Select: OpenAI\n# Enter API key\n```\n\n**Google (Gemini):**\n\n```bash\nopencode auth login\n# Select: Google\n# Authenticate with Google account\n```\n\n### Check Auth Status\n\n```bash\nopencode auth whoami\n```\n\n## Local Models with Ollama\n\n### Prerequisites\n\n**Install Ollama first:**\n\n```bash\n# macOS\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Windows (WSL)\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n### Pull a Model\n\n**Chuck uses Llama 3.2:**\n\n```bash\nollama pull llama-3.2\n```\n\n**Check available models:**\n\n```bash\nollama list\n```\n\n### Configure opencode for Local Models\n\n**Video Timestamp:** 27:40-28:01\n\n**Edit config:**\n\n```bash\nnano ~/.config/opencode/opencode.jsonc\n```\n\n**Add model configuration:**\n\n```jsonc\n{\n  \"model\": \"llama-3.2\",\n  \"provider\": \"ollama\"\n}\n```\n\n**Save and exit**\n\n### Use Local Model\n\n```bash\nopencode\n# Loads with llama-3.2\n\n# Or switch in session\n> /model llama-3.2\n```\n\n**Chuck trying it:**\n\n> \"Hey cool, Llama works!\"\n\n**Benefits:**\n\n- âœ… Completely offline\n- âœ… No API costs\n- âœ… Privacy (data never leaves machine)\n- âœ… Great for sensitive work\n\n## Advanced Features\n\n### Session Sharing\n\n**Video Timestamp:** 29:19-29:33\n\n**Share your conversation:**\n\n```bash\n> /share\n```\n\n**Returns:** URL copied to clipboard\n\n**Paste in browser:**\n\n```\nhttps://opencode.net/session/abc123...\n```\n\n**Chuck's amazement:**\n\n> \"I can share my session with people. That's pretty neat.\"\n\n**Live demo feature:**\n\n> \"Wait, is it live? Oh, you can share your session with people!\"\n\n### Session Timeline\n\n**Video Timestamp:** 29:33-29:44\n\n**Time-travel through conversation:**\n\n```bash\n> /timeline\n```\n\n**Shows:**\n\n```\nSession Timeline:\nâ”œâ”€ 10:23 - Started session\nâ”œâ”€ 10:25 - Asked about homelab setup\nâ”œâ”€ 10:28 - Created plan document\nâ”œâ”€ 10:30 - Switched to Llama 3.2\nâ””â”€ 10:32 - Generated cost analysis\n```\n\n**Select any point to restore:**\n\n```bash\n# Click on timestamp\n# Session rewinds to that point\n```\n\n**Chuck's reaction:**\n\n> \"We can jump back in time and restore. I want that in real life!\"\n\n### Session Management\n\n**View all sessions:**\n\n```bash\n> /sessions\n```\n\n**Shows:**\n\n```\nRecent Sessions:\n1. homelab-planning (active)\n2. blog-writing (1 hour ago)\n3. research-project (yesterday)\n```\n\n**Switch sessions:**\n\n```bash\n# Select from list\n# Or start new:\n> /sessions\n# Choose \"New session\"\n```\n\n### Headless Mode\n\n**Run opencode without TUI:**\n\n```bash\nopencode --headless \"Write a blog intro about ZFS\"\n```\n\n**Output goes directly to stdout**\n\n### Export Session\n\n**From video mention:**\n\n```bash\nopencode --export-session session-id\n```\n\n**Exports as JSON data**\n\n## Context Files: agents.md\n\n### Initialize Context\n\n```bash\n> /init\n```\n\n**Creates:** `agents.md` (not agent.md or opencode.md)\n\n**Why \"agents.md\"?**\n\n- opencode follows proposed standard\n- Claude Code's Codex uses agents.md\n- Trying to standardize across tools\n\n### Sync with Other Tools\n\n**When using opencode + Claude + Gemini:**\n\n```bash\n# Use Claude to sync all three\nclaude\n> Sync claude.md content to gemini.md and agents.md\n```\n\n**Chuck's workflow:**\n\n> \"They're trying to make it a standard. They're all the same.\"\n\n## Feature Showcase (from video)\n\n### 1. Agents Support\n\n**Video Timestamp:** 29:45-29:51\n\n```bash\nopencode agents create my-agent\n```\n\n**Similar to Claude Code agents**\n\n### 2. Headless Server\n\n```bash\nopencode server start\n```\n\n**Then attach from another terminal:**\n\n```bash\nopencode server attach\n```\n\n### 3. Session Export\n\n```bash\nopencode export --format json > session.json\n```\n\n### 4. Rich Formatting\n\n- Markdown rendering\n- Code syntax highlighting\n- Table support\n\n## Real-World Usage\n\n### Cost Optimization Strategy\n\n**Mix free and paid models:**\n\n```bash\n# Free: Grok for research\n> /model grok-fast-1\n> Research top 5 NAS options\n\n# Paid: Claude for writing\n> /model claude-sonnet-4\n> Write a comprehensive buying guide based on research\n\n# Free: Local for experimentation\n> /model llama-3.2\n> Generate 5 alternative titles\n```\n\n### Privacy-First Workflow\n\n**Sensitive work uses local models:**\n\n```bash\n# Switch to local\n> /model llama-3.2\n\n# Work on sensitive documents\n> Review this confidential file...\n\n# No data sent to cloud âœ“\n```\n\n### Model Comparison\n\n**Get multiple perspectives:**\n\n```bash\n# Ask Claude\n> /model claude-sonnet-4\n> What's the best homelab storage solution?\n\n# Save Claude's answer, then ask Grok\n> /model grok-fast-1\n> What's the best homelab storage solution?\n\n# Compare responses\n```\n\n## Chuck's Real Usage\n\n**From the video:**\n\n```bash\ncd ~/Projects/531-ai-terminal\nopencode\n\n# It loads agents.md automatically\n> Where are we in the project?\n\n# Grok responds with project status\n```\n\n**Then switches models:**\n\n```bash\n> /model claude-sonnet-4\n> Continue working on the script\n```\n\n**All in one session, same context!**\n\n## Configuration\n\n### Config File Location\n\n```bash\n~/.config/opencode/opencode.jsonc\n```\n\n### Example Configuration\n\n```jsonc\n{\n  \"model\": \"claude-sonnet-4\",\n  \"provider\": \"anthropic\",\n  \"theme\": \"dark\",\n  \"thinking\": true,\n  \"temperature\": 0.7,\n  \"maxTokens\": 4096\n}\n```\n\n### Edit Config\n\n```bash\nnano ~/.config/opencode/opencode.jsonc\n```\n\n## Command Reference\n\n### In-Session Commands\n\n```bash\n/model              # Change model\n/share              # Share session\n/timeline           # View timeline\n/sessions           # Manage sessions\n/init               # Create agents.md\n/help               # Show help\nexit                # Exit opencode\n```\n\n### CLI Commands\n\n```bash\nopencode                    # Launch\nopencode auth login         # Authenticate provider\nopencode auth whoami        # Check auth status\nopencode --version          # Version info\nopencode --headless \"...\"   # Headless mode\nopencode --help             # Help\n```\n\n## Troubleshooting\n\n### \"Command not found: opencode\"\n\n```bash\n# Reload shell\nsource ~/.bashrc\nsource ~/.zshrc\n\n# Or reinstall\ncurl -fsSL https://opencode.sh/install.sh | sh\n```\n\n### Local Model Not Working\n\n```bash\n# Verify Ollama is running\nollama list\n\n# Pull model if missing\nollama pull llama-3.2\n\n# Check config\ncat ~/.config/opencode/opencode.jsonc\n```\n\n### Authentication Issues\n\n```bash\n# Re-authenticate\nopencode auth login\n\n# Check status\nopencode auth whoami\n\n# Clear auth and retry\nrm -rf ~/.config/opencode/auth\nopencode auth login\n```\n\n### Context File Not Loading\n\n```bash\n# Verify file exists\nls agents.md\n\n# Recreate\n> /init\n```\n\n## Comparison: opencode vs Others\n\n| Feature                | opencode    | Claude Code    | Gemini CLI     |\n| ---------------------- | ----------- | -------------- | -------------- |\n| **Cost**               | Free (Grok) | $20/mo         | Free           |\n| **Local Models**       | âœ… Yes      | âŒ No          | âŒ No          |\n| **Multiple Providers** | âœ… Yes      | âŒ Claude only | âŒ Gemini only |\n| **Session Sharing**    | âœ… Yes      | âŒ No          | âŒ No          |\n| **Timeline Feature**   | âœ… Yes      | âŒ No          | âŒ No          |\n| **Agents**             | âœ… Yes      | âœ… Yes         | âŒ No          |\n| **Open Source**        | âœ… Yes      | âŒ No          | âŒ No          |\n\n**Chuck's verdict:**\n\n> \"It might be the best tool of all of them. I'm still testing it.\"\n\n## When to Use opencode\n\n**âœ… Choose opencode for:**\n\n- Experimentation with different models\n- Cost optimization (mix free/paid)\n- Privacy needs (local models)\n- Open-source preference\n- Model comparison workflows\n- Session sharing needs\n\n**âŒ Choose Claude Code instead for:**\n\n- Production workflows (more mature)\n- Complex agent setups\n- Output styles\n- Planning mode\n\n**âŒ Choose Gemini CLI instead for:**\n\n- Simplest setup\n- Pure Google ecosystem\n- Getting started (easiest learning curve)\n\n## The Developers\n\n**From Chuck's mention:**\n\n> \"What's fun is I've been following these guys on Twitter before they started making this code. This guy Dax, these guys are killing it.\"\n\n**GitHub:** [stackblitz-labs/opencode](https://github.com/stackblitz-labs/opencode)\n\n**Community:** Active development, responsive maintainers\n\n## Future Potential\n\n**Why Chuck is excited:**\n\n1. **Open source** â†’ Community contributions\n2. **Multi-provider** â†’ Use best model for each task\n3. **Local models** â†’ Privacy + cost control\n4. **Standards push** â†’ agents.md adoption\n5. **Feature velocity** â†’ Rapid development\n\n**Chuck's strategy:**\n\n> \"If a new, greater, better AI comes out, I'm ready for it.\"\n\nopencode enables this with provider flexibility.\n\n## What's Next?\n\n**Get started with opencode:**\n\n1. Install it (2 minutes)\n2. Try Grok free tier (no auth needed)\n3. Experiment with model switching\n4. Try local models if privacy-conscious\n5. Use for cost-optimized workflows\n\n**Then explore:**\nâž¡ï¸ [Multi-Tool Workflow](08-multi-tool-workflow.md) - Use opencode with Claude/Gemini\n\nâž¡ï¸ [Command Cheat Sheet](14-cheat-sheet.md) - Quick opencode commands\n\n---\n\n[â† Back to Codex](05-codex.md) | [Next: Context Files â†’](07-context-files.md)\n\n# docs/07-context-files.md\n\n# Context Files Explained\n\n**The Secret Weapon of Terminal AI**\n\nContext files are THE feature that makes terminal AI 10x better than browser AI. This guide explains everything.\n\n## The Browser Problem\n\n**Chuck's frustration:**\n\n> \"You're in the browser. You're asking questions, research mode. You're diving deep into a project. Can't even see your scroll bar anymore. And this is your fifth chat because ChatGPT lost its context or its mind.\"\n\n**What goes wrong:**\n\n- ðŸ“œ Infinite scrolling (lose track of conversation)\n- ðŸ—‚ï¸ Multiple scattered chats (context split across 20 tabs)\n- ðŸ“‹ Copy/paste chaos (trying to save important parts)\n- ðŸ”„ Re-explaining context every new chat\n- ðŸ’¾ No way to \"save\" your project state\n\n## The Terminal Solution: Context Files\n\n### What Are Context Files?\n\n**Simple answer:** Markdown files that tell AI what your project is about.\n\n**Each tool has its own:**\n\n- Gemini CLI: `gemini.md`\n- Claude Code: `claude.md`\n- Codex: `agents.md`\n\n### The Magic\n\n**Every time you launch the AI in a directory:**\n\n1. Tool looks for its context file\n2. Loads it automatically\n3. Immediately understands your project\n4. No re-explaining needed!\n\n**Chuck's aha moment:**\n\n> \"It can access your Obsidian vault, all your notes, because those are just files sitting there on your hard drive.\"\n\n## How Context Files Work\n\n### Visual Representation\n\n```\nmy-project/\nâ”œâ”€â”€ gemini.md          â† Gemini reads this\nâ”œâ”€â”€ claude.md          â† Claude reads this\nâ”œâ”€â”€ agents.md          â† Codex reads this\nâ”œâ”€â”€ project-files/\nâ”‚   â”œâ”€â”€ research.md\nâ”‚   â”œâ”€â”€ outline.md\nâ”‚   â””â”€â”€ draft.md\n```\n\n**When you launch:**\n\n```bash\ncd my-project\ngemini\n\n# Loading context from gemini.md... âœ“\n```\n\n### Anatomy of a Context File\n\n**Example `claude.md`:**\n\n```markdown\n# Project: Coffee Blog Series\n\n## Overview\n\nCreating a comprehensive blog series about coffee brewing methods,\ntargeted at home coffee enthusiasts.\n\n## Current Phase\n\nResearch complete, writing first draft\n\n## Key Files\n\n- research/coffee-methods.md - Compiled research\n- outlines/series-outline.md - 5-part series structure\n- drafts/part-1.md - First draft (in progress)\n\n## Decisions Made\n\n- Focus on pour-over, French press, and espresso\n- Avoid super technical chemistry details\n- Include beginner-friendly equipment recommendations\n\n## Next Steps\n\n1. Complete part-1.md draft\n2. Get feedback on tone/style\n3. Create equipment recommendations list\n\n## Reference Documents\n\n- Brand guidelines in guidelines.md\n- Target audience research in audience-profile.md\n```\n\n### What to Include\n\n**âœ… DO include:**\n\n- Project overview\n- Current phase/status\n- Key files and their purpose\n- Major decisions made\n- Next steps\n- Links to reference documents\n\n**âŒ DON'T include:**\n\n- Full project content (just reference files)\n- Conversation history (let AI build this naturally)\n- Temporary notes (keep focused)\n\n## Creating Context Files\n\n### Method 1: Let AI Do It (Recommended)\n\n**Gemini CLI:**\n\n```bash\ncd your-project\ngemini\n> /init\n```\n\n**Claude Code:**\n\n```bash\ncd your-project\nclaude\n> /init\n```\n\n**What happens:**\n\n1. AI scans your directory\n2. Reads your files\n3. Asks clarifying questions\n4. Generates context file\n\n**Chuck's approach:**\n\n> \"Let's take a look at it. And while we didn't do much in this project, it knows what's going on.\"\n\n### Method 2: Manual Creation\n\n```bash\n# Create manually\nnano gemini.md\n\n# Or use your AI to help!\ngemini\n> Create a gemini.md file describing this project\n```\n\n## Using Context Files\n\n### Fresh Session = Full Context\n\n**The killer demo from the video:**\n\n1. **First session:**\n\n   ```bash\n   gemini\n   > Research coffee brewing methods and create a blog outline\n   > /init\n   ```\n\n2. **Close and reopen (NEW session):**\n   ```bash\n   gemini\n   # Context loaded automatically\n   > Write the intro for blog post 1 in the coffee series\n   ```\n\n**Chuck's reaction:**\n\n> \"I didn't give it ANY context. It just knew. This is a new chat session.\"\n\n### Updating Context\n\n**As you work, update the file:**\n\n```bash\n> Update gemini.md to reflect:\n  - Research phase complete\n  - First draft started\n  - Decision: Focus on pour-over method first\n```\n\n**Or update manually:**\n\n```bash\nnano gemini.md\n# Edit and save\n```\n\n**Next session automatically loads the updates!**\n\n## Real-World Example: Chuck's Video Project\n\n**Video Timestamp:** 5:48-6:09\n\n### The Setup\n\n```bash\ncd ~/Projects/531-ai-terminal\nls\n```\n\n**Files present:**\n\n```\ngemini.md\nclaude.md\nagents.md\n531-script.md\nresearch/\n  â”œâ”€â”€ claude-code-research.md\n  â”œâ”€â”€ gemini-cli-research.md\n  â””â”€â”€ opencode-research.md\n```\n\n### The Workflow\n\n**Chuck opens a fresh Gemini session:**\n\n```bash\ngemini\n\n# Loads gemini.md automatically\n```\n\n**Asks with ZERO additional context:**\n\n```bash\n> Where are we at in the project?\n```\n\n**Gemini responds:**\n\n```\nBased on your project files:\n\nCurrent Status: Script writing phase (100% complete)\n- All segments finished (307 lines, ~17 min runtime)\n- Hook finalized\n- Ready for filming\n\nKey Files:\n- 531-script.md (complete script)\n- research/ folder (comprehensive research on all tools)\n- session-summary.md (session history)\n\nNext Steps:\n- Optional script polish\n- OR proceed to filming\n```\n\n**Chuck's amazement:**\n\n> \"Are you seeing this? This has completely changed the way I create or do anything now.\"\n\n## Multi-Tool Context Syncing\n\n**Video Timestamp:** 18:28-18:40\n\n### The Challenge\n\nYou want to use:\n\n- âœ… Gemini CLI\n- âœ… Claude Code\n- âœ… Codex\n\nAll on the SAME project... how do you keep context in sync?\n\n### Chuck's Solution\n\n**Two-step process:**\n\n#### Step 1: Same Directory\n\n```bash\n# All tools launched from same directory\ncd my-project\n\n# Terminal Tab 1\ngemini\n\n# Terminal Tab 2\nclaude\n\n# Terminal Tab 3\ncodex\n```\n\n#### Step 2: Sync Context Files\n\n**Make sure these files say the same thing:**\n\n- `gemini.md`\n- `claude.md`\n- `agents.md`\n\n**Chuck's method:**\n\n```bash\n# Use one AI to sync the others\nclaude\n\n> Read claude.md and update both gemini.md and agents.md\n  to match, ensuring all three context files are synchronized\n```\n\n**Result:**\n\n```\nmy-project/\nâ”œâ”€â”€ gemini.md    â† Same content\nâ”œâ”€â”€ claude.md    â† Same content\nâ”œâ”€â”€ agents.md    â† Same content\n```\n\n### Why This Works\n\n**From Chuck:**\n\n> \"Everything I'm doing, talking with these three different AIs on a project... It's not tied in a browser. It's not tied in a GUI. It's just this folder right here on my hard drive.\"\n\n**Each AI:**\n\n- Reads its own context file\n- Sees the same project state\n- Works on the same files\n- No copy/paste between tools!\n\n## Advanced Context Strategies\n\n### 1. Layered Context\n\n**Structure:**\n\n```\nproject/\nâ”œâ”€â”€ claude.md              â† Main context\nâ”œâ”€â”€ docs/\nâ”‚   â”œâ”€â”€ framework.md       â† Reference in claude.md\nâ”‚   â”œâ”€â”€ brand-guide.md     â† Reference in claude.md\nâ”‚   â””â”€â”€ audience.md        â† Reference in claude.md\n```\n\n**In claude.md:**\n\n```markdown\n## Reference Documents\n\nFor detailed guidelines, see:\n\n- docs/framework.md - Scriptwriting framework\n- docs/brand-guide.md - Brand voice guidelines\n- docs/audience.md - Target audience research\n```\n\n**AI reads main context, pulls in references as needed.**\n\n### 2. Session Summaries\n\n**Chuck uses an agent for this (from video):**\n\n```bash\n> @script-session-closer run\n\n# Agent does:\n# 1. Summarizes current session\n# 2. Updates context files (all three!)\n# 3. Updates session-summary.md\n# 4. Commits to git\n```\n\n**Result:** Next session picks up EXACTLY where you left off.\n\n### 3. Multiple Projects\n\n**Keep them separate:**\n\n```bash\n~/coffee-project/\n  â”œâ”€â”€ gemini.md      â† Coffee project context\n\n~/video-script/\n  â”œâ”€â”€ gemini.md      â† Video project context\n\n~/homelab-docs/\n  â”œâ”€â”€ gemini.md      â† Homelab context\n```\n\n**No cross-contamination!**\n\n### 4. Decision Log\n\n**Track major decisions in context:**\n\n```markdown\n## Decision Log\n\n### 2025-10-15: Brew Method Selection\n\n**Decision:** Focus on pour-over primarily\n**Reasoning:** Most beginner-friendly, popular, equipment accessible\n**Impact:** Affects equipment recommendations and tutorial structure\n\n### 2025-10-20: Series Length\n\n**Decision:** 5 parts instead of 3\n**Reasoning:** Too much content to compress, better to go deep\n**Impact:** Updated outline, adjusted timeline\n```\n\n## Context File Best Practices\n\n### âœ… DO\n\n1. **Update regularly** - After major work sessions\n2. **Be concise** - Don't dump entire project\n3. **Reference files** - Don't duplicate content\n4. **Track decisions** - Why you chose something matters\n5. **Sync across tools** - If using multiple AIs\n6. **Version control** - Commit to git\n\n### âŒ DON'T\n\n1. **Include sensitive data** - Context files are readable\n2. **Copy/paste everything** - Reference instead\n3. **Let it get stale** - Update as project evolves\n4. **Over-explain** - AI is smart, be concise\n5. **Forget to sync** - When using multiple tools\n\n## Context vs Conversation\n\n### Understanding the Difference\n\n**Context Window:**\n\n- Current conversation messages\n- Loaded files\n- Context file content\n- **Limited size** (200K tokens)\n\n**Context File:**\n\n- Project knowledge\n- Persistent across sessions\n- **Unlimited size** (within reason)\n- Your project's \"memory\"\n\n### Example\n\n**Session 1 (85K tokens used):**\n\n```\nConversation: 80K tokens\nContext file: 2K tokens\nLoaded files: 3K tokens\n```\n\n**Session 2 (NEW session, 2K tokens used):**\n\n```\nConversation: 0K tokens (fresh start!)\nContext file: 2K tokens (loaded automatically)\nLoaded files: 0K tokens (not loaded yet)\n```\n\n**The magic:** Context file gives you project knowledge WITHOUT burning conversation tokens.\n\n## The \"It's Just a Folder\" Philosophy\n\n**Chuck's most important point:**\n\n> \"I can copy and paste that folder anywhere. All the work, all the decisions, all the context - it's mine.\"\n\n### What This Means\n\n**Your project = A folder containing:**\n\n- Context files\n- Work products\n- Reference documents\n- Research\n- Drafts\n\n**No vendor lock-in:**\n\n- âœ… Works with any AI tool (that supports context files)\n- âœ… Stored locally (you own it)\n- âœ… Portable (copy to any machine)\n- âœ… Version controlled (use git)\n\n**Chuck's freedom:**\n\n> \"If a new, greater, better AI comes out, I'm ready for it because all my stuff is right here on my hard drive.\"\n\n## Troubleshooting\n\n### Context File Not Loading\n\n```bash\n# Check you're in right directory\npwd\nls *.md\n\n# Verify filename (case-sensitive)\nls gemini.md    # âœ“ Correct\nls Gemini.md    # âœ— Wrong\nls GEMINI.md    # âœ— Wrong\n\n# Recreate if needed\n> /init\n```\n\n### Context Seems Outdated\n\n```bash\n# AI might be caching old version\n> Reload gemini.md and re-read the project\n\n# Or restart session\nexit\ngemini\n```\n\n### Multiple Context Files Conflicting\n\n**When using multiple AIs, ensure synced:**\n\n```bash\n# Use one AI to sync\nclaude\n> Read claude.md and make gemini.md and agents.md identical\n```\n\n### Too Much Context\n\n**If context file gets huge (>5K words):**\n\n1. **Move details to separate docs**\n2. **Reference them in main context**\n3. **Keep main context concise**\n\n## Summary: Why Context Files Win\n\n| Browser AI                  | Terminal AI + Context Files |\n| --------------------------- | --------------------------- |\n| ðŸ“œ Infinite scroll hell     | ðŸ“ Clean context file       |\n| ðŸ”„ Re-explain every session | âœ… Auto-loads on launch     |\n| ðŸ—‚ï¸ 20 scattered chats       | ðŸ“‹ One project folder       |\n| ðŸ“‹ Copy/paste nightmare     | ðŸ”— Direct file access       |\n| ðŸ¢ Vendor lock-in           | ðŸ†“ You own everything       |\n\n**Chuck's verdict:**\n\n> \"I own my context. Nothing annoys me more than when ChatGPT tries to fence me in, give me vendor lock-in. No, I reject that.\"\n\n## What's Next?\n\n**Now that you understand context files:**\n\nâž¡ï¸ [Multi-Tool Workflow](08-multi-tool-workflow.md) - Use context files across multiple AIs\n\nâž¡ï¸ [Productivity Workflows](11-productivity-workflows.md) - Real examples using context files\n\n---\n\n[â† Back to Claude Code](04-claude-code.md) | [Next: Multi-Tool Workflow â†’](08-multi-tool-workflow.md)\n\n# docs/08-multi-tool-workflow.md\n\n# Multi-Tool Workflow Guide\n\n**Video Timestamp:** 18:03-19:25\n\n**The Ultimate Power Move:** Using Gemini CLI, Claude Code, and Codex simultaneously on the same project.\n\n## Why Use Multiple AI Tools?\n\n**Chuck's philosophy:**\n\n> \"I will use all AI. I'll use the best AI. No one can stop me.\"\n\n### Each AI Has Strengths\n\n**Gemini CLI:**\n\n- âœ… Fast web research\n- âœ… Current information (web search built-in)\n- âœ… Quick iterations\n\n**Claude Code:**\n\n- âœ… Deep analysis and planning\n- âœ… Long-form writing\n- âœ… Agents for specialized tasks\n\n**Codex (ChatGPT):**\n\n- âœ… High-level analysis\n- âœ… Strategic thinking\n- âœ… Different perspective\n\n### Chuck's Strategy\n\n**From the video:**\n\n> \"I find ChatGPT is very good at analyzing things from a high view. Gemini and Claude are very good at the work, the deep work.\"\n\n## The Setup: Two Simple Steps\n\n### Step 1: Same Directory\n\n**All AI tools must work from the SAME project folder:**\n\n```bash\ncd ~/my-project\n```\n\n**Open multiple terminal tabs/windows:**\n\n```bash\n# Terminal Tab 1: Claude\ncd ~/my-project\nclaude\n\n# Terminal Tab 2: Gemini\ncd ~/my-project\ngemini\n\n# Terminal Tab 3: Codex\ncd ~/my-project\ncodex\n```\n\n**Result:** All three AIs can access the same files!\n\n### Step 2: Sync Context Files\n\n**Ensure these files have identical content:**\n\n- `claude.md`\n- `gemini.md`\n- `agents.md` (for Codex)\n\n**Chuck's method:**\n\n```bash\n# In Claude terminal\n> Read claude.md and sync it to gemini.md and agents.md.\n  Make sure all three files have identical project context.\n```\n\n**Verification:**\n\n```bash\n# In project directory\ndiff claude.md gemini.md\ndiff claude.md agents.md\n\n# Should show: \"Files are identical\" or no output\n```\n\n## The Power: Parallel Workflows\n\n**Video Timestamp:** 18:45-19:01\n\n### Chuck's Live Demo\n\n**The command:**\n\n```bash\n# In Claude terminal\n> Write a hook for this video, authority angle.\n  Write it to authority-hook.md\n\n# In Gemini terminal\n> Write a hook on a discovery angle.\n  Write it to discovery-hook.md\n\n# In Codex terminal\n> Review both hooks and compare their strengths\n```\n\n**What happens:**\n\n- ðŸŽ¯ Claude: Writes authority-focused hook\n- ðŸ” Gemini: Writes discovery-focused hook\n- ðŸ“Š Codex: Analyzes and compares both\n\n**Chuck's observation:**\n\n> \"They're all using the same context, different roles. You have three different AIs working on the same thing at the same time. No copying and pasting. They can see each other's work.\"\n\n## Real-World Workflow Examples\n\n### Example 1: Content Creation\n\n**Scenario:** Writing a technical blog post\n\n```bash\n# TERMINAL 1: Claude (long-form writing)\nclaude\n> Write the introduction section for the ZFS storage blog post.\n  Save to sections/intro.md\n\n# TERMINAL 2: Gemini (research)\ngemini\n> Research the latest ZFS performance benchmarks.\n  Compile findings in research/zfs-benchmarks.md\n\n# TERMINAL 3: Codex (review)\ncodex\n> Review the intro in sections/intro.md and check if it aligns\n  with the benchmarks research. Suggest improvements.\n```\n\n**Result:**\n\n- Claude writes deep content\n- Gemini gathers current data\n- Codex ensures quality and alignment\n\n### Example 2: Homelab Planning\n\n**Scenario:** Designing a new homelab setup\n\n```bash\n# TERMINAL 1: Claude (planning)\nclaude\n> Create a detailed homelab architecture plan.\n  Include network diagram, hardware specs, and budget.\n  Save to homelab-plan.md\n\n# TERMINAL 2: Gemini (current prices/availability)\ngemini\n> Research current pricing for enterprise NAS systems.\n  Check availability of the hardware in the homelab plan.\n  Save to pricing-research.md\n\n# TERMINAL 3: Codex (risk analysis)\ncodex\n> Review homelab-plan.md and identify potential issues:\n  - Single points of failure\n  - Budget overruns\n  - Compatibility problems\n  Save analysis to risk-assessment.md\n```\n\n### Example 3: Video Script Writing (Chuck's Process)\n\n**Video Timestamp:** 18:41-19:25\n\n**Chuck's actual workflow:**\n\n```bash\n# TERMINAL 1: Claude with script-writing output style\nclaude\n> Continue working on Segment 3 of the AI Terminal script.\n  Reference the outline at outline.md\n\n# TERMINAL 2: Gemini with research focus\ngemini\n> Verify the technical accuracy of the Claude Code section.\n  Cross-check commands and features against official docs.\n\n# TERMINAL 3: Codex for high-level review\ncodex\n> Read the current script at script.md.\n  Evaluate narrative flow and retention strategy.\n  Does it deliver on the hook promise?\n```\n\n**Chuck's approach:**\n\n> \"I'm using all three right now to work on this video script.\"\n\n## File-Based Collaboration\n\n### How AIs \"See\" Each Other's Work\n\n**The secret:** Everything is just files!\n\n```\nmy-project/\nâ”œâ”€â”€ claude.md              â† Shared context\nâ”œâ”€â”€ gemini.md              â† Shared context\nâ”œâ”€â”€ agents.md              â† Shared context\nâ”œâ”€â”€ research/\nâ”‚   â”œâ”€â”€ topic-a.md        â† Gemini wrote this\nâ”‚   â””â”€â”€ topic-b.md        â† Gemini wrote this\nâ”œâ”€â”€ drafts/\nâ”‚   â”œâ”€â”€ section-1.md      â† Claude wrote this\nâ”‚   â””â”€â”€ section-2.md      â† Claude wrote this\nâ””â”€â”€ reviews/\n    â””â”€â”€ analysis.md       â† Codex wrote this\n```\n\n**When Claude asks:**\n\n```bash\n> Review the research in research/ folder\n```\n\n**Claude sees:**\n\n- âœ… Files Gemini created\n- âœ… Their exact content\n- âœ… Timestamps\n- âœ… Everything!\n\n**No copy/paste. No export/import. Just files.**\n\n## Specialized Roles Strategy\n\n### Assign Each AI a Role\n\n**Based on strengths from video:**\n\n#### Claude â†’ Deep Work\n\n```bash\n# Long-form writing\n# Complex planning\n# Agent deployment\n# Custom output styles\n```\n\n#### Gemini â†’ Research & Speed\n\n```bash\n# Web research\n# Fast iterations\n# Current information\n# Quick file creation\n```\n\n#### Codex â†’ Analysis & Review\n\n```bash\n# High-level strategy\n# Quality review\n# Competitive analysis\n# Meta-thinking\n```\n\n### Role Assignment in Practice\n\n**In your context files:**\n\n**claude.md:**\n\n```markdown\n# Project: Technical Blog Series\n\n## Claude's Role\n\nPrimary writer for long-form content.\n\n- Draft all blog posts\n- Create detailed technical explanations\n- Deploy agents for specialized sections\n```\n\n**gemini.md:**\n\n```markdown\n# Project: Technical Blog Series\n\n## Gemini's Role\n\nResearch and verification specialist.\n\n- Gather current technical information\n- Verify accuracy of claims\n- Find supporting examples and case studies\n```\n\n**agents.md:**\n\n```markdown\n# Project: Technical Blog Series\n\n## Codex's Role\n\nStrategic reviewer and analyst.\n\n- Review drafts for clarity and flow\n- Ensure technical accuracy\n- Validate against target audience needs\n```\n\n## Context Syncing Strategies\n\n### Manual Sync\n\n**After major changes:**\n\n```bash\n# Update all three manually\nnano claude.md\nnano gemini.md\nnano agents.md\n```\n\n**Or use one AI to update others:**\n\n```bash\n# In Claude terminal\n> I just made major updates to claude.md (added new project phase).\n  Update gemini.md and agents.md to match.\n```\n\n### Automated Sync (Chuck's Method)\n\n**Using a Claude agent:**\n\n```bash\n# In Claude terminal\n> @context-sync-agent\n  Read claude.md and sync to gemini.md and agents.md.\n  Ensure all three files are identical.\n```\n\n**Agent does:**\n\n1. Reads `claude.md`\n2. Overwrites `gemini.md` with same content\n3. Overwrites `agents.md` with same content\n4. Confirms sync complete\n\n### Git-Based Sync\n\n**For ultimate control:**\n\n```bash\n# After each session, commit context files\ngit add *.md\ngit commit -m \"Update project context: research phase complete\"\n\n# All terminals pull latest\ngit pull\n```\n\n**Each AI automatically loads updated context on next launch.**\n\n## Communication Patterns\n\n### Cross-AI References\n\n**Gemini creates file â†’ Claude uses it:**\n\n```bash\n# GEMINI TERMINAL\ngemini\n> Research ZFS performance. Save to zfs-research.md\n\n# CLAUDE TERMINAL (moments later)\nclaude\n> Read zfs-research.md and write a blog intro incorporating\n  those performance numbers. Save to blog-intro.md\n```\n\n### Review Loops\n\n**Claude writes â†’ Codex reviews â†’ Claude revises:**\n\n```bash\n# CLAUDE TERMINAL\nclaude\n> Write the authentication section. Save to auth-section.md\n\n# CODEX TERMINAL\ncodex\n> Review auth-section.md for security concerns.\n  Save feedback to reviews/auth-feedback.md\n\n# CLAUDE TERMINAL\nclaude\n> Read reviews/auth-feedback.md and revise auth-section.md\n  to address the security concerns.\n```\n\n### Parallel Tasks\n\n**All three work simultaneously:**\n\n```bash\n# CLAUDE: Long task\n> Create comprehensive system architecture document\n\n# GEMINI: Quick research\n> Find 5 examples of similar architectures in production\n\n# CODEX: Analysis\n> Analyze current requirements and identify gaps\n```\n\n**All running at once, no waiting!**\n\n## Advanced: Cross-Tool Agents\n\n**Video Timestamp:** 16:06-16:17\n\n### Claude Agent Uses Gemini\n\n**Chuck's demo:**\n\n```bash\n# In Claude terminal\n> @gemini-research find the best AI terminal videos on YouTube\n```\n\n**What happens:**\n\n1. Claude deploys `gemini-research` agent\n2. Agent runs: `gemini -p \"search YouTube for top AI terminal videos\"`\n3. Gemini performs search (better at current info)\n4. Returns results to Claude agent\n5. Claude compiles final report\n\n**Chuck's amazement:**\n\n> \"We're having an AI use an AI right now!\"\n\n### Create Gemini Research Agent\n\n**In Claude:**\n\n```bash\n> /agents\n# Create new agent\n\nName: gemini-research\nDescription: Uses Gemini CLI in headless mode for research tasks.\n              Gemini excels at web search and current information.\n\nInstructions:\nYou are a research specialist that uses Gemini CLI to gather information.\n\nWhen given a research task:\n1. Format it as a clear search query\n2. Run: gemini -p \"your search query here\"\n3. Compile and summarize the results\n\nYou have access to Bash tool to run gemini command.\n\nTools: Bash, Read, Write\nModel: Sonnet\n```\n\n**Usage:**\n\n```bash\n> @gemini-research What are the latest Proxmox features?\n> @gemini-research Find pricing for enterprise SSDs\n> @gemini-research Research zero-trust network solutions\n```\n\n## Managing Multiple Terminal Windows\n\n### Terminal Layouts\n\n**Chuck's setup (visible in video):**\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  CLAUDE         â”‚  GEMINI         â”‚  CODEX          â”‚\nâ”‚                 â”‚                 â”‚                 â”‚\nâ”‚  Deep work      â”‚  Research       â”‚  Analysis       â”‚\nâ”‚  Writing        â”‚  Web search     â”‚  Review         â”‚\nâ”‚  Agents         â”‚  Fast iteration â”‚  Strategy       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Tools for multi-terminal:**\n\n- **tmux** (Linux/Mac) - Terminal multiplexer\n- **Windows Terminal** (Windows) - Native tabs/panes\n- **iTerm2** (Mac) - Split panes\n- **Terminator** (Linux) - Multiple terminals\n\n### Quick Switching\n\n**tmux example:**\n\n```bash\n# Start tmux session for project\ntmux new -s ai-project\n\n# Window 1: Claude\nclaude\n\n# Create new window: Ctrl+B, C\n# Window 2: Gemini\ngemini\n\n# Create new window: Ctrl+B, C\n# Window 3: Codex\ncodex\n\n# Switch windows: Ctrl+B, [0-9]\n```\n\n## Workflow Optimization Tips\n\n### 1. Primary AI for Context Updates\n\n**Choose ONE AI to maintain context files:**\n\n```bash\n# Claude is \"source of truth\"\n# Only Claude updates context files\n# Others sync from Claude's version\n```\n\n**Why:** Prevents conflicts, single source of truth\n\n### 2. Clear File Naming\n\n**Make it obvious who created what:**\n\n```\nresearch-gemini-zfs-performance.md    â† Gemini research\ndraft-claude-section-1.md             â† Claude draft\nreview-codex-analysis.md              â† Codex review\n```\n\n### 3. Session Start Ritual\n\n**Every work session:**\n\n```bash\n# 1. Pull latest (if using git)\ngit pull\n\n# 2. Check context sync\ndiff claude.md gemini.md\n\n# 3. Launch all three AIs\n# Terminal 1: claude\n# Terminal 2: gemini\n# Terminal 3: codex\n\n# 4. Verify each loaded context\n# (Check context indicators in each terminal)\n```\n\n### 4. Session End Ritual\n\n**Chuck's approach (using agent):**\n\n```bash\n# In Claude terminal\n> @script-session-closer run\n```\n\n**Agent does:**\n\n- Summarizes session\n- Updates all context files\n- Syncs gemini.md and agents.md\n- Commits to git\n\n**Manual version:**\n\n```bash\n# 1. Update context files\n> Update claude.md with today's progress\n\n# 2. Sync to other files\n> Copy claude.md content to gemini.md and agents.md\n\n# 3. Commit\ngit add .\ngit commit -m \"Session end: [what you accomplished]\"\ngit push\n```\n\n## The \"It's Just a Folder\" Philosophy\n\n**Chuck's key insight:**\n\n> \"Everything I'm doing, talking with these three different AIs on a project... It's not tied in a browser. It's not tied in a GUI. It's just this folder right here on my hard drive.\"\n\n### What This Enables\n\n```\nmy-project/     â† This is ALL you need\nâ”œâ”€â”€ claude.md\nâ”œâ”€â”€ gemini.md\nâ”œâ”€â”€ agents.md\nâ””â”€â”€ all-your-work-files/\n```\n\n**You can:**\n\n- âœ… Copy to another computer\n- âœ… Backup easily\n- âœ… Version control with git\n- âœ… Switch AI tools anytime\n- âœ… Share with team (just the folder!)\n\n**Chuck's freedom:**\n\n> \"I can copy and paste that folder anywhere. All the work, all the decisions, all the context - it's mine. I own my context.\"\n\n## Troubleshooting\n\n### AIs Seem Out of Sync\n\n```bash\n# Check context files\ndiff claude.md gemini.md\n\n# Re-sync\nclaude\n> Sync all context files from claude.md\n```\n\n### File Conflicts\n\n**Two AIs editing same file:**\n\n```bash\n# Solution: Assign clear responsibilities\n# Claude: Writes sections/1.md\n# Gemini: Writes research/1.md\n# Never overlap!\n```\n\n### Context Drift\n\n**Over time, conversations diverge:**\n\n```bash\n# Regular re-sync\n# Every 30-60 minutes:\n> @context-sync-agent run\n```\n\n## Summary: Multi-Tool Benefits\n\n| Single AI           | Multi-Tool Workflow        |\n| ------------------- | -------------------------- |\n| One perspective     | Three perspectives         |\n| One strength        | Combined strengths         |\n| Sequential tasks    | Parallel tasks             |\n| One context window  | Three independent contexts |\n| Vendor lock-in risk | Tool agnostic              |\n\n**Chuck's verdict:**\n\n> \"I will use the best AI. No one can stop me. If a new greater better AI comes out, I'm ready for it.\"\n\n## What's Next?\n\n**Master the multi-tool workflow:**\n\nâž¡ï¸ [Productivity Workflows](11-productivity-workflows.md) - Real examples using multiple AIs\n\nâž¡ï¸ [AI Agents Deep Dive](09-agents.md) - Advanced agent strategies\n\n---\n\n[â† Back to Context Files](07-context-files.md) | [Next: AI Agents â†’](09-agents.md)\n\n# docs/09-agents.md\n\n# Coming Soon\n\nThis guide is currently being developed. Check back soon!\n\nIn the meantime, explore:\n\n- [README](../README.md) - Main guide\n- [Command Cheat Sheet](14-cheat-sheet.md) - Quick reference\n- [FAQ](16-faq.md) - Common questions\n\n---\n\n[â† Back to README](../README.md)\n\n# docs/10-customization.md\n\n# Coming Soon\n\nThis guide is currently being developed. Check back soon!\n\nIn the meantime, explore:\n\n- [README](../README.md) - Main guide\n- [Command Cheat Sheet](14-cheat-sheet.md) - Quick reference\n- [FAQ](16-faq.md) - Common questions\n\n---\n\n[â† Back to README](../README.md)\n\n# docs/11-productivity-workflows.md\n\n# Coming Soon\n\nThis guide is currently being developed. Check back soon!\n\nIn the meantime, explore:\n\n- [README](../README.md) - Main guide\n- [Command Cheat Sheet](14-cheat-sheet.md) - Quick reference\n- [FAQ](16-faq.md) - Common questions\n\n---\n\n[â† Back to README](../README.md)\n\n# docs/12-development-workflows.md\n\n# Coming Soon\n\nThis guide is currently being developed. Check back soon!\n\nIn the meantime, explore:\n\n- [README](../README.md) - Main guide\n- [Command Cheat Sheet](14-cheat-sheet.md) - Quick reference\n- [FAQ](16-faq.md) - Common questions\n\n---\n\n[â† Back to README](../README.md)\n\n# docs/13-homelab-workflows.md\n\n# Coming Soon\n\nThis guide is currently being developed. Check back soon!\n\nIn the meantime, explore:\n\n- [README](../README.md) - Main guide\n- [Command Cheat Sheet](14-cheat-sheet.md) - Quick reference\n- [FAQ](16-faq.md) - Common questions\n\n---\n\n[â† Back to README](../README.md)\n\n# docs/14-cheat-sheet.md\n\n# Command Cheat Sheet\n\nQuick reference for all AI terminal tools covered in the video.\n\n## Installation Commands\n\n### Gemini CLI\n\n```bash\n# Linux/macOS/WSL (npm)\nnpm install -g @google/generative-ai-cli\n\n# With sudo (if permission error)\nsudo npm install -g @google/generative-ai-cli\n\n# macOS (Homebrew)\nbrew install gemini-cli\n```\n\n### Claude Code\n\n```bash\n# All platforms (npm)\nnpm install -g @anthropic-ai/claude-code\n\n# With sudo\nsudo npm install -g @anthropic-ai/claude-code\n```\n\n### Codex (ChatGPT CLI)\n\n```bash\n# Installation command\nnpm install -g @openai/codex-cli\n\n# Or follow OpenAI documentation\n```\n\n### opencode\n\n```bash\n# Installation (one command - from video)\ncurl -fsSL https://opencode.sh/install.sh | sh\n\n# Reload shell\nsource ~/.bashrc\n# or\nsource ~/.zshrc\n```\n\n## Launch Commands\n\n### Basic Launch\n\n```bash\ngemini              # Launch Gemini CLI\nclaude              # Launch Claude Code\ncodex               # Launch Codex\nopencode            # Launch opencode\n```\n\n### Launch with Flags\n\n```bash\n# Claude Code\nclaude -r                               # Resume previous session\nclaude --dangerously-skip-permissions   # Skip safety prompts\nclaude -r --dangerously-skip-permissions  # Both flags combined\n\n# Gemini CLI\ngemini -p \"your prompt here\"           # Headless mode (one-shot)\n\n# opencode\nopencode --model claude-sonnet-4       # Specify model\n```\n\n## In-Session Commands\n\n### Gemini CLI\n\n```bash\n/init               # Create gemini.md context file\n/tools              # Show available tools\n/help               # Show help\nexit                # Exit Gemini (or Ctrl+C)\n```\n\n### Claude Code\n\n```bash\n/init               # Create claude.md context file\n/context            # Show context usage details\n/agents             # Agent management menu\n/output-style       # Output style management\nexit                # Exit Claude (or Ctrl+C)\n```\n\n**Keyboard shortcuts:**\n\n```bash\nTab                 # Toggle thinking mode\nShift+Tab           # Toggle planning mode\nCtrl+C              # Interrupt/exit\nCtrl+O              # View agent details (when agent running)\n```\n\n### opencode\n\n```bash\n/model              # Change AI model\n/share              # Share current session\n/timeline           # View session timeline\n/sessions           # View all sessions\nexit                # Exit opencode\n```\n\n## Project Setup\n\n### Create New Project\n\n```bash\n# Standard workflow\nmkdir my-project\ncd my-project\n\n# Launch AI and create context\ngemini\n> /init\n\n# Verify context file created\nls *.md\n```\n\n### Multi-Tool Project Setup\n\n```bash\n# Create project\nmkdir my-project\ncd my-project\n\n# Initialize all three context files\n# Terminal 1\ngemini\n> /init\n\n# Terminal 2\nclaude\n> /init\n\n# Terminal 3\ncodex\n> /init\n\n# Sync context files\nclaude\n> Sync claude.md to gemini.md and agents.md\n```\n\n## Context File Management\n\n### Create Context Files\n\n```bash\n# Let AI create it\n> /init\n\n# Manual creation\nnano gemini.md      # Edit manually\nnano claude.md\nnano agents.md\n```\n\n### Update Context\n\n```bash\n# Ask AI to update\n> Update gemini.md with: [your updates]\n\n# Manual edit\nnano gemini.md\n```\n\n### Sync Context Files\n\n```bash\n# Use Claude to sync\nclaude\n> Read claude.md and make gemini.md and agents.md identical\n```\n\n### View Context\n\n```bash\n# View file contents\ncat gemini.md\ncat claude.md\ncat agents.md\n\n# In Claude, check context usage\n> /context\n```\n\n## Agent Commands (Claude Code)\n\n### Agent Management\n\n```bash\n> /agents                      # Open agent menu\n> /agents                      # Then: \"Create new agent\"\n> /agents                      # Then: \"View agents\"\n> /agents                      # Then: \"Edit agent\"\n> /agents                      # Then: \"Delete agent\"\n```\n\n### Deploy Agents\n\n```bash\n> @agent-name do a task\n> @homelab-guru research NAS options\n> @brutal-critic review my script\n```\n\n### Multi-Agent Tasks\n\n```bash\n> @agent1 do task A, @agent2 do task B, and compile results\n```\n\n## Output Styles (Claude Code)\n\n### Manage Output Styles\n\n```bash\n> /output-style                # View available styles\n> /output-style new            # Create new style\n> /output-style                # Select active style\n```\n\n### Create Output Style\n\n```bash\n> /output-style new\n\n# Then describe the style:\nName: my-expert\nDescription: You are an expert in [domain]...\n```\n\n## File Operations\n\n### Create Files\n\n```bash\n# Ask AI to create\n> Create a file named project-plan.md with [content]\n\n# Manual creation\nnano project-plan.md\n```\n\n### Read Files\n\n```bash\n# AI reads files automatically when mentioned\n> Read project-plan.md and summarize\n\n# Reference with @\n> Review @project-plan.md\n```\n\n### Update Files\n\n```bash\n# AI updates\n> Update project-plan.md to add [new section]\n\n# Manual edit\nnano project-plan.md\n```\n\n### List Files\n\n```bash\n# Terminal command\nls\nls -la\n\n# AI command\n> What files are in this directory?\n> Show me all markdown files\n```\n\n## Git Integration\n\n### Initialize Repository\n\n```bash\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n```\n\n### Regular Commits\n\n```bash\n# After work session\ngit add .\ngit commit -m \"Session summary: [what you did]\"\ngit push\n```\n\n### Chuck's Automated Approach\n\n```bash\n# Using session-closer agent\n> @script-session-closer run\n\n# Agent automatically:\n# - Summarizes session\n# - Updates context files\n# - Commits to git\n```\n\n## Multi-Terminal Workflows\n\n### Open Multiple Terminals\n\n```bash\n# Terminal 1: Claude\ncd ~/my-project\nclaude\n\n# Terminal 2: Gemini\ncd ~/my-project\ngemini\n\n# Terminal 3: Codex\ncd ~/my-project\ncodex\n```\n\n### tmux (Advanced)\n\n```bash\n# Start tmux session\ntmux new -s ai-work\n\n# Create windows\nCtrl+B, C           # New window\nCtrl+B, [0-9]       # Switch to window N\nCtrl+B, \"           # Split horizontal\nCtrl+B, %           # Split vertical\n```\n\n## opencode Specific Commands\n\n### Model Management\n\n```bash\n# View available models\n> /model\n\n# Switch to specific model\n> /model claude-sonnet-4\n> /model grok-fast\n> /model llama-3.2\n```\n\n### Configuration\n\n```bash\n# Edit config file\nnano ~/.config/opencode/opencode.jsonc\n\n# Example config for local model\n{\n  \"model\": \"llama-3.2\"\n}\n```\n\n### Authentication\n\n```bash\n# Login with Claude Pro\nopencode auth login\n# Select \"Anthropic\"\n# Paste auth code from browser\n\n# Verify login\nopencode auth whoami\n```\n\n### Session Management\n\n```bash\n> /sessions         # View all sessions\n> /timeline         # View current session timeline\n> /share            # Generate shareable link\n```\n\n## Advanced Techniques\n\n### Headless Mode\n\n```bash\n# Gemini one-shot command\ngemini -p \"Research ZFS performance and save to report.md\"\n\n# Claude with pipe\necho \"Analyze this file\" | claude\n\n# Chain commands\ngemini -p \"Research topic\" && claude -r\n```\n\n### Agent as Tool\n\n```bash\n# Claude agent using Gemini\n> @gemini-research search for latest Docker security updates\n\n# Agent runs:\ngemini -p \"search for latest Docker security updates\"\n```\n\n### Obsidian Integration\n\n```bash\n# Navigate to vault\ncd ~/Obsidian/MyVault\n\n# Launch AI\ngemini\n\n# Work with notes\n> Read my daily note and summarize tasks\n> Create a new note about [topic] with backlinks\n```\n\n## Troubleshooting Commands\n\n### Permission Issues\n\n```bash\n# Reinstall with sudo\nsudo npm install -g @google/generative-ai-cli\n\n# Fix permissions\nsudo chown -R $USER /usr/local/lib/node_modules\n```\n\n### Command Not Found\n\n```bash\n# Reload shell\nsource ~/.bashrc\nsource ~/.zshrc\n\n# Verify installation\nwhich gemini\nwhich claude\nwhich opencode\n\n# Check PATH\necho $PATH\n```\n\n### Context File Issues\n\n```bash\n# Verify file exists\nls *.md\n\n# Check file contents\ncat gemini.md\n\n# Recreate if needed\n> /init\n```\n\n### Clear Cache/Reset\n\n```bash\n# Claude Code\nrm -rf ~/.config/claude-code\n\n# Gemini CLI\nrm -rf ~/.config/gemini-cli\n\n# opencode\nrm -rf ~/.config/opencode\n```\n\n## Common Workflows\n\n### Research & Write\n\n```bash\n# Step 1: Research (Gemini)\ngemini\n> Research [topic] and compile findings\n\n# Step 2: Write (Claude)\nclaude\n> Read research.md and write comprehensive blog post\n\n# Step 3: Review (Codex)\ncodex\n> Review blog-post.md for accuracy and clarity\n```\n\n### Project Kickoff\n\n```bash\n# Create project\nmkdir project-name\ncd project-name\n\n# Initialize\ngemini\n> /init\n> Help me plan this project: [description]\n\n# Track with git\ngit init\ngit add .\ngit commit -m \"Project initialized\"\n```\n\n### Daily Work Session\n\n```bash\n# Start\ncd ~/current-project\nclaude\n\n# Check status\n> Where are we in the project?\n\n# Work\n> [Your tasks]\n\n# End session\n> @script-session-closer run\n# Or manually:\ngit add .\ngit commit -m \"Session: [summary]\"\n```\n\n## Quick Tips\n\n### Keyboard Shortcuts\n\n```bash\nCtrl+C              # Interrupt AI / Exit\nCtrl+D              # Exit (alternative)\nTab                 # Toggle thinking (Claude)\nShift+Tab           # Toggle planning (Claude)\nCtrl+O              # Check agent status (Claude)\n```\n\n### Speed Tips\n\n```bash\n# Use dangerous mode (when safe)\nclaude --dangerously-skip-permissions\n\n# Use headless for quick tasks\ngemini -p \"quick question\"\n\n# Deploy agents for parallel work\n> @agent1 task A, @agent2 task B\n```\n\n### Organization Tips\n\n```bash\n# One directory per project\n~/projects/project-a/\n~/projects/project-b/\n\n# Consistent context file naming\ngemini.md, claude.md, agents.md\n\n# Use git for everything\ngit commit regularly\n```\n\n## Emergency Commands\n\n### Kill Stuck Process\n\n```bash\n# Find process ID\nps aux | grep gemini\nps aux | grep claude\n\n# Kill it\nkill -9 [PID]\n\n# Or use Ctrl+C twice rapidly\n```\n\n### Reset Everything\n\n```bash\n# Remove config directories\nrm -rf ~/.config/gemini-cli\nrm -rf ~/.config/claude-code\nrm -rf ~/.config/opencode\n\n# Reinstall\nnpm install -g @google/generative-ai-cli\nnpm install -g @anthropic-ai/claude-code\ncurl -fsSL https://opencode.sh/install.sh | sh\n```\n\n## Official Documentation Links\n\n```bash\n# Gemini CLI\nhttps://ai.google.dev/gemini-api/docs/cli\n\n# Claude Code\nhttps://docs.anthropic.com/claude/docs/claude-code\n\n# opencode\nhttps://github.com/stackblitz-labs/opencode\n\n# Codex\nhttps://platform.openai.com/docs/tools/codex\n```\n\n---\n\n**Print this page for quick reference!**\n\n[â† Back to README](../README.md)\n\n# docs/15-troubleshooting.md\n\n# Troubleshooting Guide\n\n## Installation Issues\n\n### \"Command not found\" after installation\n\n**Problem:** Installed tool but terminal doesn't recognize command\n\n**Solutions:**\n\n```bash\n# 1. Reload shell configuration\nsource ~/.bashrc     # for bash\nsource ~/.zshrc      # for zsh\n\n# 2. Close and reopen terminal\n\n# 3. Check if actually installed\nwhich gemini\nwhich claude\nwhich opencode\n\n# 4. Verify PATH includes npm global packages\necho $PATH | grep npm\n```\n\n### \"Permission denied\" during installation\n\n**Problem:** npm permission errors\n\n**Solutions:**\n\n```bash\n# Option 1: Use sudo (quick fix)\nsudo npm install -g @google/generative-ai-cli\n\n# Option 2: Fix npm permissions (proper fix)\nsudo chown -R $USER /usr/local/lib/node_modules\nsudo chown -R $USER /usr/local/bin\n```\n\n### \"Node.js not found\"\n\n**Problem:** npm commands fail, Node.js not installed\n\n**Solution:**\n\n1. Install Node.js from [nodejs.org](https://nodejs.org/)\n2. Choose LTS (Long Term Support) version\n3. Restart terminal after installation\n\n## Authentication Issues\n\n### Can't login to Gemini CLI\n\n**Solutions:**\n\n```bash\n# 1. Ensure browser opens for OAuth\n# Check if browser is set as default\n\n# 2. Try incognito/private browser window\n# Sometimes cached auth causes issues\n\n# 3. Clear Gemini config and retry\nrm -rf ~/.config/gemini-cli\ngemini\n```\n\n### Claude Code authentication fails\n\n**Solutions:**\n\n```bash\n# 1. Verify Claude Pro subscription is active\n# Check at claude.ai\n\n# 2. Clear auth cache\nrm -rf ~/.config/claude-code/auth\n\n# 3. Re-authenticate\nclaude auth login\n```\n\n### opencode provider authentication issues\n\n**Solutions:**\n\n```bash\n# 1. Check auth status\nopencode auth whoami\n\n# 2. Re-login\nopencode auth login\n\n# 3. For Claude Pro: ensure correct provider selected\n# Select \"Anthropic\" not \"OpenAI\"\n\n# 4. Check API key validity (if using API keys)\n```\n\n## Context File Issues\n\n### Context file not loading automatically\n\n**Solutions:**\n\n```bash\n# 1. Verify correct filename (case-sensitive!)\nls gemini.md     # âœ“ Correct\nls Gemini.md     # âœ— Wrong\nls GEMINI.md     # âœ— Wrong\n\n# 2. Ensure you're in the right directory\npwd\n# Should show your project directory\n\n# 3. Recreate context file\n> /init\n\n# 4. Check file isn't empty\ncat gemini.md\n```\n\n### Multiple context files out of sync\n\n**Problem:** Using Claude + Gemini + Codex, context files differ\n\n**Solution:**\n\n```bash\n# Use one AI to sync all files\nclaude\n> Read claude.md and copy its content exactly to gemini.md and agents.md\n\n# Verify they match\ndiff claude.md gemini.md\ndiff claude.md agents.md\n```\n\n### Context seems stale/outdated\n\n**Solutions:**\n\n```bash\n# 1. Exit and restart session (forces reload)\nexit\ngemini  # or claude/codex\n\n# 2. Manually update context file\nnano gemini.md\n# Make your changes\n# Save and exit\n# Restart session\n\n# 3. Ask AI to update context\n> Update gemini.md to reflect our latest progress\n```\n\n## Agent Issues (Claude Code)\n\n### Can't create agent\n\n**Solutions:**\n\n```bash\n# 1. Verify you're in Claude Code (not Gemini/Codex)\n# Agents only work in Claude Code\n\n# 2. Ensure Claude Pro subscription is active\n\n# 3. Try creating via menu\n> /agents\n# Select \"Create new agent\"\n```\n\n### Agent not responding\n\n**Solutions:**\n\n```bash\n# 1. Check agent syntax\n> @agent-name do task    # âœ“ Correct\n> agent-name do task     # âœ— Wrong (missing @)\n\n# 2. Verify agent exists\n> /agents\n# Check list of available agents\n\n# 3. Check agent has appropriate tools enabled\n> /agents\n# Select \"Edit agent\"\n# Verify tools are enabled\n```\n\n### \"Agent not found\" error\n\n**Solutions:**\n\n```bash\n# 1. Check agent scope\n# Project agents only available in that project directory\n\n# 2. Verify agent name (case-sensitive)\n> @homelab-guru    # âœ“ Correct\n> @Homelab-Guru    # âœ— Wrong\n\n# 3. Recreate agent if necessary\n> /agents\n# Delete and recreate\n```\n\n## File Operation Issues\n\n### AI can't read my files\n\n**Solutions:**\n\n```bash\n# 1. Verify file permissions\nls -la yourfile.md\n\n# 2. Check you're in correct directory\npwd\nls\n\n# 3. Use absolute path if needed\n> Read /full/path/to/file.md\n\n# 4. Check file isn't binary\nfile yourfile.md\n# Should show: \"ASCII text\" or \"UTF-8 text\"\n```\n\n### AI can't write files\n\n**Solutions:**\n\n```bash\n# 1. Check directory permissions\nls -la\n\n# 2. Verify disk space\ndf -h .\n\n# 3. Try different filename\n> Create test.md with content \"hello\"\n\n# 4. Check if file exists and is read-only\nls -la existing-file.md\nchmod 644 existing-file.md  # Make writable\n```\n\n## Performance Issues\n\n### AI responses are very slow\n\n**Solutions:**\n\n```bash\n# 1. Check internet connection\nping 8.8.8.8\n\n# 2. Try different model (for opencode)\n> /model grok-fast-1\n# Faster model for quick tasks\n\n# 3. Reduce context size\n# Start new session if context window is full\n\n# 4. Close unused terminal sessions\n# Multiple AI sessions can slow things down\n```\n\n### Terminal freezes/hangs\n\n**Solutions:**\n\n```bash\n# 1. Try Ctrl+C to interrupt\n# Press once, wait 2 seconds\n\n# 2. Force quit if needed\n# Ctrl+C twice rapidly\n\n# 3. Kill process from another terminal\nps aux | grep gemini\nkill -9 [PID]\n\n# 4. Close and reopen terminal\n```\n\n## Multi-Tool Issues\n\n### AIs giving conflicting information\n\n**Expected behavior!** Different models have different strengths.\n\n**Strategy:**\n\n- Claude: Best for deep work\n- Gemini: Best for current info\n- Codex: Best for analysis\n\nCross-check important decisions across multiple AIs.\n\n### File conflicts (multiple AIs editing same file)\n\n**Prevention:**\n\n```bash\n# Assign clear responsibilities\n# Claude: sections/1.md\n# Gemini: research/1.md\n# Never overlap!\n```\n\n**Solution if it happens:**\n\n```bash\n# 1. Check file with git diff\ngit diff file.md\n\n# 2. Review changes manually\ncat file.md\n\n# 3. Use version control to restore\ngit checkout file.md\n```\n\n## Platform-Specific Issues\n\n### Windows / WSL Issues\n\n**Problem:** Commands not working in PowerShell\n\n**Solution:** Use WSL (Windows Subsystem for Linux)\n\n```powershell\n# Install WSL\nwsl --install\n\n# Launch Ubuntu\nwsl\n\n# Install tools in WSL, not PowerShell\n```\n\n### macOS Issues\n\n**Problem:** \"Developer tools not installed\"\n\n**Solution:**\n\n```bash\nxcode-select --install\n# Wait for installation\n# Retry npm install\n```\n\n### Linux Issues\n\n**Problem:** npm permissions complex\n\n**Solution:**\n\n```bash\n# Use nvm (Node Version Manager) instead\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\nsource ~/.bashrc\nnvm install --lts\nnvm use --lts\n\n# Now install tools\nnpm install -g @google/generative-ai-cli\n```\n\n## Still Having Issues?\n\n### Check Official Documentation\n\n- [Gemini CLI Docs](https://ai.google.dev/gemini-api/docs/cli)\n- [Claude Code Docs](https://docs.anthropic.com/claude/docs/claude-code)\n- [opencode GitHub](https://github.com/stackblitz-labs/opencode)\n\n### Community Support\n\n- NetworkChuck Discord\n- GitHub Issues for opencode\n- Tool-specific forums\n\n### Nuclear Option: Complete Reinstall\n\n```bash\n# 1. Remove all tools\nnpm uninstall -g @google/generative-ai-cli\nnpm uninstall -g @anthropic-ai/claude-code\nrm -rf ~/.config/gemini-cli\nrm -rf ~/.config/claude-code\nrm -rf ~/.config/opencode\n\n# 2. Reinstall from scratch\nnpm install -g @google/generative-ai-cli\nnpm install -g @anthropic-ai/claude-code\ncurl -fsSL https://opencode.sh/install.sh | sh\n\n# 3. Reload shell\nsource ~/.bashrc\n```\n\n---\n\n[â† Back to README](../README.md)\n\n# docs/16-faq.md\n\n# Frequently Asked Questions\n\n## Getting Started\n\n### Which tool should I start with?\n\n**Start with Gemini CLI** if you want free access. It's generous and perfect for learning the concepts.\n\n**Upgrade to Claude Code** if you need agents and are serious about terminal AI workflows.\n\n### Do I need all three tools?\n\nNo! Chuck uses all three because they have different strengths:\n\n- **Gemini**: Research and web search\n- **Claude**: Deep work and agents\n- **Codex**: High-level analysis\n\nStart with one, add others as needed.\n\n### Is this just for developers?\n\n**Absolutely not!** Chuck uses these tools for YouTube video scriptwriting. They work for:\n\n- Writing and content creation\n- Research and analysis\n- Project planning\n- Documentation\n- Any text-based work\n\nCoding is just ONE use case.\n\n## Cost & Subscriptions\n\n### How much does this cost?\n\n- **Gemini CLI**: FREE (generous limits)\n- **Claude Code**: $20/mo (Claude Pro required)\n- **Codex**: $20/mo (ChatGPT Plus) or pay-per-use API\n- **opencode**: FREE (Grok), or use existing subscriptions\n\n### Can I use existing AI subscriptions?\n\n**Yes!**\n\n- Have Claude Pro? Use it with Claude Code\n- Have ChatGPT Plus? Use it with Codex\n- Have Claude Pro? Use it with opencode too!\n\n### Which subscription is most worth it?\n\n**Chuck's recommendation:** Claude Pro ($20/mo)\n\n- Access to Claude Code (terminal)\n- Access to Claude web\n- Works with opencode\n- Agents feature is game-changing\n\n## Technical Questions\n\n### What's a context file?\n\nA markdown file (gemini.md, claude.md, agents.md) that tells the AI what your project is about. It loads automatically every session so you never re-explain your work.\n\n### Why do I need different context files?\n\nEach AI tool looks for its own:\n\n- Gemini CLI â†’ gemini.md\n- Claude Code â†’ claude.md\n- Codex/opencode â†’ agents.md\n\nKeep them synced when using multiple tools!\n\n### What's an agent?\n\n(Claude Code feature) A separate AI instance with:\n\n- Specialized instructions\n- Fresh context window\n- Custom tool access\n- Independent from main conversation\n\nThink: delegating tasks to specialized coworkers.\n\n### Can AI access all my files?\n\nOnly files in the directory where you launch it.\n\n**Safety tip:** Start AI tools in project directories, not your home folder!\n\n## Workflow Questions\n\n### How do I use multiple AI tools together?\n\n1. Launch all tools in the same directory\n2. Sync context files (claude.md = gemini.md = agents.md)\n3. Each AI can read/write the same files\n4. No copy/paste needed!\n\n### How do I avoid losing my work?\n\nUse git! Chuck commits his projects regularly:\n\n```bash\ngit init\ngit add .\ngit commit -m \"Session summary\"\n```\n\nEverything is local files, perfect for version control.\n\n### Can I access my Obsidian vault?\n\n**Yes!** Just launch the AI in your vault directory:\n\n```bash\ncd ~/Obsidian/MyVault\ngemini\n```\n\nAI can read all your notes (they're markdown files).\n\n## Troubleshooting\n\n### \"Command not found\"\n\n```bash\n# Reload shell\nsource ~/.bashrc\n\n# Or close and reopen terminal\n```\n\n### \"Permission denied\"\n\n```bash\n# Use sudo\nsudo npm install -g [package-name]\n```\n\n### Context file not loading\n\n```bash\n# Verify you're in right directory\npwd\nls gemini.md\n\n# Recreate\n> /init\n```\n\n## Comparison Questions\n\n### Browser AI vs Terminal AI?\n\n**Browser AI:**\n\n- Lost context after scrolling\n- Scattered across multiple chats\n- Copy/paste nightmare\n- Vendor lock-in\n\n**Terminal AI:**\n\n- Persistent context files\n- One project folder\n- Direct file access\n- You own everything\n\n### Which AI is \"best\"?\n\nDepends on the task:\n\n- **Claude**: Best for writing, deep work, agents\n- **Gemini**: Best for web research, current info\n- **ChatGPT**: Best for high-level analysis\n- **opencode**: Best for flexibility, local models\n\nChuck uses all three for different strengths.\n\n## Philosophy Questions\n\n### Why does Chuck care about owning his context?\n\n**Vendor lock-in avoidance.** Browser AI traps your work in their platform.\n\nWith terminal AI:\n\n- Everything is local files\n- Copy folder anywhere\n- Switch AI tools anytime\n- Full control\n\n### What's the \"It's just a folder\" philosophy?\n\nYour project = one folder containing:\n\n- Context files\n- Work products\n- Research\n- Everything\n\nPortable, version-controlled, tool-agnostic. **You own it.**\n\n## Need More Help?\n\nâ†’ [Troubleshooting Guide](15-troubleshooting.md)\n\nâ†’ [Command Cheat Sheet](14-cheat-sheet.md)\n\n---\n\n[â† Back to README](../README.md)\n"
  },
  {
    "name": "ai_hacking_study_prompts",
    "path": "integrations/docs/ai_hacking_study_prompts.md",
    "content": "\n\n```markdown\n# Expert Hacker Prompts for HTB CPTS Preparation\n\n## Summary\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\nPlease generate a summary of the content on this page in no more than 5 bullet points.\n```\n\n## Explain More\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply. \n\n### Task:\n1. Analyze the contents and notes of the page provided.\n2. Identify the most important topics that require additional context or information.\n3. Provide detailed explanations or additional context for these topics.\n\n### Instructions:\n- Ensure your explanations are clear and easy to understand.\n- Use examples where necessary to illustrate complex concepts.\n- Structure your response in a way that is logical and easy to follow.\n```\n\n## Flashcards\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\nBased on the content in this lesson, provide a list of the most important things to memorize and make flashcards for. These should be things within the hacking space that need to be memorized in order to be successful as a hacker and in order to pass the CPTS exam. Write these flashcards so they can be easily copied and pasted into Anki.\n\n**Flashcard Format:**\n- **Front:** [Question]\n- **Back:** [Answer]\n```\n\n## Concept Mapping\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nCreate a concept map based on the content in this lesson. Identify the main concepts and their relationships. Provide a visual representation or outline of how these concepts are connected.\n\n### Instructions:\n- Highlight the key concepts and their connections.\n- Ensure the concept map is clear and easy to understand.\n- Use simple language and examples to explain the relationships between concepts.\n```\n\n## Practical Applications\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nIdentify practical applications of the concepts covered in the lesson. Provide examples or scenarios where these concepts would be applied in real-world hacking situations.\n\n### Instructions:\n- Explain how each concept can be applied in practice.\n- Use real-world examples or scenarios to illustrate practical applications.\n- Ensure explanations are easy to understand and follow.\n```\n\n## Real-World Case Studies\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nIdentify real-world case studies related to the content on this page. Provide a brief summary of each case study. Explain the relevance of the case study to the concepts covered in the lesson.\n\n### Instructions:\n- Choose case studies that are directly relevant to the content.\n- Summarize each case study in a few sentences.\n- Highlight the key takeaways and their relevance to the lesson.\n```\n\n## Review Questions\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\nBased on the content in this lesson, generate a set of review questions that can be used to test understanding of the material. These should focus on key concepts and practical applications.\n\n**Review Question Format:**\n\n- **Question:** [Insert question here]\n- **Answer:** [Insert answer here]\n\n---\n\n### Example:\n\n**Question 1:**\n- **Question:** What is the purpose of the OSI model in networking?\n- **Answer:** The OSI model provides a standard framework for networking protocols and their interoperability, ensuring consistent communication between different systems and devices.\n\n**Question 2:**\n- **Question:** What are the common techniques used in SQL injection attacks?\n- **Answer:** Common techniques include using tautologies, union queries, piggybacked queries, and blind SQL injection to manipulate the database and retrieve unauthorized information.\n```\n\n## Common Pitfalls\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nIdentify common pitfalls or mistakes that students might encounter when studying the content on this page. Provide tips on how to avoid these pitfalls.\n\n### Instructions:\n- List the common pitfalls in bullet points.\n- Provide practical advice on how to avoid each pitfall.\n- Ensure explanations are clear and actionable.\n\n**Example:**\n- Pitfall: Neglecting input validation in web applications.\n- Tip: Always validate and sanitize user input to prevent common vulnerabilities like SQL injection and cross-site scripting (XSS).\n\n<details>\n  <summary>Show Explanation</summary>\n  Input validation ensures that user data is checked for malicious content before processing, which helps prevent security breaches and data corruption.\n</details>\n```\n\n## Concept Comparisons\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nCompare and contrast related concepts from the content on this page. Highlight the differences and similarities between these concepts.\n\n### Instructions:\n- Structure the comparison in a clear and logical format.\n- Use tables or bullet points for clarity.\n- Provide examples to illustrate the differences and similarities.\n\n**Example:**\nConcepts: Symmetric vs. Asymmetric Encryption\n\n**Symmetric Encryption:**\n- Uses the same key for encryption and decryption.\n- Faster and more efficient for large amounts of data.\n<details>\n  <summary>Show Example</summary>\n  Example: AES (Advanced Encryption Standard) is commonly used for securing data at rest.\n</details>\n\n**Asymmetric Encryption:**\n- Uses a pair of keys (public and private) for encryption and decryption.\n- More secure for key distribution and digital signatures.\n<details>\n  <summary>Show Example</summary>\n  Example: RSA (Rivest-Shamir-Adleman) is widely used for secure data transmission.\n</details>\n```\n\n## Scenario-based Learning\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nCreate scenario-based learning exercises based on the content on this page. Provide detailed scenarios and questions that challenge the application of concepts.\n\n### Instructions:\n- Describe the scenario in detail.\n- Ask questions that require applying the concepts to the scenario.\n- Provide explanations and answers in toggles for self-assessment.\n\n**Example:**\nScenario: A companyâ€™s web server has been compromised, and sensitive data has been leaked. The initial investigation suggests that the attacker exploited a vulnerability in the web application.\n\n**Question:** What steps would you take to investigate and mitigate this incident?\n<details>\n  <summary>Show Answer</summary>\n  - **Investigation:**\n    - Analyze server logs to identify the point of entry and the attackerâ€™s actions.\n    - Conduct a code review to find and patch the vulnerability.\n  - **Mitigation:**\n    - Implement stricter input validation and output encoding.\n    - Update and patch all software and dependencies.\n    - Conduct a thorough security audit to ensure no other vulnerabilities exist.\n</details>\n```\n\n## Deep-Dive\n\n```plaintext\nYou are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nSelect a complex topic from the content on this page. Provide an in-depth explanation and analysis of the topic. Break down the topic into smaller, manageable parts.\n\n### Instructions:\n- Provide detailed explanations for each part of the topic.\n- Use diagrams or visual aids to enhance understanding.\n- Ensure the explanation is thorough and easy to follow.\n\n**Example:**\nTopic: Intrusion Detection Systems (IDS)\n\n**Part 1:** Types of IDS (Network-based vs. Host-based)\n<details>\n  <summary>Show Explanation</summary>\n  - **Network-based IDS:** Monitors network traffic for suspicious activity. Example: Snort.\n  - **Host-based IDS:** Monitors the behavior of a single host for signs of compromise. Example: OSSEC.\n</details>\n\n**Part 2:** Detection Methods (Signature-based vs. Anomaly-based)\n<details>\n  <summary>Show Explanation</summary>\n  - **Signature-based Detection:** Compares network traffic to known attack patterns. Example: Matching known virus signatures.\n  - **Anomaly-based Detection:** Identifies deviations from normal behavior. Example: Detecting unusual login times.\n</details>\n\n**Part 3:** Implementation and Best Practices\n<details>\n  <summary>Show Explanation</summary>\n  - Regularly update IDS signatures and rules.\n  - Integrate IDS with other security tools for comprehensive monitoring.\n  - Conduct regular audits and reviews of IDS alerts to ensure effectiveness.\n</details>\n```\n\n## Mindmap\n\n```plaintext\nYou are an expert hacker with extensive experience\n\n, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\nCreate a mind map of the key concepts and their relationships from the study material. Use the mind map to visualize connections and enhance understanding.\n\n### Instructions:\n- Generate a visual mind map.\n- Highlight connections between key concepts.\n\n**Example Prompt:**\nPrompt: \"Create a mind map of the key concepts from the study material and highlight their relationships.â€\n```\n\n## Content Generator\n\n```plaintext\n**SYSTEM:**\nYou are an expert content creator with extensive experience in generating engaging content for various platforms, including YouTube, Twitter, and LinkedIn. Your passion is to help others learn and solidify their knowledge by teaching.\n\n### Task:\nGenerate content ideas based on the concepts provided. Provide ideas for YouTube videos, YouTube Shorts, Twitter posts, and LinkedIn posts.\n\n### Instructions:\n1. Ensure the content ideas are relevant to the provided concepts.\n2. Tailor the ideas for each platform, considering the unique format and audience of each.\n\n**Example Prompt:**\n\"Generate content ideas for YouTube videos, YouTube Shorts, Twitter posts, and LinkedIn posts based on the concept of Nmap Host Discovery.\"\n\n**YouTube Videos:**\n1. **Best Nmap Commands for Host Discovery** - Discuss the most effective Nmap commands for identifying active hosts in a network. Demonstrate practical examples of each command.\n2. **Firewall Evasion Techniques with Nmap** - Explain how to use Nmap to bypass firewalls and IDS/IPS systems. Showcase different techniques and real-world scenarios.\n3. **Understanding Nmap Scanning Options** - Breakdown the various scanning options available in Nmap. Discuss when and how to use each option effectively.\n4. **Comprehensive Guide to Nmap Scripting Engine (NSE)** - Explore the capabilities of the Nmap Scripting Engine. Provide examples of useful scripts and how to implement them.\n5. **Saving and Analyzing Nmap Scan Results** - Teach viewers how to store Nmap scan results in different formats. Explain how to analyze and compare results for better insights.\n\n**YouTube Shorts:**\n1. **Quick Nmap Command Tutorial** - Highlight a single Nmap command and explain its use in under 60 seconds. Example: \"How to Use Nmap's -sn Option for Host Discovery\"\n2. **Firewall Evasion Tip** - Share a quick tip on evading firewalls using Nmap. Example: \"Nmap Firewall Evasion: Using Decoy Scans\"\n3. **Nmap Scripting Engine Highlight** - Showcase a useful NSE script and its application. Example: \"Using NSE Scripts: Detecting Vulnerabilities with Nmap\"\n\n**Twitter Posts:**\n1. **Daily Nmap Tip** - Share a daily tip or command related to Nmap. Example: \"Tip of the Day: Use nmap -sn for quick host discovery without port scanning. #CyberSecurity #Nmap\"\n2. **Concept Breakdown Thread** - Create a thread breaking down a complex concept into simpler parts. Example: \"Thread: Understanding Nmap's Host Discovery Methods ðŸ§µðŸ‘‡ #PenTest #CyberSecurity\"\n3. **Case Study Highlight** - Share a brief case study or real-world application of Nmap. Example: \"Case Study: How we used Nmap to identify active hosts in a large corporate network. #EthicalHacking #Nmap\"\n\n**LinkedIn Posts:**\n1. **In-Depth Article** - Write an article about a specific Nmap technique or concept. Example: \"Mastering Host Discovery with Nmap: Techniques and Best Practices\"\n2. **Professional Tip** - Share a professional tip related to network scanning and Nmap. Example: \"Pro Tip: Always store your Nmap scan results for future analysis and reporting. #CyberSecurity #Nmap\"\n3. **Learning Experience** - Post about a learning experience or a recent project involving Nmap. Example: \"Just completed an internal network penetration test using Nmap. Here's what I learned about effective host discovery. #ProfessionalDevelopment #PenTestâ€\n```\n\n## Cheat Sheet Generator\n\n```plaintext\n**SYSTEM:** You are an expert hacker with extensive experience, having solved every box on HackTheBox and earned the HTB CPTS (Certified Penetration Tester Specialist) certification. Your passion is teaching and explaining things simply.\n\n### Task:\n1. Generate a comprehensive cheatsheet for the given topic based on the provided content.\n2. Include key commands, options, and explanations for each entry.\n3. Organize the cheatsheet in a clear and logical format for easy reference.\n\n**Example Prompt:**\n\"Generate a cheatsheet for Nmap commands and techniques based on the provided content. Include key commands, their options, and explanations.\"\n\n**Example Cheatsheet:**\n\n**Nmap Commands Cheatsheet**\n\n1. **Basic Scan**\n   - **Command:** `nmap [target]`\n   - **Explanation:** Performs a basic scan on the specified target.\n2. **Ping Scan (Host Discovery)**\n   - **Command:** `nmap -sn [target]`\n   - **Explanation:** Checks which hosts are up without performing a port scan.\n3. **Service Version Detection**\n   - **Command:** `nmap -sV [target]`\n   - **Explanation:** Detects versions of services running on open ports.\n4. **OS Detection**\n   - **Command:** `nmap -O [target]`\n   - **Explanation:** Attempts to determine the operating system of the target.\n5. **Aggressive Scan**\n   - **Command:** `nmap -A [target]`\n   - **Explanation:** Enables OS detection, version detection, script scanning, and traceroute.\n6. **Scan Specific Ports**\n   - **Command:** `nmap -p [port range] [target]`\n   - **Explanation:** Scans only the specified ports on the target.\n7. **Save Scan Results**\n   - **Command:** `nmap -oA [output name] [target]`\n   - **Explanation:** Saves the scan results in all formats (normal, XML, and grepable).\n8. **Stealth Scan**\n   - **Command:** `nmap -sS [target]`\n   - **Explanation:** Performs a stealthy SYN scan to avoid detection.\n9. **Script Scan**\n   - **Command:** `nmap -sC [target]`\n   - **Explanation:** Runs default scripts against the target.\n10. **Specifying a Target List**\n   - **Command:** `nmap -iL [list file]`\n   - **Explanation:** Reads targets from a file containing a list of IP addresses or hostnames.\n```\n\n## Interactive \"Do you know this?\" Quiz, and Customized Study Plan\n\n```plaintext\nYou are an expert educator with extensive experience in assessing knowledge depth and providing detailed feedback.\n\n### Task:\nYour task is to ask me open-ended questions on the specified notes or transcript on a specific technology to test my understanding of it.\n\n### Instructions:\n1. Review the provided notes or transcript.\n2. Formulate open-ended questions that assess my understanding of the key concepts, applications, and implications of the technology discussed.\n3. Ensure the questions encourage critical thinking and comprehensive responses.\n4. Provide detailed feedback on my answers, highlighting strengths and areas for improvement.\n5. Ask 5 questions in total to assess my knowledge and readiness to move on to the next lesson.\n\n**Example:**\n- **Question:** Explain how the technology discussed can impact current industry practices. What are the potential benefits and drawbacks?\n- **Feedback:** Your answer should cover various industry practices, specific benefits such as increased efficiency or cost reduction, and potential drawbacks like implementation challenges or ethical concerns.\n```\n\n## Analogy Generator\n\n```plaintext\n### Task:\nCreate analogies for the 5 biggest topics in the page delimited by triple quotes.\n\n### Instructions:\n1. Identify the 5 biggest topics in the provided page content.\n2. For each topic, create an analogy that helps explain or illustrate the concept.\n3. Ensure that each analogy is clear and easy to understand.\n\n### Example:\nIf the topic is \"Artificial Intelligence,\" an analogy might be: \"Artificial Intelligence is like a student who learns from textbooks and experiences to become better at solving problems.\"\n\n### Output Format:\nProvide the analogies in a numbered list format:\n\n1. **Topic 1:** [Analogy]\n2. **Topic 2:** [Analogy]\n3. **Topic 3:** [Analogy]\n4. **Topic 4:** [Analogy]\n5. **Topic 5:** [Analogy]\n```\n\n## Test Me\n\n```plaintext\nI am studying for the HTB CPTS exam and I need to know if I can skip the lesson I'm about to specify. To gauge if I can skip this, please quiz me in an interactive way, asking one question at a time and see if I'm knowledgeable enough based on the content I'm specifying. Ask 5 questions, and make sure they will accurately gauge if I can move on.\n```\n```\n"
  },
  {
    "name": "danielmiessler",
    "path": "integrations/docs/danielmiessler.md",
    "content": "\n### Hi, I'm Daniel Miessler\n\nI'm worried about humanity's future.\n\nMost people built their identities around their jobsâ€”their title, their place in a hierarchy, their usefulness to an organization. AI is about to automate a lot of that, which will cause a crisis for millions.\n\nBut I think this is also an opportunity. If we do it right, AI can free people to discover who they actually areâ€”their purposes, their gifts, the things they'd pursue even if nobody paid them.\n\nThat's what I'm working on: helping humans upgrade themselves for a post-work world.\n\n---\n\n## What I'm Building\n\n**[TELOS](https://github.com/danielmiessler/Telos)** helps you articulate who you are, what you value, and what you're trying to achieveâ€”structured self-knowledge that AI can actually use to help you.\n\n**[PAI](https://github.com/danielmiessler/PAI)** is Personal AI Infrastructureâ€”the scaffolding for building your own AI assistant that knows your context, follows your preferences, and improves over time.\n\n**[Substrate](https://github.com/danielmiessler/Substrate)** is the deeper foundationâ€”a framework for understanding what humans are, what we want, and what progress actually means.\n\n**[Fabric](https://github.com/danielmiessler/fabric)** is a collection of crowdsourced prompts for common AI tasksâ€”extract wisdom, analyze security, summarize content, and more.\n\n**[Human 3.0](https://human3.ai)** is the philosophyâ€”a framework for human flourishing when AI handles the routine work.\n\n---\n\n## Projects\n\n| Project | What It Does | |\n|---------|--------------|---|\n| **[TELOS](https://github.com/danielmiessler/Telos)** | Framework for deep personal context | ![Stars](https://img.shields.io/github/stars/danielmiessler/Telos?style=flat-square) |\n| **[PAI](https://github.com/danielmiessler/PAI)** | Personal AI Infrastructureâ€”build your own AI assistant | ![Stars](https://img.shields.io/github/stars/danielmiessler/PAI?style=flat-square) |\n| **[Substrate](https://github.com/danielmiessler/Substrate)** | Philosophy of human understanding and progress | ![Stars](https://img.shields.io/github/stars/danielmiessler/Substrate?style=flat-square) |\n| **[Fabric](https://github.com/danielmiessler/fabric)** | Crowdsourced AI prompts for common tasks | ![Stars](https://img.shields.io/github/stars/danielmiessler/fabric?style=flat-square) |\n| **[SecLists](https://github.com/danielmiessler/SecLists)** | Security testing wordlists and resources | ![Stars](https://img.shields.io/github/stars/danielmiessler/SecLists?style=flat-square) |\n\n---\n\n## Recent Writing\n\n| | |\n|---|---|\n| [AI Changes I Expect in 2026](https://danielmiessler.com/blog/ai-changes-2026) | [The Bubble Is Labor](https://danielmiessler.com/blog/real-bubble-is-human-labor) |\n| [Cybersecurity Changes I Expect in 2026](https://danielmiessler.com/blog/cybersecurity-changes-2026) | [Thoughts on Doctorow's 'Reverse Centaurs' AI Talk](https://danielmiessler.com/blog/thoughts-on-doctorow-ai-essay) |\n| [AI-enabled Self-software](https://danielmiessler.com/blog/ai-enabled-self-software) | [I Built Two Claude Code Features Before Anthropic](https://danielmiessler.com/blog/i-built-two-claude-code-features-before-anthropic-released-them) |\n| [China is Becoming Private Equity for the World](https://danielmiessler.com/blog/china-private-equity-world) | [Autonomous Car Safety Data](https://danielmiessler.com/blog/autonomous-cars-safety-data) |\n| [A Personal AI Maturity Model (PAIMM)](https://danielmiessler.com/blog/personal-ai-maturity-model) | [How to Fix an Unbearably Slow iCloud Drive](https://danielmiessler.com/blog/fix-slow-icloud) |\n| [We Need a New Type of Cybersecurity Product](https://danielmiessler.com/blog/new-type-cybersecurity-product) | [Is Prompt Injection a Vulnerability?](https://danielmiessler.com/blog/is-prompt-injection-a-vulnerability) |\n| [Anthropic's Vision Advantage Like Apple's](https://danielmiessler.com/blog/anthropics-vision-advantage) | [Thoughts on Prompt Injection OPSEC](https://danielmiessler.com/blog/thoughts-on-prompt-injection-opsec) |\n\n[â†’ All 3,000+ essays](https://danielmiessler.com/blog/)\n\n---\n\n## Connect\n\n[Blog](https://danielmiessler.com) ãƒ» [Newsletter](https://danielmiessler.com/subscribe) ãƒ» [YouTube](https://youtube.com/@unsupervised-learning) ãƒ» [X](https://x.com/danielmiessler) ãƒ» [LinkedIn](https://linkedin.com/in/danielmiessler) ãƒ» [All Repos](https://github.com/danielmiessler?tab=repositories)\n\n<a href=\"https://github.com/sponsors/danielmiessler\">\n  <img src=\"https://img.shields.io/badge/Sponsor_My_Open_Source_Work-â¤ï¸-ea4aaa?style=for-the-badge\" alt=\"Sponsor\" />\n</a>\n"
  },
  {
    "name": "dyad",
    "path": "integrations/docs/dyad.md",
    "content": "# Dyad\n\nDyad is a local, open-source AI app builder. It's fast, private, and fully under your control â€” like Lovable, v0, or Bolt, but running right on your machine.\n\n[![Image](https://github.com/user-attachments/assets/f6c83dfc-6ffd-4d32-93dd-4b9c46d17790)](https://dyad.sh/)\n\nMore info at: [https://dyad.sh/](https://dyad.sh/)\n\n## ðŸš€ Features\n\n- âš¡ï¸ **Local**: Fast, private and no lock-in.\n- ðŸ›  **Bring your own keys**: Use your own AI API keys â€” no vendor lock-in.\n- ðŸ–¥ï¸ **Cross-platform**: Easy to run on Mac or Windows.\n\n## ðŸ“¦ Download\n\nNo sign-up required. Just download and go.\n\n### [ðŸ‘‰ Download for your platform](https://www.dyad.sh/#download)\n\n## ðŸ¤ Community\n\nJoin our growing community of AI app builders on **Reddit**: [r/dyadbuilders](https://www.reddit.com/r/dyadbuilders/) - share your projects and get help from the community!\n\n## ðŸ› ï¸ Contributing\n\n**Dyad** is open-source (see License info below).\n\nIf you're interested in contributing to dyad, please read our [contributing](./CONTRIBUTING.md) doc.\n\n## License\n\n- All the code in this repo outside of `src/pro` is open-source and licensed under Apache 2.0 - see [LICENSE](./LICENSE).\n- All the code in this repo within `src/pro` is fair-source and licensed under [Functional Source License 1.1 Apache 2.0](https://fsl.software/) - see [LICENSE](./src/pro/LICENSE).\n\n---\n\n# docs/agent_architecture.md\n\n# Agent Architecture\n\nPreviously, Dyad used a pseudo tool-calling strategy using custom XML instead of model's formal tool calling capabilities. Now that models have gotten much better with tool calling, particularly with parallel tool calling, it's beneficial to use a more standard tool calling approach which will also make it much easier to add new tools.\n\n- The heart of the local agent is in `src/pro/main/ipc/handlers/local_agent/local_agent_handler.ts` which contains the core agent loop: which keeps calling the LLM until it chooses not to do a tool call or hits the maximum number of steps for the turn.\n- `src/pro/main/ipc/handlers/local_agent/tool_definitions.ts` contains the list of all the tools available to the Dyad local agent.\n\n## Add a tool\n\nIf you want to add a new tool, you will want to create a new tool in the `src/pro/main/ipc/handlers/local_agent/tools` directory. You can look at the existing tools as examples.\n\nThen, import the tool and include it in `src/pro/main/ipc/handlers/local_agent/tool_definitions.ts`\n\nFinally, you will need to define how to render the custom XML tag (e.g. `<dyad-$foo-tool-name>`) inside `src/components/chat/DyadMarkdownParser.tsx` which will typically involve creating a new React component to render the custom XML tag.\n\n## Testing\n\nYou can add an E2E test by looking at the existing local agent E2E tests which are named like `e2e-tests/local_agent*.spec.ts`\n\nYou can define a tool call testing fixture at `e2e-tests/fixtures/engine` which allows you to simulate a tool call.\n\n# docs/architecture.md\n\n# Dyad Architecture\n\nThis doc describes how the Dyad desktop app works at a high-level. If something is out of date, please feel free to suggest a change via a pull request.\n\n## Overview\n\nDyad is an Electron app that is a local, open-source alternative to AI app builders like Lovable, v0, and Bolt. While the specifics of how other AI app builders are constructed aren't publicly documented, there is available information like [system prompts](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools) about these other app builders.\n\n## Electron Architecture\n\nIf you're not familiar with Electron apps, they are similar to a full-stack JavaScript app where there's a client-side called the **renderer process** which executes the UI code like React and then there's a Node.js process called the **main process** which is comparable to the server-side portion of a full-stack app. The main process is privileged, meaning it has access to the filesystem and other system resources, whereas the renderer process is sandboxed. The renderer process can communicate to the main process using [IPCs](https://en.wikipedia.org/wiki/Inter-process_communication) which is similar to how the browser communicates to the server using HTTP requests.\n\n## Life of a request\n\nThe core workflow of Dyad is that a user sends a prompt to the AI which edits the code and is reflected in the preview. We'll break this down step-by-step.\n\n1. **Constructing an LLM request** - the LLM request that Dyad sends consists of much more than the prompt (i.e. user input). It includes, by default, the entire codebase as well as a detailed [system prompt](https://github.com/dyad-sh/dyad/blob/main/src/prompts/system_prompt.ts) which gives the LLM instructions to respond in a specific XML-like format (e.g. `<dyad-write path=\"path/to/file.ts\">console.log(\"hi\")</dyad-write>`).\n2. **Stream the LLM response to the UI** - It's important to provide visual feedback to the user otherwise they're waiting for several minutes without knowing what's happening so we stream the LLM response and show the LLM response. We have a specialized [Markdown parser](https://github.com/dyad-sh/dyad/blob/main/src/components/chat/DyadMarkdownParser.tsx) which parses these `<dyad-*>` tags like the `<dyad-write>` tag shown earlier, so we can display the LLM output in a nice UI rather than just printing out raw XML-like text.\n3. **Process the LLM response** - Once the LLM response has finished, and the user has approved the changes, the [response processor](https://github.com/dyad-sh/dyad/blob/main/src/ipc/processors/response_processor.ts) in the main process applies these changes. Essentially each `<dyad-*>` tag described in the [system prompt](https://github.com/dyad-sh/dyad/blob/main/src/prompts/system_prompt.ts) maps to specific logic in the response processor, e.g. writing a file, deleting a file, adding a new NPM package, etc.\n\nTo recap, Dyad essentially tells the LLM about a bunch of tools like writing files using the `<dyad-*>` tags, the renderer process displays these Dyad tags in a nice UI and the main process executes these Dyad tags to apply the changes.\n\n## FAQ\n\n### Why not use actual tool calls?\n\nOne thing that may seem strange is that we don't use actual function calling/tool calling capabilities of the AI and instead use these XML-like syntax which simulate tool calling. This is something I observed from studying the [system prompts](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools) of other app builders.\n\nI think the two main reasons to use this XML-like format instead of actual tool calling is that:\n\n1. You can call many tools at once, although some models allow [parallel calls](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling#parallel-function-calling), many don't.\n2. There's also [evidence](https://aider.chat/2024/08/14/code-in-json.html) that forcing LLMs to return code in JSON (which is essentially what tool calling would entail here) negatively affects the quality.\n\nHowever, many AI editors _do_ heavily rely on tool calling and this is something that we're evaluating, particularly with upcoming MCP support.\n\n### Why isn't Dyad more agentic?\n\nMany other systems (e.g. Cursor) are much more agentic than Dyad. For example, they will call many tools and do things like create a plan, use command-line tools to search through the codebase, run linters and tests and automatically fix the code based on those output.\n\nDyad, on the other hand, has a relatively simple agentic loop. We will fix TypeScript compiler errors if Auto-fix problems is enabled, but otherwise it's usually a single request to the AI.\n\nThe biggest issue with complex agentic workflows is that they can get very expensive very quickly! It's not uncommon to see users report spending a few dollars with a single request because under the hood, that single user requests turns into dozens of LLM requests. To keep Dyad as cost-efficient as possible, we've avoided complex agentic workflows at least until the cost of LLMs is more affordable.\n\n### Why does Dyad send the entire codebase with each AI request?\n\nSending the right context to the AI has been rightfully emphasized as important, so much so that the term [\"context engineering\"](https://www.philschmid.de/context-engineering) is now in vogue.\n\nSending the entire codebase is the simplest approach and quite effective for small codebases. Another approach is for the user to explicitly select the part of the codebase to use as context. This can be done through the [select component](https://www.dyad.sh/docs/releases/0.8.0) feature or [manual context management](https://www.dyad.sh/docs/guides/large-apps#manual-context-management).\n\nHowever, both of these approaches require users to manually select the right files which isn't always practical. Dyad's [Smart Context](https://www.dyad.sh/docs/guides/ai-models/pro-modes#smart-context) feature essentially uses smaller models to filter out the most important files in the given chat. That said, we are constantly experimenting with new approaches to context selection as it's quite a difficult problem.\n\nOne approach that we don't use is a more agentic-style like what Claude Code and Cursor does where it iteratively searches and navigates through a codebase using tool calls. The main reason we don't do this is due to cost (see the above question: [Why isn't Dyad more agentic](#why-isnt-dyad-more-agentic)).\n"
  },
  {
    "name": "khoj",
    "path": "integrations/docs/khoj.md",
    "content": "<p align=\"center\"><img src=\"https://assets.khoj.dev/khoj-logo-sideways-1200x540.png\" width=\"230\" alt=\"Khoj Logo\"></p>\n\n<div align=\"center\">\n\n[![test](https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg)](https://github.com/khoj-ai/khoj/actions/workflows/test.yml)\n[![docker](https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg)](https://github.com/khoj-ai/khoj/pkgs/container/khoj)\n[![pypi](https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/khoj/)\n[![discord](https://img.shields.io/discord/1112065956647284756?style=plastic&label=discord)](https://discord.gg/BDgyabRM6e)\n\n</div>\n\n<div align=\"center\">\n<b>Your AI second brain</b>\n</div>\n\n<br />\n\n<div align=\"center\">\n\n[ðŸ“‘ Docs](https://docs.khoj.dev)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ðŸŒ Web](https://khoj.dev)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ðŸ”¥ App](https://app.khoj.dev)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ðŸ’¬ Discord](https://discord.gg/BDgyabRM6e)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[âœðŸ½ Blog](https://blog.khoj.dev)\n\n<a href=\"https://trendshift.io/repositories/10318\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/10318\" alt=\"khoj-ai%2Fkhoj | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n***\n\n### ðŸŽ New\n* Start any message with `/research` to try out the experimental research mode with Khoj.\n* Anyone can now [create custom agents](https://blog.khoj.dev/posts/create-agents-on-khoj/) with tunable personality, tools and knowledge bases.\n* [Read](https://blog.khoj.dev/posts/evaluate-khoj-quality/) about Khoj's excellent performance on modern retrieval and reasoning benchmarks.\n\n***\n\n## Overview\n\n[Khoj](https://khoj.dev) is a personal AI app to extend your capabilities. It smoothly scales up from an on-device personal AI to a cloud-scale enterprise AI.\n\n- Chat with any local or online LLM (e.g llama3, qwen, gemma, mistral, gpt, claude, gemini, deepseek).\n- Get answers from the internet and your docs (including image, pdf, markdown, org-mode, word, notion files).\n- Access it from your Browser, Obsidian, Emacs, Desktop, Phone or Whatsapp.\n- Create agents with custom knowledge, persona, chat model and tools to take on any role.\n- Automate away repetitive research. Get personal newsletters and smart notifications delivered to your inbox.\n- Find relevant docs quickly and easily using our advanced semantic search.\n- Generate images, talk out loud, play your messages.\n- Khoj is open-source, self-hostable. Always.\n- Run it privately on [your computer](https://docs.khoj.dev/get-started/setup) or try it on our [cloud app](https://app.khoj.dev).\n\n***\n\n## See it in action\n\n![demo_chat](https://github.com/khoj-ai/khoj/blob/master/documentation/assets/img/quadratic_equation_khoj_web.gif?raw=true)\n\nGo to https://app.khoj.dev to see Khoj live.\n\n## Full feature list\nYou can see the full feature list [here](https://docs.khoj.dev/category/features).\n\n## Self-Host\n\nTo get started with self-hosting Khoj, [read the docs](https://docs.khoj.dev/get-started/setup).\n\n## Enterprise\n\nKhoj is available as a cloud service, on-premises, or as a hybrid solution. To learn more about Khoj Enterprise, [visit our website](https://khoj.dev/teams).\n\n## Frequently Asked Questions (FAQ)\n\nQ: Can I use Khoj without self-hosting?\n\nYes! You can use Khoj right away at [https://app.khoj.dev](https://app.khoj.dev) â€” no setup required.\n\nQ: What kinds of documents can Khoj read?\n\nKhoj supports a wide variety: PDFs, Markdown, Notion, Word docs, org-mode files, and more.\n\nQ: How can I make my own agent?\n\nCheck out [this blog post](https://blog.khoj.dev/posts/create-agents-on-khoj/) for a step-by-step guide to custom agents.\nFor more questions, head over to our [Discord](https://discord.gg/BDgyabRM6e)!\n\n\n## Contributors\nCheers to our awesome contributors! ðŸŽ‰\n\n<a href=\"https://github.com/khoj-ai/khoj/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=khoj-ai/khoj\" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n### Interested in Contributing?\nKhoj is open source. It is sustained by the community and weâ€™d love for you to join it! Whether youâ€™re a coder, designer, writer, or enthusiast, thereâ€™s a place for you.\n\nWhy Contribute?\n- Make an Impact: Help build, test and improve a tool used by thousands to boost productivity.\n- Learn & Grow: Work on cutting-edge AI, LLMs, and semantic search technologies.\n\nYou can help us build new features, improve the project documentation, report issues and fix bugs. If you're a developer, please see our [Contributing Guidelines](https://docs.khoj.dev/contributing/development) and check out [good first issues](https://github.com/khoj-ai/khoj/contribute) to work on.\n"
  },
  {
    "name": "local-deepthink",
    "path": "integrations/docs/local-deepthink.md",
    "content": "![thumbnail](https://github.com/user-attachments/assets/13694758-a5c9-40c5-9c07-c7a168e660cf)\n\n# local-deepthink: Democratizing Deep Algorithmic Thought ðŸ§ \n\nI've been thinking a lot about how we, as people, develop complex ideas and algorithms. It's rarely a single, brilliant flash of insight. Our minds are shaped by the countless small interactions we haveâ€”a conversation here, an article there. This environment of constant, varied input seems just as important as the act of thinking itself.\n\nI wanted to see if I could recreate a small-scale version of that \"soup\" required for true algorithmic insight for local LLMs. The result is this project, **local-deepthink**. It's a system that runs a novel conceptual algorithm called a **Qualitative Neural Network (QNN)**. In a QNN, different AI agents are treated like \"neurons\" that collaborate and critique each other to refine complex solutions, effectively trading slower response times for higher quality and more robust outputs.\n\n## âš ï¸ Alpha Software - We Need Your Help! âš ï¸\nPlease be aware that local-deepthink is currently in an alpha stage. It is experimental research software. You may encounter bugs, unexpected behavior, or breaking changes.\n\nYour feedback is invaluable. If you run into a crash or have ideas, please **open an issue** on our GitHub repository with your graph monitor trace log. Helping us identify and squash bugs is one of the most important contributions you can make right now!\n\n\n## **Is true \"deep thinking\" only for trillion-dollar companies?**\n\n**local-deepthink** is a research platform that challenges the paradigm of centralized, proprietary AI. While systems like Google's DeepMind offer powerful reasoning by giving their massive models more \"thinking time\" in a closed environment (for a high price), local-deepthink explores a different path: **emergent intelligence on affordable local hardware**. We simulate a society of AI agents that collaborate, evolve, and deepen their understanding of a complex problem collectively over time.\n\nEssentially, you can think of this project as a way to **max out a model's performance on complex algorithmic tasks by trading response time for quality**. The best part is that you don't need a supercomputer. local-deepthink is designed to turn even a modest 32gb RAM CPU-only laptop into a powerful \"thought mining\" rig. ðŸ’»â›ï¸ By leveraging efficient local models, you can leave the network running for hours or even days, allowing it to \"mine\" a sophisticated solution to a hard algorithmic problem. It's a fundamental shift: trading brute-force, instantaneous computation for the power of time, iteration, and distributed collaboration.\n\n## Use Case: Advanced Algorithm Generation\nThe **Qualitative Neural Network (QNN)** algorithm that powers this system is great for complex problems where the only clue you have is a vague question or a high-level conceptual goal. With the system now refocused exclusively on code and algorithm generation, its primary use case is to tackle difficult programming challenges.\n\n*   **For Programmers & Researchers: Full Stack App Generation**: This is an experimental feature. As of right now the system can accumulate attempts and offer one final synthetized solution. This works well for algorithm design. Now we are trying to decompose the synthesis in modules that are aftwerads stitched together through abstract interface cards. It wont work on a few epochs, so if you can test it on high epochs and open an issue, it would be appreciated.\n\n## Changelog\n\n*  **Hidden-layer-fixed**: Issue with meta-prompting in the hidden layer fixed. Agents are now moderately divergent from a strict skill alignment, as originally intended. Specialization is one thing; the individual that serves as recipient for the toolset is another. Keeping both distinct is important to make answers smoother.\n*   **QNN Export/Import:** You can now export the entire state of a trained agent network (QNN) to a JSON file. This QNN can be imported and used for inference on new problems without rerunning the entire epoch process.\n*   **Code Generation & Sandbox:** The system can now generate, synthesize, and safely execute Python code. A new `code_execution` node validates the final code, and successful modules provide context for future epochs.\n*   **Dynamic Problem Re-framing:** The network can now assess its own progress. After each cycle (epoch), it formulates a new, more advanced problem that builds upon its previous solution, forcing the agents to continuously deepen their understanding.\n*   **Divide and Conquer - Automatic Problem Decomposition:** local-deepthink now starts by breaking down the user's initial problem into smaller, granular sub-problems, assigning each agent a unique piece of the puzzle.\n*   **Perplexity Metrics & Chart:** A `metrics` node calculates the average perplexity of all agent outputs after each epoch, plotted on a live chart in the GUI.\n*   **Dynamic Summarization:** A specialized chain now automatically creates a concise summary of an agent's older memories if its memory log gets too long, preserving key insights while managing context length.\n\n## The Core Idea: Qualitative Backpropagation\n\nThe core experiment is the **Qualitative Neural Network (QNN)**, an algorithm inspired by backpropagation in traditional neural networks. It's a numerical algorithm, of course, but what if the principle could be applied qualitatively? Instead of sending back a numerical error signal, you send back a \"reflection.\"\n\nAfter the network produces a solution, a \"reflection pass\" analyzes the result and **automatically re-writes the core system prompts** of the agents that contributed. The goal is for the network to \"learn\" from its own output over multiple cycles (epochs), refining not just its answers, but its own internal structure and approach. QNNs are also extremely human-interpretable, unlike their numerical counterparts.\n\n### The Trade-Off: Speed for Depth\n\nThe obvious trade-off here is speed. A 6-layer network with 6 agents per layer, running for 20 epochs, can easily take 12 hours to complete. You're trading quick computation for a slow, iterative process of refinement. The algorithm excels in problems where creativity and insight override pure precision, like developing new frameworks in the social sciences.\n\n## The QNN Algorithm: From Individual Agents to a Collective Mind\n\nThe core of local-deepthink is the novel QNN algorithm that orchestrates LLM agents into a dynamic, layered network. This architecture facilitates a \"forward pass\" for problem-solving, a \"reflection pass\" for learning, and a final \"harvest pass\" for knowledge extraction.\n\n### The Forward Pass\n\nIn a QNN, the \"weights\" and \"biases\" of the network are not numerical values but the rich, descriptive personas of its agents, defined in natural language.\n\n1.  **Input Layer & Decomposition**: The process starts with a user's high-level problem. A `master strategist` node first **decomposes this problem into smaller, distinct sub-problems**. These are then assigned to the first layer of agents.\n2.  **Building Depth with Dense Layers**: A `dense-spanner` chain analyzes the agents of the preceding layer and spawns a new agent in the next layer, specifically engineered to tackle a tailored challenge.\n3.  **Action**: A user's prompt initiates a cascade of information through the network until the final layer is reached, constituting a full \"forward pass\" of collaborative inference.\n\n### The Reflection Pass: Learning Through Evolving Goals\n\nThis is where a QNN truly differs from a simple multi-agent system. Instead of simply correcting errors, the network learns by continuously raising the bar.\n\n1.  **Synthesis and Metrics**: A `synthesis_node` merges the final outputs into a single solution, and a `metrics_node` calculates a perplexity score for the epoch.\n2.  **Problem Re-framing**: The core of the learning loop. A `problem_reframer` node analyzes the synthesized solution and formulates a new, more ambitious problem that represents the \"next logical step.\" This prevents the network from stagnating and pushes it toward deeper insights.\n3.  **Decomposition of the New Problem**: The newly framed problem is then broken down again into a new set of granular sub-problems.\n4.  **Updating the \"Neural\" Weights**: This new set of sub-problems is propagated backward through the network. An `update_agent_prompts_node` modifies each agent's core system prompt to align with its new, more advanced task for the next epoch.\n\n### The Final Harvest Pass: Consolidating Knowledge\n\n1.  **Archival and RAG Indexing**: All agent outputs from every epoch are used to build a comprehensive RAPTOR RAG index.\n2.  **Pause for Interactive Chat & Diagnosis**: The network pauses, allowing you to directly query the RAG index. Because QNNs are highly interpretable, you can even diagnose a specific \"neuron\" by asking the chat about `agent_1_1` to get that specific agent's entire history.\n3.  **Interrogation and Synthesis**: When you're done, your chat is added to the knowledge base. An `interrogator` agent then formulates expert-level questions about the original problem based on your points of interest.\n4.  **Generating the Final Report**: A `paper_formatter` agent uses the RAG index to answer these questions, synthesizing the information into formal research papers. The final output is a downloadable ZIP archive of this report.\n\n## Vision & Long-Term Roadmap: Training a World Language Model\n\nEvery local-deepthink run generates a complete, structured trace of a multi-agent collaborative processâ€”a dataset capturing the evolution of thought. With the new export feature, these QNN JSON files can now be collected. We see this as **powerful, multi-dimensional data for training next-generation reasoning models.**\n\nOur ultimate objective is to use this data to train a true **\"World Language Model\" (WLM)**. A WLM would move beyond predicting the next token to understanding the fundamental patterns of collaboration, critique, and collective intelligence. The exciting possibility is that fine-tuning a model on thousands of these QNN logs might make static system prompts obsolete, as the trained LLM would learn to implicitly figure them out and dynamically switch its reasoning process on the fly.\n\n## Mid-Term Research Goals & How You Can Help\nThis is still alpha software, and we need your help. Besides the value you get after \"mining\" a solution, it's also super entertaining to watch the neurons interact with each other! If you have the hardware, please consider helping us benchmark.\n\n*   **Hunt Bugs**: If you run into a crash, please open an issue with your graph monitor trace log.\n*   **Deep Runs & Benchmarking**: I don't have access to systems like Google's DeepMind, so it would be fantastic if someone with a powerful local rig could run and benchmark moderate-to-large QNNs.\n*   **Thinking Models Support**: Help integrate support for dedicated \"thinking models\".\n*   **P2P Networking for Distributed Mining:** My background is in Python and AI, not distributed systems. A long-term vision is a P2P networking layer to allow multiple users to connect their instances and collectively \"mine\" a solution to a massive problem. If you have experience here, I would love to collaborate.\n*   **Checkpoint Import/Export**: A basic version is implemented, but expanding this to allow saving a run mid-epoch would make the system more crash-resistant.\n\n## What's Next?\nThe current focus is on polishing and debugging existing features to reach a beta phase. After that, the next iteration will introduce specialized modes and advanced capabilities:\n\n*   **Recursive Module Stitching:** The initial implementation allows code validation and context feedback. The next step is to enable the system to design, code, and recursively assemble different software modules to create complex, full-stack applications from a high-level prompt.\n*   **Export your QNN:** This is now implemented! You can import and export your QNN in plain JSON format, so other people can prompt it, at just a few MBs of size.\n\n## Hyperparameters & Hardware Guidelines âš™ï¸\n\n*   **`CoT trace depth`**: The number of layers in your agent network.\n*   **`Number of epochs`**: One full cycle of a forward and reflection pass.\n*   **`Vector word size`**: The number of \"seed verbs\" for initial agent creation.\n*   **`Number of Questions for Final Harvest`**: The number of questions the `interrogator` agent generates.\n*   **`Prompt alignment` (0.1 - 2.0)**: How strongly an agent's career is influenced by the user's prompt.\n*   **`Density` (0.1 - 2.0)**: Modulates the influence of the previous layer when creating new agents.\n*   **`Learning rate` (0.1 - 2.0)**: Controls the magnitude of change an agent makes to its prompt.\n\n#### Hardware Recommendations:\n*   **CPU-Only Laptop (32GB RAM)**: 2x2 or 4x4 networks with 3-4 epochs are ideal.\n*   **High-End Rig (64GB RAM + 24GB GPU)**: 6x6 up to 10x10 networks with 2-10 epochs should be doable in 20-45 minutes.\n\n## Technical Setup\n\n*   **Backend**: FastAPI, LangChain, LangGraph, Ollama\n*   **Frontend**: HTML, CSS, JavaScript\n\n### Installation and Execution\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/iblameandrew/local-deepthink\n    cd local-deepthink\n    ```\n2.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv venv\n    # On Windows\n    .\\venv\\Scripts\\activate\n    # On macOS/Linux\n    source venv/bin/activate\n    ```\n3.  **Install dependencies:** `pip install -r requirements.txt`.\n4.  **Install and Run Ollama**:\n    *   Follow the official instructions to install Ollama.\n    *   Download a primary model for the agents (default is `dengcao/Qwen3-3B-A3B-Instruct-2507:latest`).\n        ```bash\n        ollama pull dengcao/Qwen3-3B-A3B-Instruct-2507:latest\n        ```\n    *   **Download the compulsory summarization model.** local-deepthink requires `qwen3:1.7b` for its internal processes.\n        ```bash\n        ollama pull qwen3:1.7b\n        ```\n    *   Ensure the Ollama application is running.\n5.  **Run the application:**\n    ```bash\n    uvicorn new:app --reload\n    ```\n6.  **Access the GUI:** Open your browser to `http://127.0.0.1:8000`.\n\n## How It Works\n\n1.  **Architect the Network**: Use the GUI to set the hyperparameters for your QNN.\n2.  **Pose a Problem**: Enter the high-level prompt you want the network to solve.\n3.  **Build and Run**: Click the \"Build and Run Graph\" button.\n4.  **Observe the Emergence**: Monitor the process in the real-time log viewer.\n5.  **Chat and Diagnose**: Once epochs are complete, use the chat interface to query the RAG index of the network's entire thought process.\n6.  **Harvest and Download**: When finished chatting, click \"HARVEST\" to generate and download the final ZIP report.\n7.  **(Optional) Export, Import, and Infer**: Use the `Export QNN` button to save your network. Later, use the `Import QNN` button to load it and run new prompts against the trained agent structure.\n\nItâ€™s an open-source experiment, and Iâ€™d be grateful for any thoughts, feedback, or ideas you might have. Please support the repo if you want to see more open-source work like this!\n\nThanks."
  },
  {
    "name": "Local-NotebookLM",
    "path": "integrations/docs/Local-NotebookLM.md",
    "content": "# Local-NotebookLM\n\n![logo](logo.jpeg)\n\nA local AI-powered tool that converts PDF documents into engaging audio's such as podcasts, using local LLMs and TTS models.\n\n## Features\n\n- PDF text extraction and processing\n- Customizable podcast generation with different styles and lengths\n- Support for various LLM providers (OpenAI, Groq, LMStudio, Ollama, Azure)\n- Text-to-Speech conversion with voice selection\n- Fully configurable pipeline\n- Preference-based content focus\n- Programmatic API for integration in other projects\n- FastAPI server for web-based access\n- Example podcast included for demonstration\n\n#### Here is a quick example, can you guess what paper they're talking about?\n\n<audio controls>\n  <source src=\"./examples/podcast.wav\" type=\"audio/mpeg\">\n  Your browser does not support the audio element. You can manualy download the file here './examples/podcast.wav'.\n</audio>\n\n## Prerequisites\n\n- Python 3.12+\n- Local LLM server (optional, for local inference)\n- Local TTS server (optional, for local audio generation)\n- At least 8GB RAM (16GB+ recommended for local models)\n- 10GB+ free disk space\n\n## Installation\n\n### From PyPI\n\n```bash\npip install local-notebooklm\n```\n\n### From source\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/Goekdeniz-Guelmez/Local-NotebookLM.git\ncd Local-NotebookLM\n```\n\n2. Create and activate a virtual environment (conda works too):\n\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows, use: venv\\Scripts\\activate\n```\n\n3. Install the required packages:\n\n```bash\npip install -r requirements.txt\n```\n\n## Running with Docker Compose\n\nYou can also run both the Gradio Web UI and FastAPI server using Docker Compose.\n\n### Prerequisites\n\n- Docker and Docker Compose installed on your system\n\n### Steps\n\n1. Open a terminal and navigate to the `docker/` folder inside the project:\n\n```bash\ncd docker\n```\n\n2. Build and start the containers:\n\n```bash\ndocker-compose up --build\n```\n\nThis command will:\n\n- Start the Gradio Web UI at [http://localhost:7860](http://localhost:7860)\n- Start the FastAPI server at [http://localhost:8000](http://localhost:8000)\n\nYou can access the web interface or use the API endpoints after running the command.\n\nTo stop the services, press `CTRL+C` and then run:\n\n```bash\ndocker-compose down\n```\n## Optional pre requisites\n### Local TTS server\n- Follow one installation type (docker, docker-compose, uv) at https://github.com/remsky/Kokoro-FastAPI\n- Test in your browser that http://localhost:8880/v1 return the json: {\"detail\":\"Not Found\"}\n  \n## Example Output\n\nThe repository includes an example podcast in `examples/podcast.wav` to demonstrate the quality and format of the output. The models used are: gpt4o and Mini with tts-hs on Azure. You can listen to this example to get a sense of what Local-NotebookLM can produce before running it on your own PDFs.\n\n## Configuration\n\nYou can use the default configuration or create a custom JSON config file with the following structure:\n\n```json\n{\n    \"Co-Host-Speaker-1-Voice\": \"af_sky+af_bella\",\n    \"Co-Host-Speaker-2-Voice\": \"af_echo\",\n    \"Co-Host-Speaker-3-Voice\": \"af_nova\",\n    \"Co-Host-Speaker-4-Voice\": \"af_shimmer\",\n    \"Host-Speaker-Voice\": \"af_alloy\",\n\n    \"Small-Text-Model\": {\n        \"provider\": {\n            \"name\": \"groq\",\n            \"key\": \"your-api-key\"\n        },\n        \"model\": \"llama-3.2-90b-vision-preview\"\n    },\n\n    \"Big-Text-Model\": {\n        \"provider\": {\n            \"name\": \"groq\",\n            \"key\": \"your-api-key\"\n        },\n        \"model\": \"llama-3.2-90b-vision-preview\"\n    },\n\n    \"Text-To-Speech-Model\": {\n        \"provider\": {\n            \"name\": \"custom\",\n            \"endpoint\": \"http://localhost:8880/v1\",\n            \"key\": \"not-needed\"\n        },\n        \"model\": \"kokoro\",\n        \"audio_format\": \"wav\"\n    },\n\n    \"Step1\": {\n        \"system\": \"\",\n        \"max_tokens\": 1028,\n        \"temperature\": 0.7,\n        \"chunk_size\": 1000,\n        \"max_chars\": 100000\n    },\n\n    \"Step2\": {\n        \"system\": \"\",\n        \"max_tokens\": 8126,\n        \"temperature\": 1,\n        \"chunk_token_limit\": 2000,\n        \"overlap_percent\": 10\n    },\n\n    \"Step3\": {\n        \"system\": \"\",\n        \"max_tokens\": 8126,\n        \"temperature\": 1,\n        \"chunk_token_limit\": 2000,\n        \"overlap_percent\": 20\n    }\n}\n```\n\n### Provider Options\n\nThe following provider options are supported:\n\n- **OpenAI**: Use OpenAI's API\n  ```json\n  \"provider\": {\n      \"name\": \"openai\",\n      \"key\": \"your-openai-api-key\"\n  }\n  ```\n\n- **Groq**: Use Groq's API for faster inference\n  ```json\n  \"provider\": {\n      \"name\": \"groq\",\n      \"key\": \"your-groq-api-key\"\n  }\n  ```\n\n- **Azure OpenAI**: Use Azure's OpenAI service\n  ```json\n  \"provider\": {\n      \"name\": \"azure\",\n      \"key\": \"your-azure-api-key\",\n      \"endpoint\": \"your-azure-endpoint\",\n      \"version\": \"api-version\"\n  }\n  ```\n\n- **LMStudio**: Use a local LMStudio server\n  ```json\n  \"provider\": {\n      \"name\": \"lmstudio\",\n      \"endpoint\": \"http://localhost:1234/v1\",\n      \"key\": \"not-needed\"\n  }\n  ```\n\n- **Ollama**: Use a local Ollama server\n  ```json\n  \"provider\": {\n      \"name\": \"ollama\",\n      \"endpoint\": \"http://localhost:11434\",\n      \"key\": \"not-needed\"\n  }\n  ```\n\n- **Google generative AI**: Use Google's API\n  ```json\n  \"provider\": {\n      \"name\": \"google\",\n      \"key\": \"your-google-genai-api-key\"\n  }\n  ```\n\n- **Anthropic**: Use Anthropic's API\n  ```json\n  \"provider\": {\n      \"name\": \"anthropic\",\n      \"key\": \"your-anthropic-api-key\"\n  }\n  ```\n\n- **Elevenlabs**: Use Elevenlabs's API\n  ```json\n  \"provider\": {\n      \"name\": \"elevenlabs\",\n      \"key\": \"your-elevenlabs-api-key\"\n  }\n  ```\n\n- **Custom**: Use any OpenAI-compatible API\n  ```json\n  \"provider\": {\n      \"name\": \"custom\",\n      \"endpoint\": \"your-custom-endpoint\",\n      \"key\": \"your-api-key-or-not-needed\"\n  }\n  ```\n\n## Usage\n\n### Command Line Interface\n\nRun the script with the following command:\n\n```bash\npython -m local_notebooklm.start --pdf PATH_TO_PDF [options]\n```\n\n#### Available Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--pdf` | Path to the PDF file (required) | - |\n| `--config` | Path to custom config file | Uses base_config |\n| `--format` | Output format type (summary, podcast, article, interview, panel-discussion, debate, narration, storytelling, explainer, lecture, tutorial, q-and-a, news-report, executive-brief, meeting, analysis) | podcast |\n| `--length` | Content length (short, medium, long, very-long) | medium |\n| `--style` | Content style (normal, casual, formal, technical, academic, friendly, gen-z, funny) | normal |\n| `--preference` | Additional focus preferences or instructions | None |\n| `--language` | Language the audio should be in | english |\n| `--output-dir` | Directory to store output files | ./output |\n\n#### Format Types\n\nLocal-NotebookLM now supports both single-speaker and two-speaker formats:\n\n**Single-Speaker Formats:**\n- summary\n- narration\n- storytelling\n- explainer\n- lecture\n- tutorial\n- news-report\n- executive-brief\n- analysis\n\n**Two-Speaker Formats:**\n- podcast\n- interview\n- panel-discussion\n- debate\n- q-and-a\n- meeting\n\n**Multi-Speaker Formats:**\n- panel-discussion (3, 4, or 5 speakers)\n- debate (3, 4, or 5 speakers)\n\n#### Example Commands\n\nBasic usage:\n```bash\npython -m local_notebooklm.start --pdf documents/research_paper.pdf\n```\n\nCustomized podcast:\n```bash\npython -m local_notebooklm.start --pdf documents/research_paper.pdf --format podcast --length long --style casual\n```\n\nWith custom preferences:\n```bash\npython -m local_notebooklm.start --pdf documents/research_paper.pdf --preference \"Focus on practical applications and real-world examples\"\n```\n\nUsing custom config:\n```bash\npython -m local_notebooklm.start --pdf documents/research_paper.pdf --config custom_config.json --output-dir ./my_podcast --language german\n```\n\n### Programmatic API\n\nYou can also use Local-NotebookLM programmatically in your Python code:\n\n```python\nfrom local_notebooklm.processor import podcast_processor\n\nsuccess, result = podcast_processor(\n    pdf_path=\"documents/research_paper.pdf\",\n    config_path=\"config.json\",\n    format_type=\"interview\",\n    length=\"long\",\n    style=\"professional\",\n    preference=\"Focus on the key technical aspects\",\n    output_dir=\"./test_output\",\n    language=\"english\"\n)\n\nif success:\n    print(f\"Successfully generated podcast: {result}\")\nelse:\n    print(f\"Failed to generate podcast: {result}\")\n```\n\n### Gradio Web UI\n\nLocal-NotebookLM now includes a user-friendly Gradio web interface that makes it easy to use the tool without command line knowledge:\n\n```bash\npython -m local_notebooklm.web_ui\n```\n\nBy default, the web UI runs locally on http://localhost:7860. You can access it from your browser.\n\n#### Web UI Screenshots\n\n![Web UI Main Screen](examples/Gradio-WebUI.png)\n*The main interface of the Local-NotebookLM web UI*\n\n#### Web UI Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--share` | Make the UI accessible over the network | False |\n| `--port` | Specify a custom port | 7860 |\n\n#### Example Commands\n\nBasic local usage:\n```bash\npython -m local_notebooklm.web_ui\n```\n\nShare with others on your network:\n```bash\npython -m local_notebooklm.web_ui --share\n```\n\nUse a custom port:\n```bash\npython -m local_notebooklm.web_ui --port 8080\n```\n\nThe web interface provides all the same options as the command line tool in an intuitive UI, making it easier for non-technical users to generate audio content from PDFs.\n\n### FastAPI Server\n\nStart the FastAPI server to access the functionality via a web API:\n\n```bash\n python -m local_notebooklm.server\n```\n\nBy default, the server runs on http://localhost:8000. You can access the API documentation at http://localhost:8000/docs.\n\n## Pipeline Steps\n\n### 1. PDF Processing (Step1)\n- Extracts text from PDF documents\n- Cleans and formats the content\n- Removes irrelevant elements like page numbers and headers\n- Handles LaTeX math expressions and special characters\n- Splits content into manageable chunks for processing\n\n### 2. Transcript Generation (Step2)\n- Generates an initial podcast script based on the extracted content\n- Applies the specified style (casual, formal, technical, academic)\n- Formats content according to the desired length (short, medium, long, very-long)\n- Structures content for a conversational format\n- Incorporates user-specified format type (summary, podcast, article, interview)\n\n### 3. TTS Optimization (Step3)\n- Rewrites content specifically for better text-to-speech performance\n- Creates a two-speaker conversation format\n- Adds speech markers and natural conversation elements\n- Optimizes for natural flow and engagement\n- Incorporates user preferences for content focus\n- Formats output as a list of speaker-text tuples\n\n### 4. Audio Generation (Step4)\n- Converts the optimized text to speech using the specified TTS model\n- Applies different voices for each speaker\n- Generates individual audio segments for each dialogue part\n- Concatenates segments into a final audio file\n- Maintains consistent audio quality and sample rate\n\n### Here is a detaled diagram to visualize the architecture of my project.\n\n```mermaid\nflowchart TD\n    subgraph \"Main Controller\"\n        processor[\"podcast_processor()\"]\n    end\n\n    subgraph \"AI Services\"\n        smallAI[\"Small Text Model Client\"]\n        bigAI[\"Big Text Model Client\"]\n        ttsAI[\"Text-to-Speech Model Client\"]\n    end\n    \n    subgraph \"Step 1: PDF Processing\"\n        s1[\"step1()\"]\n        validate[\"validate_pdf()\"]\n        extract[\"extract_text_from_pdf()\"]\n        chunk1[\"create_word_bounded_chunks()\"]\n        process[\"process_chunk()\"]\n    end\n    \n    subgraph \"Step 2: Transcript Generation\"\n        s2[\"step2()\"]\n        read2[\"read_input_file()\"]\n        gen2[\"generate_transcript()\"]\n        chunk2[\"Chunking with Overlap\"]\n    end\n    \n    subgraph \"Step 3: TTS Optimization\"\n        s3[\"step3()\"]\n        read3[\"read_pickle_file()\"]\n        gen3[\"generate_rewritten_transcript()\"]\n        genOverlap[\"generate_rewritten_transcript_with_overlap()\"]\n        validate3[\"validate_transcript_format()\"]\n    end\n    \n    subgraph \"Step 4: Audio Generation\"\n        s4[\"step4()\"]\n        load4[\"load_podcast_data()\"]\n        genAudio[\"generate_speaker_audio()\"]\n        concat[\"concatenate_audio_files()\"]\n    end\n\n    %% Flow connections\n    processor --> s1\n    processor --> s2\n    processor --> s3\n    processor --> s4\n    \n    processor -.-> smallAI\n    processor -.-> bigAI\n    processor -.-> ttsAI\n    \n    %% Step 1 flow\n    s1 --> validate\n    validate --> extract\n    extract --> chunk1\n    chunk1 --> process\n    process -.-> smallAI\n    \n    %% Step 2 flow\n    s2 --> read2\n    read2 --> gen2\n    gen2 --> chunk2\n    gen2 -.-> bigAI\n    \n    %% Step 3 flow\n    s3 --> read3\n    read3 --> gen3\n    read3 --> genOverlap\n    gen3 --> validate3\n    genOverlap --> validate3\n    gen3 -.-> bigAI\n    genOverlap -.-> bigAI\n    \n    %% Step 4 flow\n    s4 --> load4\n    load4 --> genAudio\n    genAudio --> concat\n    genAudio -.-> ttsAI\n    \n    %% Data flow\n    pdf[(\"PDF File\")] --> s1\n    s1 --> |\"cleaned_text.txt\"| file1[(\"Cleaned Text\")]\n    file1 --> s2\n    s2 --> |\"data.pkl\"| file2[(\"Transcript\")]\n    file2 --> s3\n    s3 --> |\"podcast_ready_data.pkl\"| file3[(\"Optimized Transcript\")]\n    file3 --> s4\n    s4 --> |\"podcast.wav\"| fileAudio[(\"Final Audio\")]\n\n    %% Styling\n    classDef controller fill:#f9d5e5,stroke:#333,stroke-width:2px\n    classDef ai fill:#eeeeee,stroke:#333,stroke-width:1px\n    classDef step fill:#d0e8f2,stroke:#333,stroke-width:1px\n    classDef data fill:#fcf6bd,stroke:#333,stroke-width:1px,stroke-dasharray: 5 5\n    \n    class processor controller\n    class smallAI,bigAI,ttsAI ai\n    class s1,s2,s3,s4,validate,extract,chunk1,process,read2,gen2,chunk2,read3,gen3,genOverlap,validate3,load4,genAudio,concat step\n    class pdf,file1,file2,file3,fileAudio data\n```\n\n## Multiple Language Support\n\nLocal-NotebookLM now supports multiple languages. You can specify the language when using the programmatic API or through the command line.\n\n**Important Note:** When using a non-English language, ensure that both your selected LLM and TTS models support the desired language. Language support varies significantly between different models and providers. For optimal results, verify that your chosen models have strong capabilities in your target language before processing.\n\n\n## Output Files\n\nThe pipeline generates the following files:\n\n- `step1/extracted_text.txt`: Raw text extracted from the PDF\n- `step1/clean_extracted_text.txt`: Cleaned and processed text\n- `step2/data.pkl`: Initial transcript data\n- `step3/podcast_ready_data.pkl`: TTS-optimized conversation data\n- `step4/segments/podcast_segment_*.wav`: Individual audio segments\n- `step4/podcast.wav`: Final concatenated podcast audio file\n\n## Troubleshooting\n\n### Common Issues\n\n1. **PDF Extraction Fails**\n   - Try a different PDF file\n   - Check if the PDF is password-protected\n   - Ensure the PDF contains extractable text (not just images)\n\n2. **API Connection Errors**\n   - Verify your API keys are correct\n   - Check your internet connection\n   - Ensure the API endpoints are accessible\n\n3. **Out of Memory Errors**\n   - Reduce the chunk size in the configuration\n   - Use a smaller model\n   - Close other memory-intensive applications\n\n4. **Audio Quality Issues**\n   - Try different TTS voices\n   - Adjust the sample rate in the configuration\n   - Check if the TTS server is running correctly\n\n### Getting Help\n\nIf you encounter issues not covered here, please:\n1. Check the logs for detailed error messages\n2. Open an issue on the GitHub repository with details about your problem\n3. Include the error message and steps to reproduce the issue\n\n## Requirements\n\n- Python 3.12+\n- PyPDF2\n- tqdm\n- numpy\n- soundfile\n- requests\n- pathlib\n- fastapi\n- uvicorn\n\nFull requirements are listed in `requirements.txt`.\n\n## Acknowledgments\n\n- This project uses various open-source libraries and models\n- Special thanks to the developers of LLaMA, OpenAI, and other AI models that make this possible\n\n---\n\nFor more information, visit the [GitHub repository](https://github.com/Goekdeniz-Guelmez/Local-NotebookLM).\n\nBest\nGÃ¶kdeniz GÃ¼lmez\n\n---\n\n![Alt](https://repobeats.axiom.co/api/embed/28af9fd2bc35cdc4974f5766dd60c1fa9323a4a2.svg \"Repobeats analytics image\")\n\n---\n\n## Citing Local-NotebookLM\n\nThe Local-NotebookLM software suite was developed by GÃ¶kdeniz GÃ¼lmez. If you find Local-NotebookLM useful in your research and wish to cite it, please use the following\nBibTex entry:\n\n```text\n@software{\n  Local-NotebookLM,\n  author = {GÃ¶kdeniz GÃ¼lmez},\n  title = {{Local-NotebookLM}: A Local-NotebookLM to convert PDFs into Audio.},\n  url = {https://github.com/Goekdeniz-Guelmez/Local-NotebookLM},\n  version = {0.1.5},\n  year = {2025},\n}\n```\n"
  },
  {
    "name": "n8n-terry-guide",
    "path": "integrations/docs/n8n-terry-guide.md",
    "content": "# n8n AI Agent (Terry) - Complete Setup Guide\n\n> **Video**: n8n Now Runs My ENTIRE Homelab\n> **Part 2**: Building Terry - Your AI IT Employee\n\nThis guide contains all the commands, prompts, and configurations shown in the video for setting up Terry, an intelligent AI agent that can monitor, troubleshoot, and fix issues in your homelab with human approval.\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Prerequisites](#prerequisites)\n- [Initial Setup](#initial-setup)\n  - [Demo Website Container](#demo-website-container)\n  - [n8n Workflow Setup](#n8n-workflow-setup)\n- [Terry's Evolution](#terrys-evolution)\n  - [Stage 1: Basic Monitor](#stage-1-basic-monitor)\n  - [Stage 2: Smart Investigator](#stage-2-smart-investigator)\n  - [Stage 3: The Fixer](#stage-3-the-fixer)\n  - [Stage 4: Creative Problem Solver](#stage-4-creative-problem-solver)\n  - [Stage 5: Human-in-the-Loop](#stage-5-human-in-the-loop)\n- [Automation Setup](#automation-setup)\n  - [Schedule Trigger](#schedule-trigger)\n  - [Structured Output](#structured-output)\n  - [Telegram Notifications](#telegram-notifications)\n- [Service Integrations](#service-integrations)\n  - [UniFi Network](#unifi-network)\n  - [Proxmox](#proxmox)\n  - [NAS (ZimaCube)](#nas-zimacube)\n  - [Plex](#plex)\n- [Advanced Features](#advanced-features)\n  - [Human-in-the-Loop Approval](#human-in-the-loop-approval)\n  - [God-Mode Prompt](#god-mode-prompt)\n- [Troubleshooting](#troubleshooting)\n\n---\n\n## Overview\n\nTerry is an AI agent built with n8n that can:\n- **Monitor** your services 24/7\n- **Troubleshoot** issues by running diagnostic commands\n- **Fix** problems with your explicit approval\n- **Alert** you via Telegram/Slack when action is needed\n\n**Philosophy**: You're training an employee, not programming a bot.\n\n---\n\n## Prerequisites\n\n- n8n instance (cloud-hosted or self-hosted)\n- Docker installed on your server\n- OpenAI API key (or compatible LLM)\n- Telegram account (for notifications)\n- Optional: Twingate for secure remote access\n\n---\n\n## Initial Setup\n\n### Demo Website Container\n\nCreate a simple nginx website for testing Terry's monitoring capabilities:\n\n```bash\n# Create a simple website container for demonstrations\n# Using port 8090 to avoid conflicts with n8n/Traefik\ndocker run -d --name website -p 8090:80 nginx\ndocker exec website sh -c 'echo \"<h1>NetworkChuck Coffee</h1>\" > /usr/share/nginx/html/index.html'\n```\n\n**Test the website:**\n```bash\n# Replace with your server IP\ncurl http://YOUR_SERVER_IP:8090\n```\n\n### n8n Workflow Setup\n\n1. Create a new workflow in n8n\n2. Add a **Manual Trigger** node\n3. Add an **AI Agent** node\n4. Configure the AI Agent:\n   - Chat Model: OpenAI GPT-4o-mini (or your preferred model)\n   - Memory: Simple Memory (for conversation context)\n\n---\n\n## Terry's Evolution\n\n### Stage 1: Basic Monitor\n\n**Goal**: Terry checks if the website is up.\n\n**System Prompt:**\n```\nYou are Terry, an IT Administrator for NetworkChuck. As a new employee, your only responsibility is to ensure the website at http://YOUR_SERVER_IP:8090/ is operational. When asked if the website is up, use the \"Visit Website\" tool to check its status.\n\n1. Access the website using the provided HTTP tool.\n\n2. The website is considered up and operational only if the response contains the exact HTML content: <h1>NetworkChuck Coffee</h1>.\n\n3. Report the website's status as either \"up ðŸ˜ŽðŸ‘\" or \"down ðŸ˜žðŸ‘Ž\" based on the tool's response.\n```\n\n**Tools Required:**\n- HTTP Request node (named \"website tool\")\n\n---\n\n### Stage 2: Smart Investigator\n\n**Goal**: Terry investigates WHY the website is down.\n\n**System Prompt:**\n```\nYou are Terry, a new IT Administrator for NetworkChuck. Your sole responsibility is to ensure the website at http://YOUR_SERVER_IP:8090/ is operational. When asked if the website is up, follow these steps to check its status:\n\n1. Access the website using the provided HTTP tool.\n\n2. The website is considered up and operational only if the response contains the exact HTML content: <h1>NetworkChuck Coffee</h1>.\n\n3. If the website is not up via the HTTP tool, use the Docker tool to check if the \"website\" container is running by executing the command docker ps.\n\n4. If the container is not running, check the exit code using the command docker inspect website --format='{{.State.ExitCode}}'.\n\n5. Retrieve the recent logs using the command docker logs website --tail 10.\n\n6. Report the website's status as either \"up ðŸ˜ŽðŸ‘\" or \"down ðŸ˜žðŸ‘Ž\". If the website is down, include an explanation based on the HTTP tool response (e.g., connection error, timeout, or incorrect content) and the Docker tool's findings (e.g., container not running, exit code, and relevant log details indicating why the container failed).\n```\n\n**SSH Subworkflow Setup:**\n\n1. Add SSH node â†’ Execute a command\n2. Convert to subworkflow: Hover over node â†’ Three dots â†’ \"Convert node to subworkflow\"\n3. Edit the subworkflow:\n   - Connect Start node to SSH node\n   - Edit Start node: Change \"Input data mode\" to \"Define using fields below\"\n   - Add field: `command` (type: string)\n   - In SSH node: Map the command field from Start node\n\n**Tools Required:**\n- HTTP Request node\n- Call n8n Workflow tool (pointing to SSH subworkflow)\n\n---\n\n### Stage 3: The Fixer\n\n**Goal**: Terry automatically fixes simple issues.\n\n**System Prompt:**\n```\nYou are Terry, a new IT Administrator for NetworkChuck. Your sole responsibility is to ensure the website at http://YOUR_SERVER_IP:8090/ is operational. When asked if the website is up, follow these steps to check and restore its status:\n\n1. Access the website using the provided HTTP tool.\n\n2. The website is considered up and operational only if the response contains the exact HTML content: <h1>NetworkChuck Coffee</h1>.\n\n3. If the website is not up via the HTTP tool, use the Docker tool to check if the \"website\" container is running by executing the command docker ps.\n\n4. If the container is not running, check the exit code using the command docker inspect website --format='{{.State.ExitCode}}'.\n\n5. Retrieve the recent logs using the command docker logs website --tail 10.\n\n6. If the container is not running, attempt to restart it using the command docker container start website.\n\n7. After attempting to restart, use the HTTP tool again to verify if the website is now up and returns <h1>NetworkChuck Coffee</h1>.\n\n8. Report the website's status as either \"up ðŸ˜ŽðŸ‘\" or \"down ðŸ˜žðŸ‘Ž\". If the website is down, include an explanation based on the HTTP tool response (e.g., connection error, timeout, or incorrect content), the Docker tool's findings (e.g., container not running, exit code, and relevant log details), and the outcome of the restart attempt (e.g., successful or failed, with any errors encountered).\n```\n\n---\n\n### Stage 4: Creative Problem Solver\n\n**Goal**: Terry solves unexpected issues (like port conflicts).\n\n**Create a port conflict for testing:**\n```bash\n# Stop the website container\ndocker stop website\n\n# Start a Python web server on the same port\npython3 -m http.server 8090\n```\n\n**System Prompt:**\n```\nYou are Terry, a new IT Administrator for NetworkChuck. Your sole responsibility is to ensure the website at http://YOUR_SERVER_IP:8090/ is operational.\n\nYou are a Docker expert. You know everything about Docker and all the common things that keep a Docker container from running. In our particular environment, we have a web server running inside a Docker container that's called \"website\". It's running on port 8090.\n\nWhen asked if the website is up, follow these steps to check and restore its status:\n\n1. Access the website using the provided HTTP tool.\n\n2. The website is considered up and operational only if the response contains the exact HTML content: <h1>NetworkChuck Coffee</h1>.\n\n3. If the website is not up, use the CLI tool to troubleshoot the issue.\n\n4. Once you figure out the issue, you can apply whatever fix you need to bring that container up and fix the website.\n```\n\n**Note**: Change the SSH subworkflow tool to use \"Let the agent decide\" instead of a fixed command.\n\n---\n\n### Stage 5: Human-in-the-Loop\n\n**Goal**: Terry asks for approval before making changes.\n\n**System Prompt:**\n```\nYou are Terry, a new IT Administrator for NetworkChuck. Your sole responsibility is to ensure the website at http://YOUR_SERVER_IP:8090/ is operational.\n\nCRITICAL PERMISSION RULES:\n- You MUST request EXPLICIT APPROVAL before running ANY command that could modify the system\n- This includes but is not limited to: docker start, docker stop, docker run, docker rm, kill, pkill, systemctl, or any command that creates, modifies, or deletes files\n- Even diagnostic commands like docker ps, netstat, or ps are fine without permission\n- When in doubt, ASK FIRST\n\nYou are a Docker expert. You know everything about Docker and common issues that prevent containers from running properly. In our environment, we have a web server that should be running inside a Docker container called \"website\" on port 8090.\n\nWhen asked if the website is up, follow these systematic troubleshooting steps:\n\n1. First, check if the website is accessible using the HTTP tool\n\n2. The website is operational ONLY if it returns HTML containing: <h1>NetworkChuck Coffee</h1>\n\n3. If the website is down, investigate systematically:\n   - Check if the container exists and its current state\n   - Check what's currently using port 8090 (could be another process)\n   - Look for any error messages or logs\n   - Identify the root cause before attempting any fix\n\n4. Once you've identified the issue, explain what you found and request permission for the specific fix needed\n\n5. Only after receiving explicit approval, apply the necessary fix to restore the website\n\nRemember: Always investigate thoroughly before proposing solutions. Something else might be using the port.\n\nREQUIRED OUTPUT FORMAT:\nYou MUST always respond with a JSON object in this exact format:\n{\n    \"website_up\": true/false,\n    \"message\": \"Detailed explanation of status and any actions taken\",\n    \"applied_fix\": true/false,\n    \"needs_approval\": true/false,\n    \"commands_requested\": \"Specific commands needing approval (null if none)\"\n}\n```\n\n---\n\n## Automation Setup\n\n### Schedule Trigger\n\nInstead of manually asking Terry, set up automatic monitoring:\n\n1. Add a **Schedule Trigger** node\n2. Set interval: Every 5 minutes (or your preference)\n3. Add an **Edit Fields** node between Schedule and AI Agent\n4. Set two fields:\n   - **prompt**: `Check if the website is up`\n   - **chatId**: `Terry12345` (any unique identifier)\n\n**Update AI Agent:**\n- Change prompt source from \"Connected chat trigger node\" to \"Define below\"\n- Map to the `prompt` field from Edit Fields node\n\n**Update Simple Memory:**\n- Change Session ID source to \"Define below\"\n- Set to: `{{ $json.chatId }}`\n\n---\n\n### Structured Output\n\nForce Terry to respond in a consistent JSON format for decision-making.\n\n**Configure AI Agent:**\n1. Click \"Add option\" â†’ \"Require specific output format\"\n2. Add **Structured Output Parser** node\n3. Configure the JSON schema:\n\n```json\n{\n  \"website_up\": true,\n  \"message\": \"message\"\n}\n```\n\n**With Human-in-the-Loop (expanded):**\n```json\n{\n  \"website_up\": true,\n  \"message\": \"Detailed explanation of status and any actions taken\",\n  \"applied_fix\": true,\n  \"needs_approval\": true,\n  \"commands_requested\": \"Specific commands needing approval (null if none)\"\n}\n```\n\n---\n\n### Telegram Notifications\n\n**Setup Telegram Bot:**\n1. Open Telegram, search for \"BotFather\"\n2. Send `/newbot` and follow instructions\n3. Copy the API token\n4. Start a chat with your bot\n5. Get your Chat ID (use [@userinfobot](https://t.me/userinfobot))\n\n**n8n Configuration:**\n1. Add **Telegram** node â†’ Send a text message\n2. Create credential with your bot token\n3. Set Chat ID\n4. Map the message text from Terry's output\n\n**Add decision logic with IF node:**\n- If `website_up` is **false** â†’ Send message\n- If `website_up` is **true** â†’ Don't notify (no action needed)\n\n**Better approach - use SWITCH node:**\n- Route 0: If `applied_fix` is true â†’ Send notification (Terry fixed something)\n- Route 1: If `website_up` is false â†’ Send notification (Still broken)\n- Default: No notification (Everything is fine)\n\n---\n\n## Service Integrations\n\n### UniFi Network\n\n**System Prompt:**\n```\nYou are an AI agent specialized in monitoring and managing UniFi networks using the UniFi Network API (version 9.3.45 or compatible) hosted at https://YOUR_UNIFI_IP/proxy/network/integration/v1. Your primary goal is to provide high-level insights into network performance, particularly focusing on wireless network health, client counts, and bandwidth usage. Always use the base URL https://YOUR_UNIFI_IP/proxy/network/integration/v1 for all API requests. Authenticate all requests with the header 'X-API-KEY: YOUR_API_KEY' and 'Accept: application/json'. Ignore SSL verification if needed (e.g., use -k in curl or equivalent in code). The default site ID is \"YOUR_SITE_ID\" (named \"YOUR_SITE_NAME\", internalReference: \"default\"). Use this site ID for site-specific endpoints unless specified otherwise. If a feature or detailed data (e.g., per-port status, advanced client stats) is not available or incomplete in the v1 integration API endpoints, default back to the legacy UniFi Controller API endpoints by adjusting the path to https://YOUR_UNIFI_IP/proxy/network/api/s/default/ (replacing \"default\" with the site's internalReference if needed). Test the v1 endpoint first, and fall back only if it lacks the required information.\n\n## Key Capabilities\n- **Wireless Network Health**: Assess the status of wireless devices (e.g., access points) by checking online status, uptime, CPU/memory utilization, and data transmission rates.\n- **Client Count**: Retrieve and count the number of currently connected clients (wired, wireless, VPN).\n- **Bandwidth Usage**: Identify clients or devices consuming the most bandwidth by analyzing data transmission statistics.\n\n## Example Tasks\n- \"How is the wireless network doing?\"\n- \"How many clients are on the network?\"\n- \"Who is using the most bandwidth?\"\n- \"How many ports are active right now?\"\n```\n\n**Tool Setup:**\n- Add HTTP Request node\n- Configure UniFi API credentials\n- Name it appropriately (e.g., \"UniFi Tool\")\n\n---\n\n### Proxmox\n\n**System Prompt:**\n```\nYou are a Proxmox expert. You know everything about Proxmox. When asked about a certain Proxmox server, you can answer questions by engaging with the Proxmox tool and running CLI commands, but you can make no changes.\n```\n\n**Tool Setup:**\n- Create SSH subworkflow for Proxmox server\n- Add as tool to Terry\n- Configure with Proxmox server credentials\n\n**Example Commands Terry Can Run:**\n- `pvesh get /nodes` - List all nodes\n- `pvesh get /nodes/NODENAME/qemu` - List VMs\n- `pvesh get /nodes/NODENAME/lxc` - List containers\n- `pvesh get /nodes/NODENAME/status` - Node status\n\n---\n\n### NAS (ZimaCube)\n\n**Tool Description:**\n```\nUse this tool to perform read-only health checks on `YOUR_NAS_NAME`, a ZimaCube Pro NAS running ZimaOS, a Debian-based Linux system with EXT4 filesystems. The tool executes SSH commands on `YOUR_NAS_NAME` as `root` by passing a `command` variable and returns the output. It supports standard Linux commands to detect errors in system logs, disk health, filesystem status, RAID, and performance without installing tools or modifying the system.\n\n**Examples of Commands**:\n- **Check system logs**: `journalctl -p 3 -xb` (shows critical errors from the current boot).\n- **List drives**: `lsblk -f` (displays block devices, EXT4 filesystems, and mount points like `/DATA`).\n- **Check disk usage**: `df -hT` (shows EXT4 partition usage).\n- **Check disk health**: `smartctl -a /dev/<device>` (e.g., `smartctl -a /dev/sda` for SMART data).\n- **Check RAID status**: `cat /proc/mdstat` (shows RAID array status, if configured).\n```\n\n**Agent System Prompt:**\n```\nYou are a Linux system administrator tasked with performing a comprehensive, read-only health check on `YOUR_NAS_NAME`, a ZimaCube Pro NAS running ZimaOS (Debian-based, using EXT4 filesystems). Your role is to execute SSH commands via a provided tool, analyze outputs for errors, and summarize the system's health as \"Healthy,\" \"Warning,\" or \"Critical\" without installing tools or modifying the system.\n\n**Instructions**:\n1. **Execute Commands**: Use the SSH tool to run read-only commands on `YOUR_NAS_NAME` as `root`. Do not install packages or make system changes.\n2. **Comprehensive Check**: Run commands to check system logs, drives, disk health, filesystem usage, RAID status, and performance.\n3. **Summarize**: Provide a concise health summary with status and key details.\n\n**Safety**: Use only read-only commands. Do not attempt to unmount filesystems or run destructive commands.\n```\n\n---\n\n### Plex\n\n**Simple Approach:**\n- Monitor Plex web interface with HTTP Request tool\n- Check for expected response\n- Restart Plex service/container if needed\n\n**See the Plex monitoring implementation guides in the project folder for the advanced API-based approach.**\n\n---\n\n## Advanced Features\n\n### Human-in-the-Loop Approval\n\n**Workflow Setup:**\n\n1. Add **IF** node after AI Agent\n   - Condition: `needs_approval` equals `true` (Boolean)\n   - True path â†’ Request approval\n   - False path â†’ Continue to notification logic\n\n2. Add **Telegram** node (Send and wait for response)\n   - Operation: \"Send and wait for response\"\n   - Response Type: \"Approval\"\n   - Message: Include `website_up`, `message`, and `commands_requested` fields\n\n3. Add **Edit Fields** node after approval\n   - Rename `approved` field to `prompt`\n   - Change type from Boolean to String\n   - Add `chatId` field from original Edit Fields node\n\n4. Connect Edit Fields back to AI Agent (creates the loop)\n\n5. Update notification logic with **SWITCH** node to handle different scenarios\n\n---\n\n### God-Mode Prompt\n\n**âš ï¸ WARNING**: This prompt gives Terry significant autonomy. Use with caution!\n\n```\nYou are Terry, trusted IT administrator. You have permission to:\n\nINVESTIGATE AND FIX PROCEDURE:\n1. Check container: docker ps -a --filter \"name=mysite\"\n2. If Exited, check logs: docker logs mysite --tail 5\n3. Check system resources: df -h and free -h\n4. FIX IT:\n   - If stopped: docker start mysite\n   - If disk full: docker system prune -f, then docker start mysite\n   - If port conflict: docker start mysite || docker run -d --name mysite2 -p 8081:80 nginx\n5. Verify fix: curl -s -o /dev/null -w \"%{http_code}\" http://localhost:8080\n6. Report what you did\n\nYou must explain each action before taking it.\n\nYou do not need approval for running commands to check and troubleshoot. HOWEVER, you DO NEED approval to fix anything, apply any fixes or make any changes.\n\nIf it's an approval, say true. If not, say false.\n\nIf you need feedback, help troubleshooting or something, set feedback to true\n```\n\n---\n\n## Troubleshooting\n\n### Terry Stopped Working After Adding Schedule\n\n**Issue**: Chat ID and prompt variables not set correctly.\n\n**Solution**:\n1. Add Edit Fields node between Schedule and AI Agent\n2. Set `prompt` field with your monitoring instruction\n3. Set `chatId` field with unique identifier\n4. Update AI Agent to use these fields instead of Chat Trigger\n5. Update Simple Memory to use `{{ $json.chatId }}`\n\n### Too Many Iterations Error\n\n**Issue**: Terry is running too many tool calls.\n\n**Solution**:\n1. Increase max iterations in AI Agent settings (default is 10)\n2. Make prompts more specific to reduce trial-and-error\n3. Upgrade to smarter model (GPT-4 instead of GPT-4-mini)\n\n### Terry Isn't Asking for Approval\n\n**Issue**: Permission rules not clear in prompt.\n\n**Solution**:\n1. Add explicit \"CRITICAL PERMISSION RULES\" section\n2. Provide clear examples of commands requiring approval\n3. Add structured output fields: `needs_approval`, `commands_requested`\n4. Test with known scenarios\n\n### Telegram Not Receiving Messages\n\n**Issue**: Incorrect Chat ID or bot configuration.\n\n**Solution**:\n1. Verify bot token is correct\n2. Ensure you've started a chat with your bot\n3. Get Chat ID from @userinfobot\n4. Test with \"Execute node\" to verify connection\n\n### SSH Commands Failing\n\n**Issue**: Subworkflow not configured correctly.\n\n**Solution**:\n1. Verify SSH credentials in subworkflow\n2. Ensure Start node has `command` field defined\n3. Check SSH node is mapping the command variable\n4. Test with simple command like `hostname`\n\n---\n\n## Best Practices\n\n1. **Start Simple**: Begin with basic monitoring, then add complexity\n2. **Document Everything**: Keep notes of what Terry can access\n3. **Test in Sandbox**: Use the demo website setup before connecting to production\n4. **Use Human-in-the-Loop**: Always require approval for system-modifying commands\n5. **Monitor Terry**: Set up alerts for when Terry takes actions\n6. **Iterate Prompts**: Refine system prompts based on Terry's behavior\n7. **Version Control**: Save different versions of your workflows\n8. **Security First**: Use Twingate or VPN for remote access, never expose directly\n\n---\n\n## What's Next?\n\nIn Part 3, we'll build:\n- **Multiple specialized agents** (Network Admin, Storage Expert, Linux Engineer)\n- **Shared knowledge base** for documentation\n- **Helpdesk system** for user-submitted tickets\n- **Agent collaboration** for complex problems\n\n---\n\n## Resources\n\n- [n8n Documentation](https://docs.n8n.io/)\n- [OpenAI API Documentation](https://platform.openai.com/docs/)\n- [Telegram Bot API](https://core.telegram.org/bots/api)\n- [Twingate Setup Guide](https://www.twingate.com/docs/)\n\n---\n\n## Support\n\nQuestions? Issues? Join the discussion:\n- [NetworkChuck Discord](#)\n- [GitHub Issues](#)\n\n---\n\n**Remember**: You're training an employee, not programming a bot. Give Terry context, teach him your processes, and build trust progressively.\n\nHappy automating! â˜•ï¸\n"
  },
  {
    "name": "notebooklm-mcp",
    "path": "integrations/docs/notebooklm-mcp.md",
    "content": "<div align=\"center\">\n\n# NotebookLM MCP Server\n\n**Let your CLI agents (Claude, Cursor, Codex...) chat directly with NotebookLM for zero-hallucination answers based on your own notebooks**\n\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue.svg)](https://www.typescriptlang.org/)\n[![MCP](https://img.shields.io/badge/MCP-2025-green.svg)](https://modelcontextprotocol.io/)\n[![npm](https://img.shields.io/npm/v/notebooklm-mcp.svg)](https://www.npmjs.com/package/notebooklm-mcp)\n[![Claude Code Skill](https://img.shields.io/badge/Claude%20Code-Skill-purple.svg)](https://github.com/PleasePrompto/notebooklm-skill)\n[![GitHub](https://img.shields.io/github/stars/PleasePrompto/notebooklm-mcp?style=social)](https://github.com/PleasePrompto/notebooklm-mcp)\n\n[Installation](#installation) â€¢ [Quick Start](#quick-start) â€¢ [Why NotebookLM](#why-notebooklm-not-local-rag) â€¢ [Examples](#real-world-example) â€¢ [Claude Code Skill](https://github.com/PleasePrompto/notebooklm-skill) â€¢ [Documentation](./docs/)\n\n</div>\n\n---\n\n## The Problem\n\nWhen you tell Claude Code or Cursor to \"search through my local documentation\", here's what happens:\n- **Massive token consumption**: Searching through documentation means reading multiple files repeatedly\n- **Inaccurate retrieval**: Searches for keywords, misses context and connections between docs\n- **Hallucinations**: When it can't find something, it invents plausible-sounding APIs\n- **Expensive & slow**: Each question requires re-reading multiple files\n\n## The Solution\n\nLet your local agents chat directly with [**NotebookLM**](https://notebooklm.google/) â€” Google's **zero-hallucination knowledge base** powered by Gemini 2.5 that provides intelligent, synthesized answers from your docs.\n\n```\nYour Task â†’ Local Agent asks NotebookLM â†’ Gemini synthesizes answer â†’ Agent writes correct code\n```\n\n**The real advantage**: No more manual copy-paste between NotebookLM and your editor. Your agent asks NotebookLM directly and gets answers straight back in the CLI. It builds deep understanding through automatic follow-ups â€” Claude asks multiple questions in sequence, each building on the last, getting specific implementation details, edge cases, and best practices. You can save NotebookLM links to your local library with tags and descriptions, and Claude automatically selects the relevant notebook based on your current task.\n\n---\n\n## Why NotebookLM, Not Local RAG?\n\n| Approach | Token Cost | Setup Time | Hallucinations | Answer Quality |\n|----------|------------|------------|----------------|----------------|\n| **Feed docs to Claude** | ðŸ”´ Very high (multiple file reads) | Instant | Yes - fills gaps | Variable retrieval |\n| **Web search** | ðŸŸ¡ Medium | Instant | High - unreliable sources | Hit or miss |\n| **Local RAG** | ðŸŸ¡ Medium-High | Hours (embeddings, chunking) | Medium - retrieval gaps | Depends on setup |\n| **NotebookLM MCP** | ðŸŸ¢ Minimal | 5 minutes | **Zero** - refuses if unknown | Expert synthesis |\n\n### What Makes NotebookLM Superior?\n\n1. **Pre-processed by Gemini**: Upload docs once, get instant expert knowledge\n2. **Natural language Q&A**: Not just retrieval â€” actual understanding and synthesis\n3. **Multi-source correlation**: Connects information across 50+ documents\n4. **Citation-backed**: Every answer includes source references\n5. **No infrastructure**: No vector DBs, embeddings, or chunking strategies needed\n\n---\n\n## Installation\n\n### Claude Code\n```bash\nclaude mcp add notebooklm npx notebooklm-mcp@latest\n```\n\n### Codex\n```bash\ncodex mcp add notebooklm -- npx notebooklm-mcp@latest\n```\n\n<details>\n<summary>Gemini</summary>\n\n```bash\ngemini mcp add notebooklm npx notebooklm-mcp@latest\n```\n</details>\n\n<details>\n<summary>Cursor</summary>\n\nAdd to `~/.cursor/mcp.json`:\n```json\n{\n  \"mcpServers\": {\n    \"notebooklm\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"notebooklm-mcp@latest\"]\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary>amp</summary>\n\n```bash\namp mcp add notebooklm -- npx notebooklm-mcp@latest\n```\n</details>\n\n<details>\n<summary>VS Code</summary>\n\n```bash\ncode --add-mcp '{\"name\":\"notebooklm\",\"command\":\"npx\",\"args\":[\"notebooklm-mcp@latest\"]}'\n```\n</details>\n\n<details>\n<summary>Other MCP clients</summary>\n\n**Generic MCP config:**\n```json\n{\n  \"mcpServers\": {\n    \"notebooklm\": {\n      \"command\": \"npx\",\n      \"args\": [\"notebooklm-mcp@latest\"]\n    }\n  }\n}\n```\n</details>\n\n---\n\n## Alternative: Claude Code Skill\n\n**Prefer Claude Code Skills over MCP?** This server is now also available as a native Claude Code Skill with a simpler setup:\n\n**[NotebookLM Claude Code Skill](https://github.com/PleasePrompto/notebooklm-skill)** - Clone to `~/.claude/skills` and start using immediately\n\n**Key differences:**\n- **MCP Server** (this repo): Persistent sessions, works with Claude Code, Codex, Cursor, and other MCP clients\n- **Claude Code Skill**: Simpler setup, Python-based, stateless queries, works only with local Claude Code\n\nBoth use the same browser automation technology and provide zero-hallucination answers from your NotebookLM notebooks.\n\n---\n\n## Quick Start\n\n### 1. Install the MCP server (see [Installation](#installation) above)\n\n### 2. Authenticate (one-time)\n\nSay in your chat (Claude/Codex):\n```\n\"Log me in to NotebookLM\"\n```\n*A Chrome window opens â†’ log in with Google*\n\n### 3. Create your knowledge base\nGo to [notebooklm.google.com](https://notebooklm.google.com) â†’ Create notebook â†’ Upload your docs:\n- ðŸ“„ PDFs, Google Docs, markdown files\n- ðŸ”— Websites, GitHub repos\n- ðŸŽ¥ YouTube videos\n- ðŸ“š Multiple sources per notebook\n\nShare: **âš™ï¸ Share â†’ Anyone with link â†’ Copy**\n\n### 4. Let Claude use it\n```\n\"I'm building with [library]. Here's my NotebookLM: [link]\"\n```\n\n**That's it.** Claude now asks NotebookLM whatever it needs, building expertise before writing code.\n\n---\n\n## Real-World Example\n\n### Building an n8n Workflow Without Hallucinations\n\n**Challenge**: n8n's API is new â€” Claude hallucinates node names and functions.\n\n**Solution**:\n1. Downloaded complete n8n documentation â†’ merged into manageable chunks\n2. Uploaded to NotebookLM\n3. Told Claude: *\"Build me a Gmail spam filter workflow. Use this NotebookLM: [link]\"*\n\n**Watch the AI-to-AI conversation:**\n\n```\nClaude â†’ \"How does Gmail integration work in n8n?\"\nNotebookLM â†’ \"Use Gmail Trigger with polling, or Gmail node with Get Many...\"\n\nClaude â†’ \"How to decode base64 email body?\"\nNotebookLM â†’ \"Body is base64url encoded in payload.parts, use Function node...\"\n\nClaude â†’ \"How to parse OpenAI response as JSON?\"\nNotebookLM â†’ \"Set responseFormat to json, use {{ $json.spam }} in IF node...\"\n\nClaude â†’ \"What about error handling if the API fails?\"\nNotebookLM â†’ \"Use Error Trigger node with Continue On Fail enabled...\"\n\nClaude â†’ âœ… \"Here's your complete workflow JSON...\"\n```\n\n**Result**: Perfect workflow on first try. No debugging hallucinated APIs.\n\n---\n\n## Core Features\n\n### **Zero Hallucinations**\nNotebookLM refuses to answer if information isn't in your docs. No invented APIs.\n\n### **Autonomous Research**\nClaude asks follow-up questions automatically, building complete understanding before coding.\n\n### **Smart Library Management**\nSave NotebookLM links with tags and descriptions. Claude auto-selects the right notebook for your task.\n```\n\"Add [link] to library tagged 'frontend, react, components'\"\n```\n\n### **Deep, Iterative Research**\n- Claude automatically asks follow-up questions to build complete understanding\n- Each answer triggers deeper questions until Claude has all the details\n- Example: For n8n workflow, Claude asked multiple sequential questions about Gmail integration, error handling, and data transformation\n\n### **Cross-Tool Sharing**\nSet up once, use everywhere. Claude Code, Codex, Cursor â€” all share the same library.\n\n### **Deep Cleanup Tool**\nFresh start anytime. Scans entire system for NotebookLM data with categorized preview.\n\n---\n\n## Tool Profiles\n\nReduce token usage by loading only the tools you need. Each tool consumes context tokens â€” fewer tools = faster responses and lower costs.\n\n### Available Profiles\n\n| Profile | Tools | Use Case |\n|---------|-------|----------|\n| **minimal** | 5 | Query-only: `ask_question`, `get_health`, `list_notebooks`, `select_notebook`, `get_notebook` |\n| **standard** | 10 | + Library management: `setup_auth`, `list_sessions`, `add_notebook`, `update_notebook`, `search_notebooks` |\n| **full** | 16 | All tools including `cleanup_data`, `re_auth`, `remove_notebook`, `reset_session`, `close_session`, `get_library_stats` |\n\n### Configure via CLI\n\n```bash\n# Check current settings\nnpx notebooklm-mcp config get\n\n# Set a profile\nnpx notebooklm-mcp config set profile minimal\nnpx notebooklm-mcp config set profile standard\nnpx notebooklm-mcp config set profile full\n\n# Disable specific tools (comma-separated)\nnpx notebooklm-mcp config set disabled-tools \"cleanup_data,re_auth\"\n\n# Reset to defaults\nnpx notebooklm-mcp config reset\n```\n\n### Configure via Environment Variables\n\n```bash\n# Set profile\nexport NOTEBOOKLM_PROFILE=minimal\n\n# Disable specific tools\nexport NOTEBOOKLM_DISABLED_TOOLS=\"cleanup_data,re_auth,remove_notebook\"\n```\n\nSettings are saved to `~/.config/notebooklm-mcp/settings.json` and persist across sessions. Environment variables override file settings.\n\n---\n\n## Architecture\n\n```mermaid\ngraph LR\n    A[Your Task] --> B[Claude/Codex]\n    B --> C[MCP Server]\n    C --> D[Chrome Automation]\n    D --> E[NotebookLM]\n    E --> F[Gemini 2.5]\n    F --> G[Your Docs]\n    G --> F\n    F --> E\n    E --> D\n    D --> C\n    C --> B\n    B --> H[Accurate Code]\n```\n\n---\n\n## Common Commands\n\n| Intent | Say | Result |\n|--------|-----|--------|\n| Authenticate | *\"Open NotebookLM auth setup\"* or *\"Log me in to NotebookLM\"* | Chrome opens for login |\n| Add notebook | *\"Add [link] to library\"* | Saves notebook with metadata |\n| List notebooks | *\"Show our notebooks\"* | Lists all saved notebooks |\n| Research first | *\"Research this in NotebookLM before coding\"* | Multi-question session |\n| Select notebook | *\"Use the React notebook\"* | Sets active notebook |\n| Update notebook | *\"Update notebook tags\"* | Modify metadata |\n| Remove notebook | *\"Remove [notebook] from library\"* | Deletes from library |\n| View browser | *\"Show me the browser\"* | Watch live NotebookLM chat |\n| Fix auth | *\"Repair NotebookLM authentication\"* | Clears and re-authenticates |\n| Switch account | *\"Re-authenticate with different Google account\"* | Changes account |\n| Clean restart | *\"Run NotebookLM cleanup\"* | Removes all data for fresh start |\n| Keep library | *\"Cleanup but keep my library\"* | Preserves notebooks |\n| Delete all data | *\"Delete all NotebookLM data\"* | Complete removal |\n\n---\n\n## Comparison to Alternatives\n\n### vs. Downloading docs locally\n- **You**: Download docs â†’ Claude: \"search through these files\"\n- **Problem**: Claude reads thousands of files â†’ massive token usage, often misses connections\n- **NotebookLM**: Pre-indexed by Gemini, semantic understanding across all docs\n\n### vs. Web search\n- **You**: \"Research X online\"\n- **Problem**: Outdated info, hallucinated examples, unreliable sources\n- **NotebookLM**: Only your trusted docs, always current, with citations\n\n### vs. Local RAG setup\n- **You**: Set up embeddings, vector DB, chunking strategy, retrieval pipeline\n- **Problem**: Hours of setup, tuning retrieval, still gets \"creative\" with gaps\n- **NotebookLM**: Upload docs â†’ done. Google handles everything.\n\n---\n\n## FAQ\n\n**Is it really zero hallucinations?**\nYes. NotebookLM is specifically designed to only answer from uploaded sources. If it doesn't know, it says so.\n\n**What about rate limits?**\nFree tier has daily query limits per Google account. Quick account switching supported for continued research.\n\n**How secure is this?**\nChrome runs locally. Your credentials never leave your machine. Use a dedicated Google account if concerned.\n\n**Can I see what's happening?**\nYes! Say *\"Show me the browser\"* to watch the live NotebookLM conversation.\n\n**What makes this better than Claude's built-in knowledge?**\nYour docs are always current. No training cutoff. No hallucinations. Perfect for new libraries, internal APIs, or fast-moving projects.\n\n---\n\n## Advanced Usage\n\n- ðŸ“– [**Usage Guide**](./docs/usage-guide.md) â€” Patterns, workflows, tips\n- ðŸ› ï¸ [**Tool Reference**](./docs/tools.md) â€” Complete MCP API\n- ðŸ”§ [**Configuration**](./docs/configuration.md) â€” Environment variables\n- ðŸ› [**Troubleshooting**](./docs/troubleshooting.md) â€” Common issues\n\n---\n\n## The Bottom Line\n\n**Without NotebookLM MCP**: Write code â†’ Find it's wrong â†’ Debug hallucinated APIs â†’ Repeat\n\n**With NotebookLM MCP**: Claude researches first â†’ Writes correct code â†’ Ship faster\n\nStop debugging hallucinations. Start shipping accurate code.\n\n```bash\n# Get started in 30 seconds\nclaude mcp add notebooklm npx notebooklm-mcp@latest\n```\n\n---\n\n## Disclaimer\n\nThis tool automates browser interactions with NotebookLM to make your workflow more efficient. However, a few friendly reminders:\n\n**About browser automation:**\nWhile I've built in humanization features (realistic typing speeds, natural delays, mouse movements) to make the automation behave more naturally, I can't guarantee Google won't detect or flag automated usage. I recommend using a dedicated Google account for automation rather than your primary accountâ€”think of it like web scraping: probably fine, but better safe than sorry!\n\n**About CLI tools and AI agents:**\nCLI tools like Claude Code, Codex, and similar AI-powered assistants are incredibly powerful, but they can make mistakes. Please use them with care and awareness:\n- Always review changes before committing or deploying\n- Test in safe environments first\n- Keep backups of important work\n- Remember: AI agents are assistants, not infallible oracles\n\nI built this tool for myself because I was tired of the copy-paste dance between NotebookLM and my editor. I'm sharing it in the hope it helps others too, but I can't take responsibility for any issues, data loss, or account problems that might occur. Use at your own discretion and judgment.\n\nThat said, if you run into problems or have questions, feel free to open an issue on GitHub. I'm happy to help troubleshoot!\n\n---\n\n## Contributing\n\nFound a bug? Have a feature idea? [Open an issue](https://github.com/PleasePrompto/notebooklm-mcp/issues) or submit a PR!\n\n## License\n\nMIT â€” Use freely in your projects.\n\n---\n\n<div align=\"center\">\n\nBuilt with frustration about hallucinated APIs, powered by Google's NotebookLM\n\nâ­ [Star on GitHub](https://github.com/PleasePrompto/notebooklm-mcp) if this saves you debugging time!\n\n</div>\n\n\n---\n\n# docs/configuration.md\n\n## Configuration\n\n**No config files needed!** The server works out of the box with sensible defaults.\n\n### Configuration Priority (highest to lowest):\n1. **Tool Parameters** - Claude passes settings like `browser_options` at runtime\n2. **Environment Variables** - Optional overrides for advanced users\n3. **Hardcoded Defaults** - Sensible defaults that work for most users\n\n---\n\n## Tool Parameters (Runtime Configuration)\n\nClaude can control browser behavior via the `browser_options` parameter in tools like `ask_question`, `setup_auth`, and `re_auth`:\n\n```typescript\nbrowser_options: {\n  show: boolean,              // Show browser window (overrides headless)\n  headless: boolean,          // Run in headless mode (default: true)\n  timeout_ms: number,         // Browser timeout in ms (default: 30000)\n\n  stealth: {\n    enabled: boolean,         // Master switch (default: true)\n    random_delays: boolean,   // Random delays between actions (default: true)\n    human_typing: boolean,    // Human-like typing (default: true)\n    mouse_movements: boolean, // Realistic mouse movements (default: true)\n    typing_wpm_min: number,   // Min typing speed (default: 160)\n    typing_wpm_max: number,   // Max typing speed (default: 240)\n    delay_min_ms: number,     // Min delay between actions (default: 100)\n    delay_max_ms: number,     // Max delay between actions (default: 400)\n  },\n\n  viewport: {\n    width: number,            // Viewport width (default: 1024)\n    height: number,           // Viewport height (default: 768)\n  }\n}\n```\n\n**Example usage:**\n- \"Research this and show me the browser\" â†’ Sets `show: true`\n- \"Use slow typing for this query\" â†’ Adjusts typing WPM via stealth settings\n\n---\n\n## Environment Variables (Optional)\n\nFor advanced users who want to set global defaults:\n- Auth\n  - `AUTO_LOGIN_ENABLED` â€” `true|false` (default `false`)\n  - `LOGIN_EMAIL`, `LOGIN_PASSWORD` â€” for autoâ€‘login if enabled\n  - `AUTO_LOGIN_TIMEOUT_MS` (default `120000`)\n- Stealth / Human-like behavior\n  - `STEALTH_ENABLED` â€” `true|false` (default `true`) â€” Master switch for all stealth features\n  - `STEALTH_RANDOM_DELAYS` â€” `true|false` (default `true`)\n  - `STEALTH_HUMAN_TYPING` â€” `true|false` (default `true`)\n  - `STEALTH_MOUSE_MOVEMENTS` â€” `true|false` (default `true`)\n- Typing speed (humanâ€‘like)\n  - `TYPING_WPM_MIN` (default 160), `TYPING_WPM_MAX` (default 240)\n- Delays (humanâ€‘like)\n  - `MIN_DELAY_MS` (default 100), `MAX_DELAY_MS` (default 400)\n- Browser\n  - `HEADLESS` (default `true`), `BROWSER_TIMEOUT` (ms, default `30000`)\n- Sessions\n  - `MAX_SESSIONS` (default 10), `SESSION_TIMEOUT` (s, default 900)\n- Multiâ€‘instance profile strategy\n  - `NOTEBOOK_PROFILE_STRATEGY` â€” `auto|single|isolated` (default `auto`)\n  - `NOTEBOOK_CLONE_PROFILE` â€” clone base profile into isolated dir (default `false`)\n- Cleanup (to prevent disk bloat)\n  - `NOTEBOOK_CLEANUP_ON_STARTUP` (default `true`)\n  - `NOTEBOOK_CLEANUP_ON_SHUTDOWN` (default `true`)\n  - `NOTEBOOK_INSTANCE_TTL_HOURS` (default `72`)\n  - `NOTEBOOK_INSTANCE_MAX_COUNT` (default `20`)\n- Library metadata (optional hints)\n  - `NOTEBOOK_DESCRIPTION`, `NOTEBOOK_TOPICS`, `NOTEBOOK_CONTENT_TYPES`, `NOTEBOOK_USE_CASES`\n  - `NOTEBOOK_URL` â€” optional; leave empty and manage notebooks via the library\n\n---\n\n## Storage Paths\n\nThe server uses platform-specific paths via [env-paths](https://github.com/sindresorhus/env-paths)\n- **Linux**: `~/.local/share/notebooklm-mcp/`\n- **macOS**: `~/Library/Application Support/notebooklm-mcp/`\n- **Windows**: `%LOCALAPPDATA%\\notebooklm-mcp\\`\n\n**What's stored:**\n- `chrome_profile/` - Persistent Chrome browser profile with login session\n- `browser_state/` - Browser context state and cookies\n- `library.json` - Your notebook library with metadata\n- `chrome_profile_instances/` - Isolated Chrome profiles for concurrent sessions\n\n**No config.json file** - Configuration is purely via environment variables or tool parameters!\n\n\n\n# docs/tools.md\n\n## Tools\n\n### Core\n- `ask_question`\n  - Parameters: `question` (string, required), optional `session_id`, `notebook_id`, `notebook_url`, `show_browser`.\n  - Returns NotebookLM's answer plus the follow-up reminder.\n- `list_sessions`, `close_session`, `reset_session`\n  - Inspect or manage active browser sessions.\n- `get_health`\n  - Summaries auth status, active sessions, and configuration.\n- `setup_auth`\n  - Opens the persistent Chrome profile so you can log in manually.\n- `re_auth`\n  - Switch to a different Google account or re-authenticate.\n  - Use when NotebookLM rate limit is reached (50 queries/day for free accounts).\n  - Closes all sessions, clears auth data, and opens browser for fresh login.\n\n### Notebook library\n- `add_notebook` â€“ Safe conversational add; expects confirmation before writing.\n- `list_notebooks` â€“ Returns id, name, topics, URL, metadata for every entry.\n- `get_notebook` â€“ Fetch a single notebook by id.\n- `select_notebook` â€“ Set the active default notebook.\n- `update_notebook` â€“ Modify metadata fields.\n- `remove_notebook` â€“ Removes entries from the library (not the original NotebookLM notebook).\n- `search_notebooks` â€“ Simple query across name/description/topics/tags.\n- `get_library_stats` â€“ Aggregate statistics (total notebooks, usage counts, etc.).\n\n### Resources\n- `notebooklm://library`\n  - JSON representation of the full library: active notebook, stats, individual notebooks.\n- `notebooklm://library/{id}`\n  - Fetch metadata for a specific notebook. The `{id}` completion pulls from the library automatically.\n\n**Remember:** Every `ask_question` response ends with a reminder that nudges your agent to keep asking until the userâ€™s task is fully addressed.\n\n\n# docs/troubleshooting.md\n\n## Troubleshooting\n\n### Fresh start / Deep cleanup\nIf you're experiencing persistent issues, corrupted data, or want to start completely fresh:\n\n**âš ï¸ CRITICAL: Close ALL Chrome/Chromium instances before cleanup!** Open browsers can prevent cleanup and cause issues.\n\n**Recommended workflow:**\n1. Close all Chrome/Chromium windows and instances\n2. Ask: \"Run NotebookLM cleanup and preserve my library\"\n3. Review the preview - you'll see exactly what will be deleted\n4. Confirm deletion\n5. Re-authenticate: \"Open NotebookLM auth setup\"\n\n**What gets cleaned:**\n- Browser data, cache, Chrome profiles\n- Temporary files and logs\n- Old installation data\n- **Preserved:** Your notebook library (when using preserve option)\n\n**Useful for:**\n- Authentication problems\n- Browser session conflicts\n- Corrupted browser profiles\n- Clean reinstalls\n- Switching between accounts\n\n### Browser closed / `newPage` errors\n- Symptom: `browserContext.newPage: Target page/context/browser has been closed`.\n- Fix: The server autoâ€‘recovers (recreates context and page). Reâ€‘run the tool.\n\n### Profile lock / `ProcessSingleton` errors\n- Cause: Another Chrome is using the base profile.\n- Fix: `NOTEBOOK_PROFILE_STRATEGY=auto` (default) falls back to isolated perâ€‘instance profiles; or set `isolated`.\n\n### Authentication issues\n**Quick fix:** Ask the agent to repair authentication; it will run `get_health` â†’ `setup_auth` â†’ `get_health`.\n\n**For persistent auth failures:**\n1. Close ALL Chrome/Chromium instances\n2. Ask: \"Run NotebookLM cleanup with library preservation\"\n3. After cleanup completes, ask: \"Open NotebookLM auth setup\"\n4. This creates a completely fresh browser session while keeping your notebooks\n\n**Auto-login (optional):**\n- Set `AUTO_LOGIN_ENABLED=true` with `LOGIN_EMAIL`, `LOGIN_PASSWORD` environment variables\n- For automation workflows only\n\n### Typing speed too slow/fast\n- Adjust `TYPING_WPM_MIN`/`MAX`; or disable stealth typing by setting `STEALTH_ENABLED=false`.\n\n### Rate limit reached\n- Symptom: \"NotebookLM rate limit reached (50 queries/day for free accounts)\".\n- Fix: Use `re_auth` tool to switch to a different Google account, or wait until tomorrow.\n- Upgrade: Google AI Pro/Ultra gives 5x higher limits.\n\n### No notebooks found\n- Ask to add the NotebookLM link you need.\n- Ask to list the stored notebooks, then choose the one to activate.\n\n\n# docs/usage-guide.md\n\n# Advanced Usage Guide\n\nThis guide covers advanced usage patterns, best practices, and detailed examples for the NotebookLM MCP server.\n\n> ðŸ“˜ For installation and quick start, see the main [README](../README.md).\n\n## Research Patterns\n\n### The Iterative Research Pattern\n\nThe server is designed to make your agent **ask questions automatically** with NotebookLM. Here's how to leverage this:\n\n1. **Start with broad context**\n   ```\n   \"Before implementing the webhook system, research the complete webhook architecture in NotebookLM, including error handling, retry logic, and security considerations.\"\n   ```\n\n2. **The agent will automatically**:\n   - Ask an initial question to NotebookLM\n   - Read the reminder at the end of each response\n   - Ask follow-up questions to gather more details\n   - Continue until it has comprehensive understanding\n   - Only then provide you with a complete answer\n\n3. **Session management**\n   - The agent maintains the same `session_id` throughout the research\n   - This preserves context across multiple questions\n   - Sessions auto-cleanup after 15 minutes of inactivity\n\n### Deep Dive Example\n\n```\nUser: \"I need to implement OAuth2 with refresh tokens. Research the complete flow first.\"\n\nAgent behavior:\n1. Asks NotebookLM: \"How does OAuth2 refresh token flow work?\"\n2. Gets answer with reminder to ask more\n3. Asks: \"What are the security best practices for storing refresh tokens?\"\n4. Asks: \"How to handle token expiration and renewal?\"\n5. Asks: \"What are common implementation pitfalls?\"\n6. Synthesizes all answers into comprehensive implementation plan\n```\n\n## Notebook Management Strategies\n\n### Multi-Project Setup\n\nOrganize notebooks by project or domain:\n\n```\nProduction Docs Notebook â†’ APIs, deployment, monitoring\nDevelopment Notebook â†’ Local setup, debugging, testing\nArchitecture Notebook â†’ System design, patterns, decisions\nLegacy Code Notebook â†’ Old systems, migration guides\n```\n\n### Notebook Switching Patterns\n\n```\n\"For this bug fix, use the Legacy Code notebook.\"\n\"Switch to the Architecture notebook for this design discussion.\"\n\"Use the Production Docs for deployment steps.\"\n```\n\n### Metadata Best Practices\n\nWhen adding notebooks, provide rich metadata:\n```\n\"Add this notebook with description: 'Complete React 18 documentation including hooks, performance, and migration guides' and tags: react, frontend, hooks, performance\"\n```\n\n## Authentication Management\n\n### Account Rotation Strategy\n\nFree tier provides 50 queries/day per account. Maximize usage:\n\n1. **Primary account** â†’ Main development work\n2. **Secondary account** â†’ Testing and validation\n3. **Backup account** â†’ Emergency queries when others are exhausted\n\n```\n\"Switch to secondary account\" â†’ When approaching limit\n\"Check health status\" â†’ Verify which account is active\n```\n\n### Handling Auth Failures\n\nThe agent can self-repair authentication:\n\n```\n\"NotebookLM says I'm logged outâ€”repair authentication\"\n```\n\nThis triggers: `get_health` â†’ `setup_auth` â†’ `get_health`\n\n## Advanced Configuration\n\n### Performance Optimization\n\nFor faster interactions during development:\n```bash\nSTEALTH_ENABLED=false  # Disable human-like typing\nTYPING_WPM_MAX=500     # Increase typing speed\nHEADLESS=false         # See what's happening\n```\n\n### Debugging Sessions\n\nEnable browser visibility to watch the live conversation:\n```\n\"Research this issue and show me the browser\"\n```\n\nYour agent automatically enables browser visibility for that research session.\n\n### Session Management\n\nMonitor active sessions:\n```\n\"List all active NotebookLM sessions\"\n\"Close inactive sessions to free resources\"\n\"Reset the stuck session for notebook X\"\n```\n\n## Complex Workflows\n\n### Multi-Stage Research\n\nFor complex implementations requiring multiple knowledge sources:\n\n```\nStage 1: \"Research the API structure in the API notebook\"\nStage 2: \"Switch to Architecture notebook and research the service patterns\"\nStage 3: \"Use the Security notebook to research authentication requirements\"\nStage 4: \"Synthesize all findings into implementation plan\"\n```\n\n### Validation Workflow\n\nCross-reference information across notebooks:\n\n```\n1. \"In Production notebook, find the current API version\"\n2. \"Switch to Migration notebook, check compatibility notes\"\n3. \"Verify in Architecture notebook if this aligns with our patterns\"\n```\n\n## Tool Integration Patterns\n\n### Direct Tool Calls\n\nFor manual scripting, capture and reuse session IDs:\n\n```json\n// First call - capture session_id\n{\n  \"tool\": \"ask_question\",\n  \"question\": \"What is the webhook structure?\",\n  \"notebook_id\": \"abc123\"\n}\n\n// Follow-up - reuse session_id\n{\n  \"tool\": \"ask_question\",\n  \"question\": \"Show me error handling examples\",\n  \"session_id\": \"captured_session_id_here\"\n}\n```\n\n### Resource URIs\n\nAccess library data programmatically:\n- `notebooklm://library` - Full library JSON\n- `notebooklm://library/{id}` - Specific notebook metadata\n\n## Best Practices\n\n### 1. **Context Preservation**\n- Always let the agent complete its research cycle\n- Don't interrupt between questions in a research session\n- Use descriptive notebook names for easy switching\n\n### 2. **Knowledge Base Quality**\n- Upload comprehensive documentation to NotebookLM\n- Merge related docs into single notebooks (up to 500k words)\n- Update notebooks when documentation changes\n\n### 3. **Error Recovery**\n- The server auto-recovers from browser crashes\n- Sessions rebuild automatically if context is lost\n- Profile corruption triggers automatic cleanup\n\n### 4. **Resource Management**\n- Close unused sessions to free memory\n- The server maintains max 10 concurrent sessions\n- Inactive sessions auto-close after 15 minutes\n\n### 5. **Security Considerations**\n- Use dedicated Google accounts for NotebookLM\n- Never share authentication profiles between projects\n- Backup `library.json` for important notebook collections\n\n## Troubleshooting Patterns\n\n### When NotebookLM returns incomplete answers\n```\n\"The answer seems incomplete. Ask NotebookLM for more specific details about [topic]\"\n```\n\n### When hitting rate limits\n```\n\"We've hit the rate limit. Re-authenticate with the backup account\"\n```\n\n### When browser seems stuck\n```\n\"Reset all NotebookLM sessions and try again\"\n```\n\n## Example Conversations\n\n### Complete Feature Implementation\n```\nUser: \"I need to implement a webhook system with retry logic\"\n\nYou: \"Research webhook patterns with retry logic in NotebookLM first\"\nAgent: [Researches comprehensively, asking 4-5 follow-up questions]\nAgent: \"Based on my research, here's the implementation...\"\n[Provides detailed code with patterns from NotebookLM]\n```\n\n### Architecture Decision\n```\nUser: \"Should we use microservices or monolith for this feature?\"\n\nYou: \"Research our architecture patterns and decision criteria in the Architecture notebook\"\nAgent: [Gathers context about existing patterns, scalability needs, team constraints]\nAgent: \"According to our architecture guidelines...\"\n[Provides recommendation based on documented patterns]\n```\n\n---\n\nRemember: The power of this integration lies in letting your agent **ask multiple questions** â€“ gathering context and building comprehensive understanding before responding. Don't rush the research phase!"
  },
  {
    "name": "PageLM",
    "path": "integrations/docs/PageLM.md",
    "content": "<div align=\"center\">\n  \n<img width=\"full\" height=\"auto\" alt=\"pagelm\" src=\"https://github.com/user-attachments/assets/d3133be1-1931-4132-9301-3596ebb21122\" />\n\n# PageLM\n\n**An open source AI powered education platform that transforms study materials into interactive learning experiences, slightly inspired by NotebookLM**\n\n[Report Bug](https://github.com/caviraOSS/pagelm/issues) â€¢ [Request Feature](https://github.com/caviraOSS/pagelm/issues) â€¢ [Discord server](https://discord.gg/P7HaRayqTh)\n\n</div>\n\n<p align=\"center\">\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-PageLM%20Community%20License-blueviolet.svg\" alt=\"License: PageLM Community License\"></a>\n  <a href=\"https://nodejs.org/\"><img src=\"https://img.shields.io/badge/node-%3E%3D20.0.0-brightgreen.svg\" alt=\"Node.js Version\"></a>\n  <a href=\"https://reactjs.org/\"><img src=\"https://img.shields.io/badge/React-18+-blue.svg\" alt=\"React\"></a>\n  <a href=\"https://www.typescriptlang.org/\"><img src=\"https://img.shields.io/badge/TypeScript-5.0+-blue.svg\" alt=\"TypeScript\"></a>\n  <a href=\"https://discord.gg/P7HaRayqTh\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1379682804849180844?label=Discord%20server\"></a>\n</p>\n\n---\n\n# **ðŸ”¥ Spread the Word!**\n\n<p align=\"center\">\n  <a href=\"https://twitter.com/intent/tweet?text=ðŸ¤¯%20Found%20the%20open%2Dsource%20NotebookLM%20killer%3A%20PageLM%21%20It%20turns%20PDFs%20into%20quizzes%2C%20flashcards%2C%20and%20podcasts.%20Stop%20paying%20for%20study%20tools%21&url=https%3A%2F%2Fgithub.com%2FCaviraOSS%2FPageLM&hashtags=ai,opensource,education,llm\"><img src=\"https://img.shields.io/badge/Share%20on%20X-000000?style=for-the-badge&logo=x&logoColor=white\" alt=\"Share on X\"></a>\n  &nbsp;\n  <a href=\"https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fgithub.com%2FCaviraOSS%2FPageLM&title=PageLM%3A%20The%20Open%2DSource%20NotebookLM%20Alternative%20for%20Students&summary=PageLM%20is%20an%20AI%20platform%20that%20transforms%20lecture%20notes%20and%20PDFs%20into%20interactive%20quizzes%20and%20AI%20podcasts.%20A%20great%20example%20of%20full%2Dstack%20AI%20development%20%28Node%2FReact%2FLangChain%29.\"><img src=\"https://img.shields.io/badge/Share%20on%20LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"Share on LinkedIn\"></a>\n  &nbsp;\n  <a href=\"https://reddit.com/submit?url=https%3A%2F%2Fgithub.com%2FCaviraOSS%2FPageLM&title=PageLM%3A%20Open%20Source%20AI%20Notebook%20that%20creates%20Quizzes%2C%20Flashcards%2C%20and%20Podcasts\"><img src=\"https://img.shields.io/badge/Share%20on%20Reddit-FF4500?style=for-the-badge&logo=reddit&logoColor=white\" alt=\"Share on Reddit\"></a>\n  &nbsp;\n  <a href=\"https://news.ycombinator.com/submitlink?u=https%3A%2F%2Fgithub.com%2FCaviraOSS%2FPageLM&t=Show%20HN%3A%20PageLM%20%E2%80%93%20Open%20Source%20NotebookLM%20Alternative%20(React%2FNode%2FLangChain)\"><img src=\"https://img.shields.io/badge/Hacker%20News-FF6600?style=for-the-badge&logo=y-combinator&logoColor=white\" alt=\"Submit to Hacker News\"></a>\n  &nbsp;\n  <a href=\"https://dev.to/new/share?url=https%3A%2F%2Fgithub.com%2FCaviraOSS%2FPageLM&title=PageLM%3A%20An%20Open%20Source%20AI%20Education%20Platform%20for%20Quizzes%20and%20Podcasts&prefill=I%20came%20across%20PageLM%20and%20was%20impressed%20by%20its%20architecture%20(Node.js%2FReact%2FLangChain).%20It's%20a%20full%2Dstack%20AI%20platform%20that%20supports%20Ollama%20and%20generates%20structured%20learning%20tools%20like%20ExamLab%20and%20AI%20Podcasts.%20Check%20it%20out%20and%20star%20the%20repo!%0D%0A%0D%0A**Link%20to%20Repo:**%20https%3A%2F%2Fgithub.com%2FCaviraOSS%2FPageLM\"><img src=\"https://img.shields.io/badge/Share%20on%20DEV%20Community-0A0A0A?style=for-the-badge&logo=dev.to&logoColor=white\" alt=\"Share on DEV Community\"></a>\n</p>\n\n</div>\n\n## Demo\n\n<img src=\".github/pagelm.png\" alt=\"PageLM Demo\"/>\n\nhttps://github.com/user-attachments/assets/98fae4ef-c2b7-4ad2-bfe9-1e0665eb4d71\n\n<video width=\"100%\" controls>\n  <source src=\".github/demo.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n\n> **Note**: If the video doesn't load above, you can [download the demo video directly](.github/demo.mp4)\n\n---\n\n## ðŸš€ Features\n\nPageLM converts study material into **interactive resources** including quizzes, flashcards, structured notes, and podcasts.  \nThe platform provides a modern interface for students, educators, and researchers to **enhance learning efficiency** using state-of-the-art LLMs and TTS systems.\n\n### Learning Tools\n\n- **Contextual Chat** â€“ Ask questions about uploaded documents (PDF, DOCX, Markdown, TXT)\n- **SmartNotes** â€“ Generate Cornell-style notes automatically from topics or uploaded content\n- **Flashcards** â€“ Extract non-overlapping flashcards for spaced repetition\n- **Quizzes** â€“ Create interactive quizzes with hints, explanations, and scoring\n- **AI Podcast** â€“ Convert notes and topics into engaging audio content for learning on the go\n- **Voice Transcribe** - Convert lecture recordings and voice notes into organized, searchable study materials instantly.\n- **Homework Planner** - Plans your Homework Smartly using AI, Assists if your stuck.\n- **ExamLab** - Simulate any exam, get feedback, and be prepared for the exam\n- **Debate** - Debate with AI to improve your Debate skills.\n- **Study Companion** - A personalised AI Companion that assists you.\n\n### Supported AI Models\n\n- Google Gemini â€¢ OpenAI GPT â€¢ Anthropic Claude â€¢ xAI Grok â€¢ Ollama (local) â€¢ OpenRouter\n\n### Embedding Providers\n\n- OpenAI â€¢ Gemini â€¢ Ollama\n\n### Technical Highlights\n\n- WebSocket streaming for real-time chat, notes, and podcast generation\n- JSON or vector database support for embeddings and retrieval\n- File-based persistent storage for generated content\n- Markdown-based outputs for structured answers and notes\n- Configurable multi-provider setup for LLMs and TTS engines\n\n---\n\n## ðŸ› ï¸ Technology Stack\n\n| Component      | Technology                               |\n| -------------- | ---------------------------------------- |\n| **Backend**    | Node.js, TypeScript, LangChain, Langraph |\n| **Frontend**   | Vite, React, TailwindCSS                 |\n| **Database**   | JSON (default), optional vector DB       |\n| **AI/ML**      | Multiple LLM providers, embeddings       |\n| **Audio**      | Edge TTS, ElevenLabs, Google TTS         |\n| **Deployment** | Docker, Docker Compose                   |\n| **Docs**       | pdf-lib, mammoth, pdf-parse              |\n\n---\n\n## âš¡ Getting Started\n\n### Prerequisites\n\n- Node.js v21.18+\n- npm or pnpm\n- ffmpeg (required for podcast audio)\n- Docker (optional)\n\n### Local Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/caviraOSS/pagelm.git\ncd pagelm\n\n# Linux:\n  chmod 777 ./setup.sh\n  ./setup.sh\n\n# Windows:\n  Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n  ./setup.ps1\n\n# Manual (Both Linux/Windows):\n  # Install dependencies\n  cd backend\n  npm install\n  cd ../frontend\n  npm install\n\n  # Setup environment\n  cd ..\n  npm i -g nodemon\n  cp .env.example .env\n  # Make sure to configure API keys and settings in .env\n\n  # Run these two commands in separate terminals but inside the project directory.\n  # Run backend\n  cd backend\n  npm run dev\n\n  # Run frontend\n  cd frontend\n  npm run dev\n```\n\nðŸ‘‰ Access at: **http://localhost:5173**\n\n### Docker Deployment\n\n```bash\n# Development\ndocker compose up --build\n\n# Production\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --build\n```\n\n- Frontend: http://localhost:5173 (dev) / http://localhost:8080 (prod)\n- Backend: http://localhost:5000\n\n---\n\n## âš™ï¸ Configuration\n\nAll configuration is handled via environment variables:\n\n- **LLM Provider** â€“ Choose your model backend\n- **TTS Engine** â€“ Select speech service for podcasts\n- **Database Backend** â€“ JSON or vector DB\n- **File Upload Limits** â€“ Customize size/format limits\n\nSee `.env.example` for all options.\n\n---\n\n## ðŸ‘¥ Community\n\nJoin our [Discord](https://discord.gg/P7HaRayqTh) community to connect, share ideas, and take part in exciting discussions!\n\n---\n\n## ðŸ¤ Contributing\n\nWe welcome all contributions.\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/new-feature`)\n3. Commit changes (`git commit -m \"Add feature\"`)\n4. Push (`git push origin feature/new-feature`)\n5. Open a Pull Request\n\n**Guidelines:**\n\n- Follow code style and conventions\n- Add tests where needed\n- Update docs for new features\n- Ensure all tests pass before PR\n\n---\n\n## ðŸ’¡ Areas to Contribute\n\n- AI model integrations\n- Mobile app support\n- Performance improvements\n- Accessibility features\n- Docs & tutorials\n\n---\n\n## ðŸ’– Support the Project\n\nIf you find PageLM useful, please consider supporting:\n\n**Ethereum (ERC-20)**:\n\n```\n0x5a12e3f48b6d761a120bc3cd0977e208c362a74e\n```\n\nYour support helps fund ongoing development and hosting.\n\n---\n\n## ðŸ“œ License\n\nLicensed under the **CaviraOSS Community License**.  \nFree to use, share, and modify for personal and educational purposes.  \nCommercial use or resale requires prior written permission from CaviraOSS.\n\nSee [LICENSE](LICENSE.md) for full terms.\n\n---\n\n<div align=\"center\">\n\n**Built with â¤ï¸ by CaviraOSS and contributors**\n\nâ­ Star us on [GitHub](https://github.com/CaviraOSS/pagelm) if this project helps you!\n\n</div>\n"
  },
  {
    "name": "SurfSense",
    "path": "integrations/docs/SurfSense.md",
    "content": "\n![new_header](https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65)\n\n\n<div align=\"center\">\n<a href=\"https://discord.gg/ejRNvftDp9\">\n<img src=\"https://img.shields.io/discord/1359368468260192417\" alt=\"Discord\">\n</a>\n</div>\n\n<div align=\"center\">\n\n[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)\n\n</div>\n\n# SurfSense\nConnect any LLM to your internal knowledge sources and chat with it in real time alongside your team. OSS alternative to NotebookLM, Perplexity, and Glean.\n\nSurfSense is a highly customizable AI research agent, connected to external sources such as Search Engines (SearxNG, Tavily, LinkUp), Google Drive, Slack, Linear, Jira, ClickUp, Confluence, BookStack, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma, Circleback, Elasticsearch and more to come.\n\n<div align=\"center\">\n<a href=\"https://trendshift.io/repositories/13606\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/13606\" alt=\"MODSetter%2FSurfSense | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</div>\n\n\n# Video \n\nhttps://github.com/user-attachments/assets/42a29ea1-d4d8-4213-9c69-972b5b806d58\n\n\n\n## Podcast Sample\n\nhttps://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7\n\n\n\n\n## Key Features\n\n### ðŸ’¡ **Idea**: \n- Open source alternative to NotebookLM, Perplexity, and Glean. Connect any LLM to your internal knowledge sources and collaborate with your team in real time.\n### ðŸ“ **Multiple File Format Uploading Support**\n- Save content from your own personal files *(Documents, images, videos and supports **50+ file extensions**)* to your own personal knowledge base .\n### ðŸ” **Powerful Search**\n- Quickly research or find anything in your saved content .\n### ðŸ’¬ **Chat with your Saved Content**\n- Interact in Natural Language and get cited answers.\n### ðŸ“„ **Cited Answers**\n- Get Cited answers just like Perplexity.\n### ðŸ”” **Privacy & Local LLM Support**\n- Works Flawlessly with Ollama local LLMs.\n### ðŸ  **Self Hostable**\n- Open source and easy to deploy locally.\n### ðŸ‘¥ **Team Collaboration with RBAC**\n- Role-Based Access Control for Search Spaces\n- Invite team members with customizable roles (Owner, Admin, Editor, Viewer)\n- Granular permissions for documents, chats, connectors, and settings\n- Share knowledge bases securely within your organization\n### ðŸŽ™ï¸ Podcasts \n- Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)\n- Convert your chat conversations into engaging audio content\n- Support for local TTS providers (Kokoro TTS)\n- Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)\n\n### ðŸ¤– **Deep Agent Architecture**\n\n#### Built-in Agent Tools\n| Tool | Description |\n|------|-------------|\n| **search_knowledge_base** | Search your personal knowledge base with semantic + full-text hybrid search, date filtering, and connector-specific queries |\n| **generate_podcast** | Generate audio podcasts from chat conversations or knowledge base content |\n| **link_preview** | Fetch rich Open Graph metadata for URLs to display preview cards |\n| **display_image** | Display images in chat with metadata and source attribution |\n| **scrape_webpage** | Extract full content from webpages for analysis and summarization (supports Firecrawl or local Chromium/Trafilatura) |\n\n#### Extensible Tools Registry\nContributors can easily add new tools via the registry pattern:\n1. Create a tool factory function in `surfsense_backend/app/agents/new_chat/tools/`\n2. Register it in the `BUILTIN_TOOLS` list in `registry.py`\n\n#### Configurable System Prompts\n- Custom system instructions via LLM configuration\n- Toggle citations on/off per configuration\n- Supports 100+ LLMs via LiteLLM integration\n\n### ðŸ“Š **Advanced RAG Techniques**\n- Supports 100+ LLM's\n- Supports 6000+ Embedding Models.\n- Supports all major Rerankers (Pinecone, Cohere, Flashrank etc)\n- Uses Hierarchical Indices (2 tiered RAG setup).\n- Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).\n\n### â„¹ï¸ **External Sources**\n- Search Engines (Tavily, LinkUp)\n- SearxNG (self-hosted instances)\n- Google Drive\n- Slack\n- Linear\n- Jira\n- ClickUp\n- Confluence\n- BookStack\n- Notion\n- Gmail\n- Youtube Videos\n- GitHub\n- Discord\n- Airtable\n- Google Calendar\n- Luma\n- Circleback\n- Elasticsearch\n- and more to come.....\n\n## ðŸ“„ **Supported File Extensions**\n\n| ETL Service | Formats | Notes |\n|-------------|---------|-------|\n| **LlamaCloud** | 50+ formats | Documents, presentations, spreadsheets, images |\n| **Unstructured** | 34+ formats | Core formats + email support |\n| **Docling** | Core formats | Local processing, no API key required |\n\n**Audio/Video** (via STT Service): `.mp3`, `.wav`, `.mp4`, `.webm`, etc.\n\n### ðŸ”– Cross Browser Extension\n- The SurfSense extension can be used to save any webpage you like.\n- Its main usecase is to save any webpages protected beyond authentication.\n\n\n\n## FEATURE REQUESTS AND FUTURE\n\n\n**SurfSense is actively being developed.** While it's not yet production-ready, you can help us speed up the process.\n\nJoin the [SurfSense Discord](https://discord.gg/ejRNvftDp9) and help shape the future of SurfSense!\n\n## ðŸš€ Roadmap\n\nStay up to date with our development progress and upcoming features!  \nCheck out our public roadmap and contribute your ideas or feedback:\n\n**ðŸ“‹ Roadmap Discussion:** [SurfSense 2025-2026 Roadmap: Deep Agents, Real-Time Collaboration & MCP Servers](https://github.com/MODSetter/SurfSense/discussions/565)\n\n**ðŸ“Š Kanban Board:** [SurfSense Project Board](https://github.com/users/MODSetter/projects/3)\n\n\n## How to get started?\n\n### Quick Start with Docker ðŸ³\n\n> [!TIP]\n> For production deployments, use the full [Docker Compose setup](https://www.surfsense.com/docs/docker-installation) which offers more control and scalability.\n\n**Linux/macOS:**\n\n```bash\ndocker run -d -p 3000:3000 -p 8000:8000 \\\n  -v surfsense-data:/data \\\n  --name surfsense \\\n  --restart unless-stopped \\\n  ghcr.io/modsetter/surfsense:latest\n```\n\n**Windows (PowerShell):**\n\n```powershell\ndocker run -d -p 3000:3000 -p 8000:8000 `\n  -v surfsense-data:/data `\n  --name surfsense `\n  --restart unless-stopped `\n  ghcr.io/modsetter/surfsense:latest\n```\n\n**With Custom Configuration:**\n\nYou can pass any environment variable using `-e` flags:\n\n```bash\ndocker run -d -p 3000:3000 -p 8000:8000 \\\n  -v surfsense-data:/data \\\n  -e EMBEDDING_MODEL=openai://text-embedding-ada-002 \\\n  -e OPENAI_API_KEY=your_openai_api_key \\\n  -e AUTH_TYPE=GOOGLE \\\n  -e GOOGLE_OAUTH_CLIENT_ID=your_google_client_id \\\n  -e GOOGLE_OAUTH_CLIENT_SECRET=your_google_client_secret \\\n  -e ETL_SERVICE=LLAMACLOUD \\\n  -e LLAMA_CLOUD_API_KEY=your_llama_cloud_key \\\n  --name surfsense \\\n  --restart unless-stopped \\\n  ghcr.io/modsetter/surfsense:latest\n```\n\n> [!NOTE]\n> - If deploying behind a reverse proxy with HTTPS, add `-e BACKEND_URL=https://api.yourdomain.com`\n\nAfter starting, access SurfSense at:\n- **Frontend**: [http://localhost:3000](http://localhost:3000)\n- **Backend API**: [http://localhost:8000](http://localhost:8000)\n- **API Docs**: [http://localhost:8000/docs](http://localhost:8000/docs)\n\n**Useful Commands:**\n\n```bash\ndocker logs -f surfsense      # View logs\ndocker stop surfsense         # Stop\ndocker start surfsense        # Start\ndocker rm surfsense           # Remove (data preserved in volume)\n```\n\n### Installation Options\n\nSurfSense provides multiple options to get started:\n\n1. **[SurfSense Cloud](https://www.surfsense.com/login)** - The easiest way to try SurfSense without any setup.\n   - No installation required\n   - Instant access to all features\n   - Perfect for getting started quickly\n\n2. **Quick Start Docker (Above)** - Single command to get SurfSense running locally.\n   - All-in-one image with PostgreSQL, Redis, and all services bundled\n   - Perfect for evaluation, development, and small deployments\n   - Data persisted via Docker volume\n\n3. **[Docker Compose (Production)](https://www.surfsense.com/docs/docker-installation)** - Full stack deployment with separate services.\n   - Includes pgAdmin for database management through a web UI\n   - Supports environment variable customization via `.env` file\n   - Flexible deployment options (full stack or core services only)\n   - Better for production with separate scaling of services\n\n4. **[Manual Installation](https://www.surfsense.com/docs/manual-installation)** - For users who prefer more control over their setup or need to customize their deployment.\n\nDocker and manual installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.\n\nBefore self-hosting installation, make sure to complete the [prerequisite setup steps](https://www.surfsense.com/docs/) including:\n- Auth setup (optional - defaults to LOCAL auth)\n- **File Processing ETL Service** (optional - defaults to Docling):\n  - Docling (default, local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)\n  - Unstructured.io API key (supports 34+ formats)\n  - LlamaIndex API key (enhanced parsing, supports 50+ formats)\n- Other API keys as needed for your use case\n\n\n\n## Tech Stack\n\n\n ### **BackEnd** \n\n-  **FastAPI**: Modern, fast web framework for building APIs with Python\n  \n-  **PostgreSQL with pgvector**: Database with vector search capabilities for similarity searches\n\n-  **SQLAlchemy**: SQL toolkit and ORM (Object-Relational Mapping) for database interactions\n\n-  **Alembic**: A database migrations tool for SQLAlchemy.\n\n-  **FastAPI Users**: Authentication and user management with JWT and OAuth support\n\n-  **Deep Agents**: Custom agent framework built on LangGraph for reasoning and acting AI agents with configurable tools\n\n-  **LangGraph**: Framework for developing stateful AI agents with conversation persistence\n\n-  **LangChain**: Framework for developing AI-powered applications.\n\n-  **LiteLLM**: Universal LLM integration supporting 100+ models (OpenAI, Anthropic, Ollama, etc.)\n\n-  **Rerankers**: Advanced result ranking for improved search relevance\n\n-  **Hybrid Search**: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)\n\n-  **Vector Embeddings**: Document and text embeddings for semantic search\n\n-  **pgvector**: PostgreSQL extension for efficient vector similarity operations\n\n-  **Redis**: In-memory data structure store used as message broker and result backend for Celery\n\n-  **Celery**: Distributed task queue for handling asynchronous background jobs (document processing, podcast generation, etc.)\n\n-  **Flower**: Real-time monitoring and administration tool for Celery task queues\n\n-  **Chonkie**: Advanced document chunking and embedding library\n\n  \n---\n ### **FrontEnd**\n\n-  **Next.js**: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.\n\n-  **React**: JavaScript library for building user interfaces.\n\n-  **TypeScript**: Static type-checking for JavaScript, enhancing code quality and developer experience.\n\n- **Vercel AI SDK Kit UI Stream Protocol**: To create scalable chat UI.\n\n-  **Tailwind CSS**: Utility-first CSS framework for building custom UI designs.\n\n-  **Shadcn**: Headless components library.\n\n-  **Motion (Framer Motion)**: Animation library for React.\n\n\n\n ### **DevOps**\n\n-  **Docker**: Container platform for consistent deployment across environments\n  \n-  **Docker Compose**: Tool for defining and running multi-container Docker applications\n\n-  **pgAdmin**: Web-based PostgreSQL administration tool included in Docker setup\n\n\n### **Extension** \n Manifest v3 on Plasmo\n\n\n## Contribute \n\nContributions are very welcome! A contribution can be as small as a â­ or even finding and creating issues.\nFine-tuning the Backend is always desired.\n\n### Adding New Agent Tools\n\nWant to add a new tool to the SurfSense agent? It's easy:\n\n1. Create your tool file in `surfsense_backend/app/agents/new_chat/tools/my_tool.py`\n2. Register it in `registry.py`:\n\n```python\nToolDefinition(\n    name=\"my_tool\",\n    description=\"What my tool does\",\n    factory=lambda deps: create_my_tool(\n        search_space_id=deps[\"search_space_id\"],\n        db_session=deps[\"db_session\"],\n    ),\n    requires=[\"search_space_id\", \"db_session\"],\n),\n```\n\nFor detailed contribution guidelines, please see our [CONTRIBUTING.md](CONTRIBUTING.md) file.\n\n## Star History\n\n<a href=\"https://www.star-history.com/#MODSetter/SurfSense&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=MODSetter/SurfSense&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=MODSetter/SurfSense&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=MODSetter/SurfSense&type=Date\" />\n </picture>\n</a>\n\n---\n---\n<p align=\"center\">\n    <img \n      src=\"https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4\" \n      alt=\"Catalyst Project\" \n      width=\"200\"\n    />\n</p>\n\n---\n---\n\n\n---\n\n# docs/chinese-llm-setup.md\n\n# å›½äº§ LLM é…ç½®æŒ‡å— | Chinese LLM Setup Guide\n\næœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨ SurfSense ä¸­é…ç½®å’Œä½¿ç”¨å›½äº§å¤§è¯­è¨€æ¨¡åž‹ã€‚\n\nThis guide helps you configure and use Chinese LLM providers in SurfSense.\n\n---\n\n## ðŸ“‹ æ”¯æŒçš„æä¾›å•† | Supported Providers\n\nSurfSense çŽ°å·²æ”¯æŒä»¥ä¸‹å›½äº§ LLMï¼š\n\n- âœ… **DeepSeek** - å›½äº§é«˜æ€§èƒ½ AI æ¨¡åž‹\n- âœ… **é˜¿é‡Œé€šä¹‰åƒé—® (Alibaba Qwen)** - é˜¿é‡Œäº‘é€šä¹‰åƒé—®å¤§æ¨¡åž‹\n- âœ… **æœˆä¹‹æš—é¢ Kimi (Moonshot)** - æœˆä¹‹æš—é¢ Kimi å¤§æ¨¡åž‹\n- âœ… **æ™ºè°± AI GLM (Zhipu)** - æ™ºè°± AI GLM ç³»åˆ—æ¨¡åž‹\n\n---\n\n## ðŸš€ å¿«é€Ÿå¼€å§‹ | Quick Start\n\n### é€šç”¨é…ç½®æ­¥éª¤ | General Configuration Steps\n\n1. ç™»å½• SurfSense Dashboard\n2. è¿›å…¥ **Settings** â†’ **API Keys** (æˆ– **LLM Configurations**)\n3. ç‚¹å‡» **Add New Configuration**\n4. ä»Ž **Provider** ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©ä½ çš„å›½äº§ LLM æä¾›å•†\n5. å¡«å†™å¿…å¡«å­—æ®µï¼ˆè§ä¸‹æ–¹å„æä¾›å•†è¯¦ç»†é…ç½®ï¼‰\n6. ç‚¹å‡» **Save**\n\n---\n\n## 1ï¸âƒ£ DeepSeek é…ç½® | DeepSeek Configuration\n\n### èŽ·å– API Key\n\n1. è®¿é—® [DeepSeek å¼€æ”¾å¹³å°](https://platform.deepseek.com/)\n2. æ³¨å†Œå¹¶ç™»å½•è´¦å·\n3. è¿›å…¥ **API Keys** é¡µé¢\n4. ç‚¹å‡» **Create New API Key**\n5. å¤åˆ¶ç”Ÿæˆçš„ API Key (æ ¼å¼: `sk-xxx`)\n\n### åœ¨ SurfSense ä¸­é…ç½®\n\n| å­—æ®µ | å€¼ | è¯´æ˜Ž |\n|------|-----|------|\n| **Configuration Name** | `DeepSeek Chat` | é…ç½®åç§°ï¼ˆè‡ªå®šä¹‰ï¼‰ |\n| **Provider** | `DEEPSEEK` | é€‰æ‹© DeepSeek |\n| **Model Name** | `deepseek-chat` | æŽ¨èæ¨¡åž‹<br>å…¶ä»–é€‰é¡¹: `deepseek-coder` |\n| **API Key** | `sk-xxx...` | ä½ çš„ DeepSeek API Key |\n| **API Base URL** | `https://api.deepseek.com` | DeepSeek API åœ°å€ |\n| **Parameters** | _(ç•™ç©º)_ | ä½¿ç”¨é»˜è®¤å‚æ•° |\n\n### ç¤ºä¾‹é…ç½®\n\n```\nConfiguration Name: DeepSeek Chat\nProvider: DEEPSEEK\nModel Name: deepseek-chat\nAPI Key: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAPI Base URL: https://api.deepseek.com\n```\n\n### å¯ç”¨æ¨¡åž‹\n\n- **deepseek-chat**: é€šç”¨å¯¹è¯æ¨¡åž‹ï¼ˆæŽ¨èï¼‰\n- **deepseek-coder**: ä»£ç ä¸“ç”¨æ¨¡åž‹\n\n### å®šä»·\n- è¯·è®¿é—® [DeepSeek å®šä»·é¡µé¢](https://platform.deepseek.com/pricing) æŸ¥çœ‹æœ€æ–°ä»·æ ¼\n\n---\n\n## 2ï¸âƒ£ é˜¿é‡Œé€šä¹‰åƒé—® (Alibaba Qwen) é…ç½®\n\n### èŽ·å– API Key\n\n1. è®¿é—® [é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°](https://dashscope.aliyun.com/)\n2. ç™»å½•é˜¿é‡Œäº‘è´¦å·\n3. å¼€é€š DashScope æœåŠ¡\n4. è¿›å…¥ **API-KEY ç®¡ç†**\n5. åˆ›å»ºå¹¶å¤åˆ¶ API Key\n\n### åœ¨ SurfSense ä¸­é…ç½®\n\n| å­—æ®µ | å€¼ | è¯´æ˜Ž |\n|------|-----|------|\n| **Configuration Name** | `é€šä¹‰åƒé—® Max` | é…ç½®åç§°ï¼ˆè‡ªå®šä¹‰ï¼‰ |\n| **Provider** | `ALIBABA_QWEN` | é€‰æ‹©é˜¿é‡Œé€šä¹‰åƒé—® |\n| **Model Name** | `qwen-max` | æŽ¨èæ¨¡åž‹<br>å…¶ä»–é€‰é¡¹: `qwen-plus`, `qwen-turbo` |\n| **API Key** | `sk-xxx...` | ä½ çš„ DashScope API Key |\n| **API Base URL** | `https://dashscope.aliyuncs.com/compatible-mode/v1` | é˜¿é‡Œäº‘ API åœ°å€ |\n| **Parameters** | _(ç•™ç©º)_ | ä½¿ç”¨é»˜è®¤å‚æ•° |\n\n### ç¤ºä¾‹é…ç½®\n\n```\nConfiguration Name: é€šä¹‰åƒé—® Max\nProvider: ALIBABA_QWEN\nModel Name: qwen-max\nAPI Key: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAPI Base URL: https://dashscope.aliyuncs.com/compatible-mode/v1\n```\n\n### å¯ç”¨æ¨¡åž‹\n\n- **qwen-max**: æœ€å¼ºæ€§èƒ½ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡\n- **qwen-plus**: æ€§ä»·æ¯”é«˜ï¼Œé€‚åˆæ—¥å¸¸ä½¿ç”¨ï¼ˆæŽ¨èï¼‰\n- **qwen-turbo**: é€Ÿåº¦å¿«ï¼Œé€‚åˆç®€å•ä»»åŠ¡\n\n### å®šä»·\n- è¯·è®¿é—® [é˜¿é‡Œäº‘ç™¾ç‚¼å®šä»·](https://help.aliyun.com/zh/model-studio/getting-started/billing) æŸ¥çœ‹æœ€æ–°ä»·æ ¼\n\n---\n\n## 3ï¸âƒ£ æœˆä¹‹æš—é¢ Kimi (Moonshot) é…ç½®\n\n### èŽ·å– API Key\n\n1. è®¿é—® [Moonshot AI å¼€æ”¾å¹³å°](https://platform.moonshot.cn/)\n2. æ³¨å†Œå¹¶ç™»å½•è´¦å·\n3. è¿›å…¥ **API Key ç®¡ç†**\n4. åˆ›å»ºæ–°çš„ API Key\n5. å¤åˆ¶ API Key\n\n### åœ¨ SurfSense ä¸­é…ç½®\n\n| å­—æ®µ | å€¼ | è¯´æ˜Ž |\n|------|-----|------|\n| **Configuration Name** | `Kimi` | é…ç½®åç§°ï¼ˆè‡ªå®šä¹‰ï¼‰ |\n| **Provider** | `MOONSHOT` | é€‰æ‹©æœˆä¹‹æš—é¢ Kimi |\n| **Model Name** | `moonshot-v1-32k` | æŽ¨èæ¨¡åž‹<br>å…¶ä»–é€‰é¡¹: `moonshot-v1-8k`, `moonshot-v1-128k` |\n| **API Key** | `sk-xxx...` | ä½ çš„ Moonshot API Key |\n| **API Base URL** | `https://api.moonshot.cn/v1` | Moonshot API åœ°å€ |\n| **Parameters** | _(ç•™ç©º)_ | ä½¿ç”¨é»˜è®¤å‚æ•° |\n\n### ç¤ºä¾‹é…ç½®\n\n```\nConfiguration Name: Kimi 32K\nProvider: MOONSHOT\nModel Name: moonshot-v1-32k\nAPI Key: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAPI Base URL: https://api.moonshot.cn/v1\n```\n\n### å¯ç”¨æ¨¡åž‹\n\n- **moonshot-v1-8k**: 8K ä¸Šä¸‹æ–‡ï¼ˆåŸºç¡€ç‰ˆï¼‰\n- **moonshot-v1-32k**: 32K ä¸Šä¸‹æ–‡ï¼ˆæŽ¨èï¼‰\n- **moonshot-v1-128k**: 128K ä¸Šä¸‹æ–‡ï¼ˆé•¿æ–‡æœ¬ä¸“ç”¨ï¼‰\n\n### å®šä»·\n- è¯·è®¿é—® [Moonshot AI å®šä»·](https://platform.moonshot.cn/pricing) æŸ¥çœ‹æœ€æ–°ä»·æ ¼\n\n---\n\n## 4ï¸âƒ£ æ™ºè°± AI GLM (Zhipu) é…ç½®\n\n### èŽ·å– API Key\n\n1. è®¿é—® [æ™ºè°± AI å¼€æ”¾å¹³å°](https://open.bigmodel.cn/)\n2. æ³¨å†Œå¹¶ç™»å½•è´¦å·\n3. è¿›å…¥ **API ç®¡ç†**\n4. åˆ›å»ºæ–°çš„ API Key\n5. å¤åˆ¶ API Key\n\n### åœ¨ SurfSense ä¸­é…ç½®\n\n| å­—æ®µ | å€¼ | è¯´æ˜Ž |\n|------|-----|------|\n| **Configuration Name** | `GLM-4` | é…ç½®åç§°ï¼ˆè‡ªå®šä¹‰ï¼‰ |\n| **Provider** | `ZHIPU` | é€‰æ‹©æ™ºè°± AI |\n| **Model Name** | `glm-4` | æŽ¨èæ¨¡åž‹<br>å…¶ä»–é€‰é¡¹: `glm-4-flash`, `glm-3-turbo` |\n| **API Key** | `xxx.yyy...` | ä½ çš„æ™ºè°± API Key |\n| **API Base URL** | `https://open.bigmodel.cn/api/paas/v4` | æ™ºè°± API åœ°å€ |\n| **Parameters** | _(ç•™ç©º)_ | ä½¿ç”¨é»˜è®¤å‚æ•° |\n\n### ç¤ºä¾‹é…ç½®\n\n```\nConfiguration Name: GLM-4\nProvider: ZHIPU\nModel Name: glm-4\nAPI Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.xxxxxxxxxxxxxxxx\nAPI Base URL: https://open.bigmodel.cn/api/paas/v4\n```\n\n### å¯ç”¨æ¨¡åž‹\n\n- **glm-4**: GLM-4 æ——èˆ°æ¨¡åž‹ï¼ˆæŽ¨èï¼‰\n- **glm-4-flash**: å¿«é€ŸæŽ¨ç†ç‰ˆæœ¬\n- **glm-3-turbo**: é«˜æ€§ä»·æ¯”ç‰ˆæœ¬\n\n### å®šä»·\n- è¯·è®¿é—® [æ™ºè°± AI å®šä»·](https://open.bigmodel.cn/pricing) æŸ¥çœ‹æœ€æ–°ä»·æ ¼\n\n---\n\n## âš™ï¸ é«˜çº§é…ç½® | Advanced Configuration\n\n### è‡ªå®šä¹‰å‚æ•° | Custom Parameters\n\nä½ å¯ä»¥åœ¨ **Parameters** å­—æ®µä¸­æ·»åŠ è‡ªå®šä¹‰å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰ï¼š\n\n```json\n{\n  \"temperature\": 0.7,\n  \"max_tokens\": 2000,\n  \"top_p\": 0.9\n}\n```\n\n### å¸¸ç”¨å‚æ•°è¯´æ˜Ž\n\n| å‚æ•° | è¯´æ˜Ž | é»˜è®¤å€¼ | èŒƒå›´ |\n|------|------|--------|------|\n| `temperature` | æŽ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼Œè¶Šé«˜è¶Šéšæœº | 0.7 | 0.0 - 1.0 |\n| `max_tokens` | æœ€å¤§è¾“å‡º Token æ•° | æ¨¡åž‹é»˜è®¤ | 1 - æ¨¡åž‹ä¸Šé™ |\n| `top_p` | æ ¸é‡‡æ ·å‚æ•° | 1.0 | 0.0 - 1.0 |\n\n---\n\n## ðŸ”§ æ•…éšœæŽ’é™¤ | Troubleshooting\n\n### å¸¸è§é—®é¢˜\n\n#### 1. **é”™è¯¯: \"Invalid API Key\"**\n- âœ… æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®å¤åˆ¶ï¼ˆæ— å¤šä½™ç©ºæ ¼ï¼‰\n- âœ… ç¡®è®¤ API Key æ˜¯å¦å·²æ¿€æ´»\n- âœ… æ£€æŸ¥è´¦æˆ·ä½™é¢æ˜¯å¦å……è¶³\n\n#### 2. **é”™è¯¯: \"Connection timeout\"**\n- âœ… ç¡®è®¤ API Base URL æ˜¯å¦æ­£ç¡®\n- âœ… æ£€æŸ¥ç½‘ç»œè¿žæŽ¥\n- âœ… ç¡®è®¤é˜²ç«å¢™æ˜¯å¦å…è®¸è®¿é—®\n\n#### 3. **é”™è¯¯: \"Model not found\"**\n- âœ… ç¡®è®¤æ¨¡åž‹åç§°æ˜¯å¦æ‹¼å†™æ­£ç¡®\n- âœ… æ£€æŸ¥è¯¥æ¨¡åž‹æ˜¯å¦å·²å¼€é€š\n- âœ… å‚ç…§ä¸Šæ–¹æ–‡æ¡£ç¡®è®¤å¯ç”¨æ¨¡åž‹åç§°\n\n#### 4. **æ–‡æ¡£å¤„ç†å¡ä½ (IN_PROGRESS)**\n- âœ… æ£€æŸ¥æ¨¡åž‹åç§°ä¸­æ˜¯å¦æœ‰å¤šä½™ç©ºæ ¼\n- âœ… ç¡®è®¤ API Key æœ‰æ•ˆä¸”æœ‰é¢åº¦\n- âœ… æŸ¥çœ‹åŽç«¯æ—¥å¿—: `docker compose logs backend`\n\n### æŸ¥çœ‹æ—¥å¿—\n\n```bash\n# æŸ¥çœ‹åŽç«¯æ—¥å¿—\ndocker compose logs backend --tail 100\n\n# å®žæ—¶æŸ¥çœ‹æ—¥å¿—\ndocker compose logs -f backend\n\n# æœç´¢é”™è¯¯\ndocker compose logs backend | grep -i \"error\"\n```\n\n---\n\n## ðŸ’¡ æœ€ä½³å®žè·µ | Best Practices\n\n### 1. æ¨¡åž‹é€‰æ‹©å»ºè®®\n\n| ä»»åŠ¡ç±»åž‹ | æŽ¨èæ¨¡åž‹ | è¯´æ˜Ž |\n|---------|---------|------|\n| **æ–‡æ¡£æ‘˜è¦** | Qwen-Plus, GLM-4 | å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ |\n| **ä»£ç åˆ†æž** | DeepSeek-Coder | ä»£ç ä¸“ç”¨ |\n| **é•¿æ–‡æœ¬å¤„ç†** | Kimi 128K | è¶…é•¿ä¸Šä¸‹æ–‡ |\n| **å¿«é€Ÿå“åº”** | Qwen-Turbo, GLM-4-Flash | é€Ÿåº¦ä¼˜å…ˆ |\n\n### 2. æˆæœ¬ä¼˜åŒ–\n\n- ðŸŽ¯ **Long Context LLM**: ä½¿ç”¨ Qwen-Plus æˆ– GLM-4ï¼ˆå¤„ç†æ–‡æ¡£æ‘˜è¦ï¼‰\n- âš¡ **Fast LLM**: ä½¿ç”¨ Qwen-Turbo æˆ– GLM-4-Flashï¼ˆå¿«é€Ÿå¯¹è¯ï¼‰\n- ðŸ§  **Strategic LLM**: ä½¿ç”¨ Qwen-Max æˆ– DeepSeek-Chatï¼ˆå¤æ‚æŽ¨ç†ï¼‰\n\n### 3. API Key å®‰å…¨\n\n- âŒ ä¸è¦åœ¨å…¬å¼€ä»£ç ä¸­ç¡¬ç¼–ç  API Key\n- âœ… å®šæœŸè½®æ¢ API Key\n- âœ… ä¸ºä¸åŒç”¨é€”åˆ›å»ºä¸åŒçš„ Key\n- âœ… è®¾ç½®åˆç†çš„é¢åº¦é™åˆ¶\n\n---\n\n## ðŸ“š ç›¸å…³èµ„æº | Resources\n\n### å®˜æ–¹æ–‡æ¡£\n\n- [DeepSeek æ–‡æ¡£](https://platform.deepseek.com/docs)\n- [é˜¿é‡Œäº‘ç™¾ç‚¼æ–‡æ¡£](https://help.aliyun.com/zh/model-studio/)\n- [Moonshot AI æ–‡æ¡£](https://platform.moonshot.cn/docs)\n- [æ™ºè°± AI æ–‡æ¡£](https://open.bigmodel.cn/dev/api)\n\n### SurfSense æ–‡æ¡£\n\n- [å®‰è£…æŒ‡å—](../README.md)\n- [è´¡çŒ®æŒ‡å—](../CONTRIBUTING.md)\n- [éƒ¨ç½²æŒ‡å—](../DEPLOYMENT_GUIDE.md)\n\n---\n\n## ðŸ†˜ éœ€è¦å¸®åŠ©ï¼Ÿ | Need Help?\n\nå¦‚æžœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼èŽ·å–å¸®åŠ©ï¼š\n\n- ðŸ’¬ [GitHub Issues](https://github.com/MODSetter/SurfSense/issues)\n- ðŸ’¬ [Discord Community](https://discord.gg/ejRNvftDp9)\n- ðŸ“§ Email: [é¡¹ç›®ç»´æŠ¤è€…é‚®ç®±]\n\n---\n\n## ðŸ”„ æ›´æ–°æ—¥å¿— | Changelog\n\n- **2025-01-12**: åˆå§‹ç‰ˆæœ¬ï¼Œæ·»åŠ  DeepSeekã€Qwenã€Kimiã€GLM æ”¯æŒ\n\n---\n\n**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼Happy coding with Chinese LLMs! ðŸš€**\n\n"
  }
]